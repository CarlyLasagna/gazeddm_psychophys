---
title: 'Supplement for Lasagna et al. (2026)'
subtitle: 'Modeling Self-Reference in Schizophrenia: General and Specific Mechanisms Underlying Decision-Making are Differentially Associated with Paranoia'
author: "Carly A. Lasagna, Timothy J. Pleskac, Alexander S. Weigard, Ivy F. Tso"
output:
  html_document:
    toc: yes
    toc_float: true
    toc_collapsed: true
    number_sections: yes
    fig_width: 4
    fig_height: 3
    df_print: paged
    toc_depth: 4
geometry: margin=2cm
classoption: 6pt
header-includes:
  - \usepackage{amsmath}
---

\newpage

*__Note:__ Throughout the supplement, Model 2 (Eye Contact) is the primary model for the eye contact detection task described in the main text, Model 3 (Gender) is the primary model for the gender identification task described in the main text, and Model 5 (Visual Integration) is the model for the visual integration task described in the main text.*

# Citation

Code and outputs in this document are supplemental material for:

Lasagna, C. A., Pleskac, T. J., Weigard, A., & Tso, I. F. (2025). Modeling Self-Reference in Schizophrenia: General and Specific Mechanisms Underlying Decision-Making are Differentially Associated with Paranoia. Preprint:  [(https://doi.org/10.31234/osf.io/9b23k_v3)](https://doi.org/10.31234/osf.io/9b23k_v3)

# Code Availability

**Code used to perform all modeling, simulation, and analysis is available on OSF and GitHub:**

-   OSF [(https://osf.io/e3sxu/?view_only=eaf31c32ae7f48a2ab21305ee2bcfe2a)](https://osf.io/e3sxu/?view_only=eaf31c32ae7f48a2ab21305ee2bcfe2a)
-   GitHub [(https://github.com/CarlyLasagna/gazeddm_psychophys)](https://github.com/CarlyLasagna/gazeddm_psychophys).

# [METHODS SUPPLEMENT] {.unnumbered}

# Behavioral Tasks

## Eye Contact Detection Task

Below are sample stimuli for the eye contact detection task. For modeling, "Yes" responses were recoded as 1 (upper boundary responses), "No" responses were recoded as 2 (lower boundary responses), and the eye contact signal strengths (shown below) were mean-centered. 

![](/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/manuscript/figures and tables/gaze_stimuli.png)


## Gender Identification Task

Below are sample stimuli for the gender identification task. For INITIAL model fits (i.e., model comparison, initial covergence tests, parameter recovery, and posterior predictions), "female" responses were recoded as 1 (upper boundary responses), "male" responses were recoded as 2 (lower boundary responses), and the gender of stimuli were effects coded as: 1=Female, -1=Male. However, this orientation was later flipped for "final model fits" and analyses (i.e., such that 1=Male [upper boundary] and -1=Female [lower boundary] for stimuli and responses). This was done to ensure that the buttons used to respond during all 3 behavioral tasks were in agreement. That way, responses made with the subjects' index finger (Yes/Male/Left) corresponded to upper boundary responses and responses made with the subjects'  middle finger (No/Female/Right) corresponded to lower boundary responses. 

![](/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/manuscript/figures and tables/gender_stimuli.png)

## Visual Integration Task

Below are sample stimuli for the Visual Integration task. In the upper plot, stimuli have been enhanced -- and a contour outline added -- to better illustrate the contour effect. In the image lower plot, as example of the unedited, original stimulus is displayed as it appeared in the Visual Integration task. 

For modeling, "Left" responses were recoded as 1 (upper boundary responses), "Right" responses were recoded as 2 (lower boundary responses), and the stimulus signal strengths (shown below) were recoded as follows: 1) first we added a small constant value of .0001 to the lowest signal strength (indicated by "^" below) to make all signals positive, 2) then we multiplied these signal strengths by the effects coded direction of each stimulus (1=Left, -1=Right). This generated a mean centered signal strength for each trial where higher magnitude signals (i.e., more positive or more negative) meant stronger visual signal strengths, values > 0 indexed left-facing stimuli, and values < 0 indexed right-facing stimuli. As shown below, this captured stimulus direction and signal strength within a single variable. 

![](/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/manuscript/figures and tables/jovi_stimuli.png)

# [MODELING SUPPLEMENT] {.unnumbered}

```{r id1, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE, out.width='1500px'}

########################################################
# SETUP ENVIRONMENT
########################################################
library(ggplot2)
library(readxl)
library(dplyr)
library(cmdstanr)
library(posterior)
library(bayesplot)
library(data.table)
library(loo)
library(kableExtra)
library(rstan)
library(HDInterval)
library(GGally)
library(gridExtra)
library(pcaMethods)
library(psych)
library(DescTools)

colors<-c('#287c8e','#482475','#addc30')

bayesplot_theme_set(theme_default(base_size = 8, base_family = "sans"))

#parent directory
dirname<-'/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm'
datadir<-paste(dirname,'/data/',sep="")

########################################################
# CUSTOM FUNCTIONS
########################################################
rename_corr_pars<-function(pars){
  pars <- gsub("alpha_gend", "Threshold Sep (Gender)", pars)
  pars <- gsub("beta_gend", "Start Point (Gender)", pars)
  pars <- gsub("delta_bias_gend", "Drift Bias (Gender)", pars)
  pars <- gsub("delta_female_gend", "Drift Rate-F (Gender)", pars)
  pars <- gsub("delta_male_gend", "Drift Rate-M (Gender)", pars)
  pars <- gsub("delta_gend", "Drift Sensitivity (Gender)", pars)
  
  pars <- gsub("alpha_jovi", "Threshold Sep (Visual Integration)", pars)
  pars <- gsub("beta_jovi", "Start Point (Visual Integration)", pars)
  pars <- gsub("delta_bias_jovi", "Drift Bias (Visual Integration)", pars)
  pars <- gsub("delta_L_jovi", "Drift Rate-L (Visual Integration)", pars)
  pars <- gsub("delta_R_jovi", "Drift Rate-R (Visual Integration)", pars)
  pars <- gsub("delta_jovi", "Drift Bias (Visual Integration)", pars)
  pars <- gsub("delta_b1_jovi", "Drift Sensitivity (Visual Integration)", pars)
  
  pars <- gsub("delta_bias", "Drift Bias (Eye Contact)", pars)
  pars <- gsub("delta_b1", "Drift Sensitivity (Eye Contact)", pars)
  pars <- gsub("alpha", "Threshold Sep (Eye Contact)", pars)
  pars <- gsub("beta", "Start Point (Eye Contact)", pars)
  pars <- gsub("age", "Age", pars)
  pars <- gsub("female_dummy", "Female", pars)
  pars <- gsub("sz_dummy", "SZ", pars)
  pars <- gsub("matrics", "General Cognition (MCCB)", pars)
  
  return(pars)
}
rename_gaze_pars<-function(pars){
  pars <- gsub("alpha", "threshold_sep", pars)
  pars <- gsub("beta", "start_point", pars)
  pars <- gsub("delta", "drift_bias", pars)
  pars <- gsub("_b1", "_drift_sensitiv", pars)
  pars <- gsub("grp_", "", pars)
  pars <- gsub("mu_", "group_", pars)
  pars <- gsub("_pr.", "_untransformed.", pars)
  
  pars <- ifelse(
  grepl("^sub_", pars), # Check if the string starts with 'sub_'
  pars, # If true, keep the string as is
  gsub("\\.1\\.", "_hc", gsub("\\.2\\.", "_sz", pars)) # Otherwise, replace .1. and .2.
  )
  return(pars)
}
rename_gender_pars<-function(pars){
  pars <- gsub("alpha", "threshold_sep", pars)
  pars <- gsub("beta", "start_point", pars)
  
  pars <- gsub("delta", "drift_rate", pars)
  pars <- gsub("_present", "_female", pars)
  pars <- gsub("_absent", "_male", pars)
  pars <- gsub("grp_", "", pars)
  pars <- gsub("mu_", "group_", pars)
  pars <- gsub("_pr.", "_untransformed.", pars)
  
  pars <- ifelse(
  grepl("^sub_", pars), # Check if the string starts with 'sub_'
  pars, # If true, keep the string as is
  gsub("\\.1\\.", "_hc", gsub("\\.2\\.", "_sz", pars)) # Otherwise, replace .1. and .2.
  )
  return(pars)
}
rename_jovi_pars<-function(pars){
  pars <- gsub("alpha", "threshold_sep", pars)
  pars <- gsub("beta", "start_point", pars)
  pars <- gsub("delta", "drift_rate", pars)
  pars <- gsub("_b1", "_drift_sensitiv", pars)
  pars <- gsub("_L", "_L", pars)
  pars <- gsub("_R", "_R", pars)
  pars <- gsub("grp_", "", pars)
  pars <- gsub("mu_", "group_", pars)
  pars <- gsub("_pr.", "_untransformed.", pars)
  
  pars <- ifelse(
  grepl("^sub_", pars), # Check if the string starts with 'sub_'
  pars, # If true, keep the string as is
  gsub("\\.1\\.", "_hc", gsub("\\.2\\.", "_sz", pars)) # Otherwise, replace .1. and .2.
  )
  return(pars)
}
zscore_cols <- function(data) {
  
  # Loop through each column in the data frame
  for (col in names(data)) {
    # Calculate mean and standard deviation of the column
    col_mean <- mean(data[[col]])
    col_sd <- sd(data[[col]])
    
    # Calculate z-score for each value in the column
    z_scores <- (data[[col]] - col_mean) / col_sd
    
    # Replace the column with z-scores
    data[[col]] <- z_scores
  }
  return(data)
} 
brms_robust_glm_compare<-function(data,test_models,outcome,outfile,credMass,nu){
  
  library(brms)
  library(loo)
  library(stats)
  
  fits<-list()
  
  if(file.exists(outfile)){
    fits<-readRDS(outfile)
  }else{
    for(i in 1:length(test_models)){
      
      formula<-paste(outcome, paste(test_models[[i]],collapse=" + "),sep=" ~ ")
      brms_formula<-bf(formula,nu=nu)
      predictors<-test_models[[i]] #get b predictors (excluding intercept/sigma)
      
      #function dynamically sets priors on standardized b predictors (input to function), as well as intercept and sigma terms, for brms models
      
      dynamic_prior<-function(predictors){ 
        #set prior for intercept and sigma
        ## note: intercept = mean centered b_Intercept = original scale. 
        prior_combine<-brms::prior_string(paste0("normal(0,1)"), class = "Intercept")+
          brms::prior_string(paste0("student_t(3, 0, 2.5)"), class = "sigma") #default prior in brms
        
        #set priors on b coefs
        for (k in 1:length(predictors)){
          tmp_prior<-brms::prior_string(paste0("normal(0,1)"), class = "b",coef=predictors[k])
          prior_combine<-prior_combine+tmp_prior
        }
        return(prior_combine)
      }
      
      fit <- brm(
        formula=brms_formula,
        data = data,
        sample_prior = TRUE,
        family = student(), 
        save_pars = save_pars(all = TRUE),
        chains = 8, cores = 8,seed=42,iter = 5000, warmup = 1000, thin = 1,
        prior = dynamic_prior(predictors),
        control = list(adapt_delta = 0.95)
        )
      
      fits[[i]] <- add_criterion(fit, "loo")
    }
    saveRDS(fits, outfile)
  }
  
  for (i in 1:length(test_models)){

    #get posterior samples
    samples <- as.data.frame(fits[[i]]) 
    
    #extract prior samples
    prior_samples<-samples[,grep("prior", names(samples), ignore.case = TRUE)]
    
    # remove extra columns (interpret b_Intercept, not Intercept,priors)
    samples <- samples[, !(names(samples) %in% c("Intercept","sigma", "prior", "lp__"))]
    samples <- samples[,-grep("prior", names(samples), ignore.case = TRUE)]
    samples <- samples[,-grep("nu", names(samples), ignore.case = TRUE)]
    
    n_pars<-ncol(samples) #define number of parameters in current model
    
    tmp_loo<-data.frame(loo(fits[[i]])[['estimates']])
    elpd_loo<-round(tmp_loo$Estimate[1],2)
    
    # calculate frequentist values (model R2)
    r2_1 <- mean(bayes_R2(fits[[1]]))
    r2_2 <- mean(bayes_R2(fits[[i]]))
    
    if(i==1){
      delta_elpd_se<-"--"
      delta_elpd<-"--"
      model_bf<-"--"
      model_bf_direction<-"--"
      model_bf_range<-"--"
      r2_change<-"--"
      cohen_f2<-"--"
    }else{
      tmp_loo_compare<-data.frame(loo_compare(fits[[i]],fits[[1]]))#compare current model to model1 
      tmp_bf_compare<-capture.output(brms::bayes_factor(fits[[i]],fits[[1]],log=FALSE)) #model-level bayes factor
      tmp_bf_compare<-tmp_bf_compare[length(tmp_bf_compare)]
      model_bf <- round(as.numeric(sub(".+\\s(\\d+\\.?\\d*)$", "\\1", tmp_bf_compare)),2) #bf of current model over model1
      
      # calculate frequentist values: r squared change and cohen's f2
      r2_change<-round(r2_2-r2_1,3)
      cohen_f2 <- round((r2_2 - r2_1) / (1 - r2_2),2)
      
      #direction of BF evidence
      if(model_bf==1){
        model_bf_direction<-"No Evidence"
      }else if(model_bf>1){
        model_bf_direction<-"Alt"
      }else{
        model_bf_direction<-"Null"
      }
      
      # strength of bayes factor evidence. ranges from Lee and Wagenmakers (2014)
      if((model_bf >= 100)){
        model_bf_range <- "Extreme"
      }else if(model_bf >= 30 && model_bf < 100){
        model_bf_range <- "Very Strong"
      }else if(model_bf >= 10 && model_bf < 30){
        model_bf_range <- "Strong"
      }else if(model_bf >= 3 && model_bf < 10){
        model_bf_range <- "Moderate"
      }else if(model_bf >= 1 && model_bf < 3){
        model_bf_range <- "Anecdotal"
      }else if(model_bf == 1){
        model_bf_range <- "No Evidence"
      }else if(model_bf >= 0.33 && model_bf < 1){
        model_bf_range <- "Anecdotal"
      }else if(model_bf >= 0.1 && model_bf < 0.33){
        model_bf_range <- "Moderate"
      }else if(model_bf >= 0.03 && model_bf < 0.1){
        model_bf_range <- "Strong"
      }else if(model_bf >= 0.01 && model_bf < 0.03){
        model_bf_range <- "Very Strong"
      }else if(model_bf < 0.01){
        model_bf_range <- "Extreme"
      }else{
        model_bf_range <- ""
      }
      
      #if current model = better fit than model 1, then it will appear in row1.
      ## then we need to get the elpd diff from row 2 and flip it.
      if(rownames(tmp_loo_compare)[1]=="fits[[i]]"){ 
        
        # get difference from row2 and flip it
        delta_elpd<-round(tmp_loo_compare$elpd_diff[2],2)*-1
        delta_elpd_se<-round(tmp_loo_compare$se_diff[2],2)
        
      }else{  #if current model = worse fit than model 1, then it will appear in row2
        delta_elpd<-round(tmp_loo_compare$elpd_diff[2],2)
        delta_elpd_se<-round(tmp_loo_compare$se_diff[2],2)
      }
    }
    
    #generate model summary
    tmp_mod_summary<-data.frame(Model=i,
                                Ref_Model=1,
                                elpd_loo=elpd_loo,
                                delta_elpd=paste0(delta_elpd," (",delta_elpd_se,")"),
                                model_bf=paste0(model_bf," (",model_bf_direction,
                                                                  "/",model_bf_range,")"),
                                r2=paste0(round(r2_2,3)," (",r2_change,")"),
                                f2=cohen_f2,
                                Predictors="", #predictor name
                                pred_bf="",#predictor bf
                                Mean_and_HDI="") #mean and 95% HDI of posterior
    
    for (j in 1:n_pars){
      tmp_par<-colnames(samples)[j]
      tmp_par <- sub("^b_", "", tmp_par)

      post_par<-paste("b_",tmp_par,sep="") #par name formatted as posterior pars
      
      if(post_par=="b_Intercept"){
        prior_par<-paste("prior_",tmp_par,sep="") #par name formatted as prior pars
      }else{
        prior_par<-paste("prior_b_",tmp_par,sep="") #par name formatted as prior pars
      }

      # CALCULATE BAYES FACTORS FOR PREDICTORS W/ SAVAGE DICKEY DENSITY RATIO (WAGENMAKERS 2010)

      ## for function, enter posterior samples, prior samples, and estimation method (we use logspline)
      
      ## we calculate the ratio to be consistent with the rest of the paper,
      ## such that values >1 favor the alternative and values <1 favor the null
      savage_dickey_ratio<-function(posterior,prior,method){ 
      
        ### logspline kernel density estimation (as in Wagenmakers 2010)
        if(method=="logspline"){
          library(logspline)
          
          log_spline_density<-invisible(logspline(posterior,silent=TRUE)) #density estimate for posterior
          posterior_density<-dlogspline(0, log_spline_density, log = F)  #density @ 0
          
          log_spline_density<-invisible(logspline(prior,silent=TRUE)) #density estimate for prior
          prior_density<-dlogspline(0, log_spline_density, log = F)  #density @ 0
          
          # a group effect of 0 is ~11 times LESS likely than it was before seeing the data (i.e., 1/.09)
          bf<-round(prior_density/posterior_density,2)
        }
        
        #direction of BF evidence
        if(bf==1){
          bf_direction<-"No Evidence"
        }else if(bf>1){
          bf_direction<-"Alt"
        }else{
          bf_direction<-"Null"
        }
        
        # strength of bayes factor evidence. ranges from Lee and Wagenmakers (2014)
        if((bf >= 100)){
          bf_range <- "Extreme"
        }else if(bf >= 30 && bf < 100){
          bf_range <- "Very Strong"
        }else if(bf >= 10 && bf < 30){
          bf_range <- "Strong"
        }else if(bf >= 3 && bf < 10){
          bf_range <- "Moderate"
        }else if(bf >= 1 && bf < 3){
          bf_range <- "Anecdotal"
        }else if(bf == 1){
          bf_range <- "No Evidence"
        }else if(bf >= 0.33 && bf < 1){
          bf_range <- "Anecdotal"
        }else if(bf >= 0.1 && bf < 0.33){
          bf_range <- "Moderate"
        }else if(bf >= 0.03 && bf < 0.1){
          bf_range <- "Strong"
        }else if(bf >= 0.01 && bf < 0.03){
          bf_range <- "Very Strong"
        }else if(bf < 0.01){
          bf_range <- "Extreme"
        }else{
          bf_range <- ""
          
        }
        combine<-list(bf=bf,
                      bf_range=bf_range,
                      bf_direction=bf_direction)
        
          return(combine)
      }
      
      # reflects how likely are data under alternative hypothesis (vs null hypothesis that effect = 0)
      ## values >1 favor the alternative; values < 1 favor the null
      pred_bf<-savage_dickey_ratio(samples[[post_par]],prior_samples[[prior_par]],"logspline")
      
      #calculate mean and HDI of given set of samples, for a given credible mass
      mean_and_hdi<-function(samples, credMass){
        library(HDInterval)
        lower<-HDInterval::hdi(samples,credMass=credMass)[1]
        upper<-HDInterval::hdi(samples,credMass=credMass)[2]
        mean<-mean(samples)
        
        if(lower>0&&upper>0|lower<0&&upper<0){
          credible<-"*"
        }else{
          credible<-""
        }
        
        summary<-paste(round(mean,2)," [",round(lower,3),", ",round(upper,3),"]",credible,sep="")
        return(summary)
      } 

      #generate predictor summary
      tmp_pred_summary<-data.frame(Model="",
                                   Ref_Model="",
                                   elpd_loo="",
                                   delta_elpd="",
                                   model_bf="",
                                   r2="",
                                   f2="",
                                   Predictors=tmp_par, #predictor name
                                   pred_bf=paste0(pred_bf[["bf"]]," (", pred_bf[["bf_direction"]],
                                                                    "/",pred_bf[["bf_range"]],")"), #predictor bf strength/direction
                                   Mean_and_HDI=mean_and_hdi(samples=samples[[post_par]],credMass = credMass)) 
      
      if(j==1){ #append to predictor summary dataframe
        running_summary<-tmp_pred_summary
      }else{
        running_summary<-rbind(running_summary,tmp_pred_summary)
      }
    }
    
    #combine model summary and predictor summary
    combined_summary<-rbind(tmp_mod_summary,running_summary)
    
    if(i==1){
      all_summary<-combined_summary
    }else{
      all_summary<-rbind(all_summary,combined_summary)
    }
  }
  
  colnames(all_summary)<-c("Model","Ref Model","ELPD",
                        paste("\u0394","ELPD (SE)",sep=""),
                        "Model BF",paste("R2 (","\u0394","R2)",sep=""),"f2","Pred",
                        "Pred BF",
                        paste0("Pred Mean [",as.character(credMass*100),"% HDI]"))
  return(all_summary)
}
print_diff_mean_hdi<-function(samples, credMass,digits){
        
        # samples: should be dataframe with nrows=nposterior samples and 2 columns
        # col names should be names of groups you want printed. diff will be
        # calculated as col 1 minus col2.
        
        #credMass = what credible interval do you want to use for HDI?
        
        #digits= how many digits to round output to
        
        library(HDInterval)
        diff<-samples[,1]-samples[,2]
        
        lower<-HDInterval::hdi(diff,credMass=credMass)[1]
        upper<-HDInterval::hdi(diff,credMass=credMass)[2]
        mean<-mean(diff)
        
        if(lower>0&&upper>0|lower<0&&upper<0){
          credible<-"*"
        }else{
          credible<-""
        }
        
        group1<-colnames(samples)[1]
        group2<-colnames(samples)[2]
        
        summary<-paste0(group1,"-",group2,": ",round(mean,digits)," [",
                        round(lower,digits),", ",
                        round(upper,digits),"]",credible)
        
        return(summary)
      }
bayes_corr_matrix<-function(data,credMass){

  library(bayestestR)
  library(BayesFactor)
  options(scipen = 999) #deters r from using scientific notation
  
  n_vars<-ncol(data)
  vars<-colnames(data)
  
  # create empty correlation matrix to hold outputs
  corr_mat<-as.data.frame(matrix(data="",ncol=n_vars,nrow=n_vars))
  rownames(corr_mat)<-colnames(corr_mat)<-vars
  
  #create correlation matrices to hold mean rho estimates, bayes factors, rho ci's, and bf ranges
  corr_mat_credible<-corr_mat_rho<-corr_mat_bf<-corr_mat_ci<-corr_mat_bf_range<-corr_mat
  
  iteration<-0
  
  for (i in 1:n_vars){ #var1 / rows
    for (j in 1:n_vars){ #var2 / cols
      
      var1<-vars[i] #name of variable1
      var2<-vars[j] #name of variable2
      data1<-as.numeric(data[[var1]]) #get data for variable1
      data2<-as.numeric(data[[var2]]) #get data for variable2
      
      n_obs<-nrow(na.omit(cbind(data1,data2)))#number of complete observations
      
      if(var1==var2){ #if variables match, set rho (corr coefficient) to 1, otherwise run bayes correlations
        corr_mat_rho[i,j]<-1
      }else{
        
        if(n_obs>2){ #if at least 3 observations, set rho to NA and skip
          iteration<-iteration+1
          
          result <- correlationBF(data1, data2) #calculate BF
          output<-describe_posterior(result, ci=credMass, centrality = "mean",ci_method="HDI") #get output 
          rho<-round(output$Mean,2) #mean of rho (correlation coefficient estimate)
          ci_lo<-round(output$CI_low,2) #lower bound of credible interval around rho
          ci_hi<-round(output$CI_high,2) #upper bound of credible interval around rho
          bf<-output$BF #bayes factor
          bf_direction<-ifelse(bf<1,"Null","Alternative")
          
          # ranges from Lee and Wagenmakers (2014)
          if(bf >= 100){
            bf_range <- "Extreme"
          }else if(bf >= 30 && bf < 100){
            bf_range <- "Very Strong"
          }else if(bf >= 10 && bf < 30){
            bf_range <- "Strong"
          }else if(bf >= 3 && bf < 10){
            bf_range <- "Moderate"
          }else if(bf >= 1 && bf < 3){
            bf_range <- "Anecdotal"
          }else if(bf == 1){
            bf_range <- "No Evidence"
          }else if(bf >= 0.33 && bf < 1){
            bf_range <- "Anecdotal"
          }else if(bf >= 0.1 && bf < 0.33){
            bf_range <- "Moderate"
          }else if(bf >= 0.03 && bf < 0.1){
            bf_range <- "Strong"
          }else if(bf >= 0.01 && bf < 0.03){
            bf_range <- "Very Strong"
          }else if(bf < 0.01){
            bf_range <- "Extreme"
          }else{
            bf_range <- ""
          }
          
          if((ci_lo>0&&ci_hi>0)|(ci_lo<0&&ci_hi<0)){
            credible<-"*"
          }else{
            credible<-""
          }
          
          tmp_long_summary<-data.frame(Variable1=var1,
                                       Variable2=var2,
                                       Mean=rho,
                                       HDI=paste("[",ci_lo,",",ci_hi,"]",credible,sep=""),
                                       BF=round(bf,2),
                                       BF_Evidence_Strength=bf_range,
                                       BF_Evidence_Favors=bf_direction,
                                       N=n_obs)
          
          corr_mat_rho[i,j]<-rho
          corr_mat_bf[i,j]<-bf
          corr_mat_bf_range[i,j]<-bf_range
          corr_mat_ci[i,j]<-paste("[",ci_lo,",",ci_hi,"]",sep="")
          corr_mat_credible[i,j]<-credible
          
          if(iteration==1){
            all_long_summary<-tmp_long_summary
          }else{
            all_long_summary<-rbind(all_long_summary,tmp_long_summary)
          }
        }else{
          corr_mat_rho[i,j]<-NA #if fewer than 3 observations, set rho to NA
          corr_mat_credible[i,j]<-""
        }
      }
    }
  }
  
  var_list<-data.frame(Variable=rownames(corr_mat_rho))
  
  # Convert dataframe to long format for Rho values
  corr_mat_rho<-cbind(var_list,corr_mat_rho)
  df_long <- melt(corr_mat_rho,id="Variable",varnames = c("Variable","value"))
  colnames(df_long)<-c("Variable1","Variable2","Mean")
  df_long$Mean<-as.numeric(df_long$Mean)
  
  #do the same for CI's
  corr_mat_ci<-cbind(var_list,corr_mat_ci)
  df_long_ci <- melt(corr_mat_ci,id="Variable",varnames = c("Variable","value"))
  colnames(df_long_ci)<-c("Variable1","Variable2","CI")
  
  #do the same for BF
  corr_mat_bf<-cbind(var_list,corr_mat_bf)
  df_long_bf <- melt(corr_mat_bf,id="Variable",varnames = c("Variable","value"))
  colnames(df_long_bf)<-c("Variable1","Variable2","BF")
  
  #do the same for BF range
  corr_mat_bf_range<-cbind(var_list,corr_mat_bf_range)
  df_long_bf_range <- melt(corr_mat_bf_range,id="Variable",varnames = c("Variable","value"))
  colnames(df_long_bf_range)<-c("Variable1","Variable2","BF_Range")
  
  #do the same for credible
  corr_mat_credible<-cbind(var_list,corr_mat_credible)
  df_long_credible <- melt(corr_mat_credible,id="Variable",varnames = c("Variable","value"))
  colnames(df_long_credible)<-c("Variable1","Variable2","Credible")
  
  # Create the plot with separate labels for Rho and corr_mat_credible
  p <- ggplot(df_long, aes(x = factor(Variable1, levels = vars), y = factor(Variable2, levels = vars))) +
    geom_tile(aes(fill = Mean), color = "black") +
    geom_text(aes(label = round(Mean, 2)), color = "black", vjust = 0.3) +  # Add Rho values
    geom_text(data = df_long_credible, aes(label = Credible), color = "black", vjust = 1.5,size=6) +  # Add credible values
    scale_fill_gradient2(low = "steelblue", high = "firebrick", midpoint = 0, na.value = "transparent", guide = "legend") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(title = "Correlation Matrix",
         x = "", y = "",
         fill = "Correlation\nStrength")
  
  rownames(all_long_summary)<-NULL
  
  combined<-list(long_summary=all_long_summary,
                 corr_mat_rho=corr_mat_rho,
                 corr_mat_ci=corr_mat_ci,
                 corr_mat_bf_range=corr_mat_bf_range,
                 corr_mat_bf=corr_mat_bf,
                 corr_mat_credible=corr_mat_credible,
                 plot=p)

  return(combined)
}
plot_parrecover_group_bysim<-function(data,par,parname,sim_id,genMean_suffix,simMean_suffix, simHDI_suffix,recover_suffix){
  text<-paste(round(mean(data[[paste0(par,recover_suffix)]]),2)*100,"% Recovery",sep="")
  data[[paste0(par,recover_suffix)]] <- factor(data[[paste0(par,recover_suffix)]], levels = c("1", "0"), labels = c("recovered", "not_recovered"))
  
  n_sims<-max(data[[sim_id]])
  data[[sim_id]]<-factor(data[[sim_id]])
  
  title<-paste0(parname,"\nParameter Recovery")
  xlabel<-"Parameter Value"
  
  xaxis_min<-min(data[[paste0(par,simHDI_suffix,"lo")]])-(min(data[[paste0(par,simHDI_suffix,"lo")]])*.1)
  xaxis_max<-min(data[[paste0(par,simHDI_suffix,"hi")]])+(max(data[[paste0(par,simHDI_suffix,"hi")]])*.1)
  
  text_position<-mean(data[[paste0(par,genMean_suffix)]])
    
  p <- ggplot(data = data, aes(x = .data[[paste0(par,simMean_suffix)]],y=.data[[sim_id]], color=.data[[paste0(par,recover_suffix)]])) +
  geom_point(data = data, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(data[[paste0(par,genMean_suffix)]]))+
  theme_bw() +
  geom_errorbar(data = data, aes(xmin = .data[[paste0(par,simHDI_suffix,"lo")]], xmax = .data[[paste0(par,simHDI_suffix,"hi")]]), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,n_sims+10)) +
  ggtitle(title) +
  xlab(xlabel) +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "label", x = text_position, y =n_sims+8, label = text, hjust = "center",size=3,lineheight=.9)
  return(p)

}
plot_parrecover_subj<-function(data,par,parname,sim_id,genMean_suffix,simMean_suffix, simHDI_suffix,recover_suffix){
  
  cor_coef<-cor(data[[paste0(par,simMean_suffix)]],data[[paste0(par,genMean_suffix)]],method = c("pearson"))
  text<-paste0(round(mean(data[[paste0(par,recover_suffix)]]),2)*100,"% Recovery; r = ",
              round(cor_coef,2))
  
  data[[paste0(par,recover_suffix)]] <- factor(data[[paste0(par,recover_suffix)]], levels = c("1", "0"), labels = c("recovered", "not_recovered"))
  
  n_sims<-max(data[[sim_id]])
  data[[sim_id]]<-factor(data[[sim_id]])
  
  title<-paste0(parname,"\nParameter Recovery")
  xlabel<-paste0("Parameter Value (",parname,")")
  
  axis_min<-min(data[[paste0(par,simHDI_suffix,"lo")]])-(min(data[[paste0(par,simHDI_suffix,"lo")]])*.05)
  axis_max<-max(data[[paste0(par,simHDI_suffix,"hi")]])+(max(data[[paste0(par,simHDI_suffix,"hi")]])*.05)
  
  text_position_x<-text_position_y<-(axis_max+axis_min)/2
  
  #text_position_x<-mean(data[[paste0(par,genMean_suffix)]])-(mean(data[[paste0(par,genMean_suffix)]])*.2)
  #text_position_y<-mean(data[[paste0(par,simMean_suffix)]])+(mean(data[[paste0(par,genMean_suffix)]])*.2)
    
  p <- ggplot(data = data, aes(x = .data[[paste0(par,genMean_suffix)]],y=.data[[paste0(par,simMean_suffix)]], color=.data[[paste0(par,recover_suffix)]])) +
  geom_point(data = data, alpha = .2) + 
  geom_errorbar(data = data, aes(ymin=.data[[paste0(par,simHDI_suffix,"lo")]],ymax=.data[[paste0(par,simHDI_suffix,"hi")]]),alpha=.15)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(axis_min,axis_max),ylim = c(axis_min, axis_max))+theme_bw()+
  ggtitle(title) +
  theme(legend.position="none",plot.title = element_text(size=12))+
  xlab("Generating Parameter Value")+
  ylab("Estimated Parameter Value")+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "label", x = text_position_x, y =text_position_y, label = text, hjust = "center",size=3,lineheight=.9)
  
  return(p)
  
}
pcamethods_bpca<-function(data,npcs,scale,center,method){
  library(pcaMethods)
  library(psych)
  library(stats)
  
  n_var<-ncol(data)
  
  if(method=="pcamethods"){
      na_rows <- which(apply(data, 1, function(x) all(is.na(x))))
      if(length(na_rows)!=0){
        data_no_na <- data[-na_rows, ]
      }else{
        data_no_na<-data
      }
      pc <- scores(pcaMethods::pca(data_no_na, nPcs = npcs, method = "bpca",scale=scale,center=center))
      
      for(i in 1:npcs){
        tmp_col<-paste0("PC",i)
        data[[tmp_col]]<-NA
        if(length(na_rows)!=0){
          data[[tmp_col]][-na_rows] <- pc[,i]
        }else{
          data[[tmp_col]] <- pc[,i]
        }
      }

      # if more that half of the correlations between PC and orig vars is negative, then *-1
      for(i in 1:npcs){
        tmp_col<-paste0("PC",i)
        corMat<-ifelse(cor(data,use="complete.obs")>0,1,0)
        tmp_mat_row<-which(rownames(corMat)==tmp_col)
        n_pos<-sum(corMat[tmp_mat_row,1:n_var]) 
        if(n_pos/n_var<.5){
          data[[tmp_col]]<-data[[tmp_col]]*-1
        }
      }
      
  }
  
  if(method=="psych"){
      na_rows <- which(apply(data, 1, function(x) all(is.na(x))))
      data_no_na <- data[-na_rows, ]
      imputed <- completeObs(pcaMethods::pca(data_no_na, nPcs = npcs, method = "bpca",scale=scale,center=center))
      
      pc<-psych::pca(imputed,npcs=npcs)$scores
      
      for(i in 1:npcs){
        tmp_col<-paste0("PC",i)
        data[[tmp_col]]<-NA
        data[[tmp_col]][-na_rows] <- pc[,i]
      }
  }
  
  

  return(data)
}
plot_posterior_diffs<-function(data,ci,par_prefix,rawpar_prefix,sig_prefix,par_name,group_nums,group_names,sample_sizes,round_digits,showtext){
  library(HDInterval)
  n_groups<-length(group_nums)

  #first define temp data frame to hold plot data based on number of groups
  column_names <- c("group", "mean", "hdi_hi", "hdi_lo")
  plot_data <- data.frame(matrix(NA, nrow = n_groups, ncol = length(column_names)))
  colnames(plot_data) <- column_names

  #define mean and ci of each groups posterior
  for (i in 1:n_groups){
    plot_data$group[i]<-group_names[i]
    plot_data$mean[i]<-mean(data[[paste0(par_prefix,i,".")]])
    plot_data$hdi_lo[i]<-HDInterval::hdi(data[[paste0(par_prefix,i,".")]],credMass = ci)[1]
    plot_data$hdi_hi[i]<-HDInterval::hdi(data[[paste0(par_prefix,i,".")]],credMass = ci)[2]
  }

  #now calculate pairwise group differences
  combinations_nums<-as.data.frame(t(combn(group_nums, 2))) #unique combinations of groups
  combinations_names<-as.data.frame(t(combn(group_names, 2))) #unique combinations of groups

  for(i in 1:nrow(combinations_nums)){
    group1_id<-combinations_nums[i,1]
    group2_id<-combinations_nums[i,2]
    group1_name<-combinations_names[i,1]
    group2_name<-combinations_names[i,2]
    group1_size<-sample_sizes[which(group_names==group1_name)]
    group2_size<-sample_sizes[which(group_names==group2_name)]

    diff_posterior<-data[[paste0(par_prefix,group1_id,".")]]-data[[paste0(par_prefix,group2_id,".")]]
    diff_mean<-round(mean(diff_posterior),round_digits)
    diff_hdi_lo<-as.numeric(HDInterval::hdi(diff_posterior,credMass=ci)[1])
    diff_hdi_hi<-as.numeric(HDInterval::hdi(diff_posterior,credMass=ci)[2])

    group1_rawpar<-data[[paste0(rawpar_prefix,group1_id,".")]]
    group2_rawpar<-data[[paste0(rawpar_prefix,group2_id,".")]]
      
    if(is.na(sig_prefix)){
      cohen_d<-NA
    }else{
      #calculate pooled SD for cohen's d effect sizes
      group1_sigma<-data[[paste0(sig_prefix,group1_id,".")]]
      group2_sigma<-data[[paste0(sig_prefix,group2_id,".")]]

      sigma_pooled <- sqrt(((mean(group1_sigma)^2 + mean(group2_sigma)^2) / 2))#Cohen 1988
      cohen_d<-round(mean(group1_rawpar-group2_rawpar)/sigma_pooled,round_digits)
    }

    if((diff_hdi_hi>0&diff_hdi_lo>0)|(diff_hdi_hi<0&diff_hdi_lo<0)){
      cred_index<-'*'
    }else{
      cred_index<-''
    }

    diff_hdi_lo<-round(diff_hdi_lo,round_digits)
    diff_hdi_hi<-round(diff_hdi_hi,round_digits)
    
    savage_dickey_ratio<-function(posterior,prior,method){ 
  
  # logspline kernel density estimation (as in Wagenmakers 2010)
  if(method=="logspline"){
    library(logspline)
    
    log_spline_density<-invisible(logspline(posterior,silent=TRUE,error.action = 1)) #density estimate for posterior
    if(is.null(log_spline_density)==FALSE){
      posterior_density<-dlogspline(0, log_spline_density, log = F)  #density @ 0
      log_spline_density<-invisible(logspline(prior,silent=TRUE)) #density estimate for prior
      prior_density<-dlogspline(0, log_spline_density, log = F)  #density @ 0
      
    }else{
      library(MASS)
      
      # Fit normal distribution to posterior samples
      fit_posterior <- fitdistr(posterior, "normal")
      posterior_mean <- fit_posterior$estimate[1]
      posterior_sd <- fit_posterior$estimate[2]
      
      # Laplace approximation Bayes Factor calculation using likelihood ratio at 0 (mode of the normal distribution)
      posterior_density <- dnorm(0, mean = posterior_mean, sd = posterior_sd)
      prior_density <- dnorm(0, mean = 0, sd = 1)
    }
    
    # a group effect of 0.09 is ~11 times LESS likely than it was before seeing the data (i.e., 1/.09)
    bf<-prior_density/posterior_density
  }else if(method=="kde"){
    library(stats)
    
    kde_density1 <- density(posterior)
    posterior_density <- approx(kde_density1$x, kde_density1$y, xout = 0)$y
    
    # if density too low to be estimated, then use the smallest representable number that R allows
    if(is.na(posterior_density)){
      posterior_density<-.Machine$double.xmin
    }
    
    kde_density2 <- density(prior)
    prior_density <- approx(kde_density2$x, kde_density2$y, xout = 0)$y
    bf<-prior_density/posterior_density
  }
  
  #direction of BF evidence
  if(bf==1){
    bf_direction<-"No Evidence"
  }else if(bf<1){
    bf_direction<-"Null"
  }else{
    bf_direction<-"Alt"
  }
  
  # strength of bayes factor evidence. ranges from Lee and Wagenmakers (2014)
  if((bf >= 100)){
    bf_range <- "Extreme"
  }else if(bf >= 30 && bf < 100){
    bf_range <- "Very Strong"
  }else if(bf >= 10 && bf < 30){
    bf_range <- "Strong"
  }else if(bf >= 3 && bf < 10){
    bf_range <- "Moderate"
  }else if(bf >= 1 && bf < 3){
    bf_range <- "Anecdotal"
  }else if(bf == 1){
    bf_range <- "No Evidence"
  }else if(bf >= 0.33 && bf < 1){
    bf_range <- "Anecdotal"
  }else if(bf >= 0.1 && bf < 0.33){
    bf_range <- "Moderate"
  }else if(bf >= 0.03 && bf < 0.1){
    bf_range <- "Strong"
  }else if(bf >= 0.01 && bf < 0.03){
    bf_range <- "Very Strong"
  }else if(bf < 0.01){
    bf_range <- "Extreme"
  }else{
    bf_range <- ""
    
  }
  combine<-list(bf=bf,
                bf_range=bf_range,
                bf_direction=bf_direction)
  
  return(combine)
}

    bf<-round(savage_dickey_ratio(group1_rawpar-group2_rawpar,rnorm(length(diff_posterior),0,1),"logspline")[["bf"]],round_digits)
    
    temp_text<-paste0(group1_name,"-",group2_name,": [",diff_hdi_lo,",",diff_hdi_hi,"], M = ",diff_mean,cred_index,"\nBF = ",bf,", d = ",cohen_d)

    #if not the last comparison, add a line break to the text
    if(i != nrow(combinations_nums)){
      temp_text<-paste0(temp_text,"\n")
    }
    if(i==1){
      text<-temp_text
    }else{
      text<-paste0(text,temp_text)
    }
  }

  #after we're done with all the combinations, make the plot and return it

  if(min(plot_data$hdi_lo)<0){
    yaxis_min<-min(plot_data$hdi_lo)*1.1
  }else{
    yaxis_min<-min(plot_data$hdi_lo)-(min(plot_data$hdi_lo)*.1)
  }

  if(max(plot_data$hdi_hi)<0){
    ytext_position<-max(plot_data$hdi_hi)-(max(plot_data$hdi_hi)*.1)
    yaxis_max<-max(plot_data$hdi_hi)-(max(plot_data$hdi_hi)*.15)
  }else{
    ytext_position<-max(plot_data$hdi_hi)+(max(plot_data$hdi_hi)*.1)
    yaxis_max<-max(plot_data$hdi_hi)+(max(plot_data$hdi_hi)*.15)
  }

  xtext_position<-mean(group_nums)

  if(showtext==FALSE){
    text<-""
  }

  p<-ggplot(data=plot_data, aes(x=group, y=mean,color=group,alpha=.4)) +
  geom_point(data=plot_data,alpha=1,position=position_dodge(width = .5),size=4) + theme_bw()+
  geom_errorbar(data=plot_data,aes(ymin=hdi_lo,ymax=hdi_hi),linewidth=1,
                width=0,alpha=1,position=position_dodge(width = .5)) +
  coord_cartesian(ylim = c(yaxis_min,yaxis_max))+
  ggtitle(par_name) + xlab("") +
  scale_x_discrete(limits=group_names)+
  scale_color_manual(values=c('#57b6c9','#966acc'))+
  ylab(par_name)+
  theme(axis.text.x = element_text(angle = 0,vjust=1,hjust=0.5,size=10),
        axis.text.y = element_text(size=10))+
  annotate(geom = "text", x = xtext_position, y = ytext_position, label = text, hjust = "center",size=3.5,lineheight=.9)
  p
  return(p)

}
custom_scatter <- function(data, mapping, ...) {
  ggplot(data, mapping) +
    geom_point(aes(color = group), alpha = 0.5, size = 1) + # Semi-transparent points
    geom_smooth(aes(color = group), method = "lm", se = FALSE, 
                linetype = "solid", size = 0.8, alpha = 0.6) + # Semi-transparent trend lines
    scale_color_manual(
      values = c("HC" = "#1f78b4", "SZ" = "#33a02c"), # Softer colors
      name = "Group" # Legend title
    ) +
    theme(legend.position = "bottom") 
}
BFanova_summary<-function(group,dv,credMass){
  
  library(BayesFactor)
  library(bayestestR)
  
  tmp_data<-data.frame(iv=group,dv=dv)
  tmp_data<-na.omit(tmp_data)
  tmp_groups<-unique(tmp_data$iv)
  
  #anova output 
  result<-anovaBF(dv~iv,data=tmp_data,progress=FALSE) #calculate anova BF
  output<-describe_posterior(result, ci=credMass, centrality = "mean",ci_method="HDI") #get summary
  group_bf<-round(exp(result@bayesFactor[["bf"]]),2) #have to exp transform bc bayesFactor is in log space
  
  #direction of BF evidence
  if(group_bf==1){
    group_bf_direction<-"No Evidence"
  }else if(group_bf>1){
    group_bf_direction<-"Alt."
  }else{
    group_bf_direction<-"Null"
  }
  
  # strength of bayes factor evidence. ranges from Lee and Wagenmakers (2014)
  if((group_bf >= 100)){
    group_bf_range <- "Extreme"
  }else if(group_bf >= 30 && group_bf < 100){
    group_bf_range <- "Very Strong"
  }else if(group_bf >= 10 && group_bf < 30){
    group_bf_range <- "Strong"
  }else if(group_bf >= 3 && group_bf < 10){
    group_bf_range <- "Moderate"
  }else if(group_bf >= 1 && group_bf < 3){
    group_bf_range <- "Anecdotal"
  }else if(group_bf == 1){
    group_bf_range <- "No Evidence"
  }else if(group_bf >= 0.33 && group_bf < 1){
    group_bf_range <- "Anecdotal"
  }else if(group_bf >= 0.1 && group_bf < 0.33){
    group_bf_range <- "Moderate"
  }else if(group_bf >= 0.03 && group_bf < 0.1){
    group_bf_range <- "Strong"
  }else if(group_bf >= 0.01 && group_bf < 0.03){
    group_bf_range <- "Very Strong"
  }else if(group_bf < 0.01){
    group_bf_range <- "Extreme"
  }else{
    group_bf_range <- ""
  }
  
  apa_format<-paste("BF=",group_bf," (",group_bf_direction,"/",group_bf_range,")",sep="")
  
  ### POST HOC TESTS
  
  # Get unique levels of the factor
  unique_levels <- unique(tmp_data$iv)
  
  # Create all unique combinations, conver to string, filter out duplicate combinations
  combinations <- t(combn(unique_levels, 2))
  combinations_strings <- apply(combinations, 1, paste, collapse = "")
  combinations_filtered <- data.frame(combo=combinations_strings[!duplicated(combinations_strings)])
  
  combinations_filtered$group1 <- substr(combinations_filtered$combo, 1, 1)
  combinations_filtered$group2 <- substr(combinations_filtered$combo, 2, 2)
  combinations_filtered<-combinations_filtered[,-1]
  
  factor_levels <- levels(tmp_data$iv) # Get levels of group factor
  
  for (j in 1:nrow(combinations_filtered)){
    
    groupidx1<-as.numeric(combinations_filtered$group1[j])
    groupidx2<-as.numeric(combinations_filtered$group2[j])
    
    group1<-factor_levels[groupidx1]
    group2<-factor_levels[groupidx2]
    
    result <- ttestBF(tmp_data$dv[tmp_data$iv == group1], tmp_data$dv[tmp_data$iv == group2])
    output<-describe_posterior(result, ci=credMass, centrality = "mean",ci_method="HDI") #get summary
    
    post_bf<-round(output$BF[which(output$Parameter=="Difference")],2)
    post_ci_lo<-round(output$CI_lo[which(output$Parameter=="Difference")],2)
    post_ci_hi<-round(output$CI_hi[which(output$Parameter=="Difference")],2)
    post_ci<-paste("[",post_ci_lo,", ",post_ci_hi,"]",sep="")
    post_mean<-round(output$Mean[which(output$Parameter=="Difference")],2)
    
    if((post_ci_lo>0&&post_ci_hi>0)|(post_ci_lo<0&&post_ci_hi<0)){ #if difference is credible
      if(post_mean<0){
        tmp_operator<-"<"
      }else{
        tmp_operator<-">"
      }
    }else{
      tmp_operator<-"≈"
    }
    
    posthoc_output<-paste(group1,tmp_operator,group2,sep="")
    
    if(j==1){
      posthoc_output_all<-posthoc_output
    }else{
      posthoc_output_all<-paste(posthoc_output_all,posthoc_output,sep="; ")
    }
  }
  
  anova_output<-list(apa_format,posthoc_output_all)
  return(anova_output)
}
chisq_summary<-function(group1,group2){
  
  tmp_data <- table(group1, group2)
  chi_sq_result <- chisq.test(tmp_data)
    
  x_value <- chi_sq_result$statistic
  p_value <- chi_sq_result$p.value
  df <- chi_sq_result$parameter
  sig<-ifelse(p_value<.05,'*',ifelse(p_value<.01,'**',ifelse(p_value<.001,"***","")))
  apa_format <- paste("X^2(", df, ")=", round(x_value,2), ", p=", round(p_value,3),sig, sep = "")

  # posthoc tests
  if(sig==""){
    posthoc_output_all<-"--"
  }else{
    posthoc <- pairwise.prop.test(tmp_data,p.adjust.method="none")
    p_values <- posthoc$p.value
    col_names<-colnames(p_values)
    row_names<-rownames(p_values)
  
    for (i in 1:length(row_names)){#post hoc pairwise
     for (j in 1:length(col_names)){
       
       grp1<-row_names[i]
       grp2<-col_names[j]
       
         if(grp1!=grp2){
           if(p_values[i,j]!='NaN'|!is.na(p_values[i,j])){
             if(p_values[i,j]>.05){
              tmp_operator<-""
             }else{
              tmp_operator<-"*"
             }
             posthoc_output<-paste(grp1,"vs",grp2,tmp_operator,sep="")
             if(i==1&j==1){
               posthoc_output_all<-posthoc_output
             }else{
               posthoc_output_all<-paste(posthoc_output_all,posthoc_output,sep="; ")
             }
           }
         }
       }
     } 
  }
  
  
  chisq_output<-list(apa_format,posthoc_output_all)
  return(chisq_output)
}
BFchisq_summary<-function(group1,group2,credMass){
  
  library(BayesFactor)
  library(bayestestR)
  
  tmp_data<-data.frame(group1=group1,group2=group2)
  tmp_data<-na.omit(tmp_data)
  
  tmp_data <- table(tmp_data$group1, tmp_data$group2)
  
  #calculate chi-squared(ish) BF
  result<-contingencyTableBF(tmp_data,sampleType="indepMulti",fixedMargin="rows",progress=FALSE) 
  output<-describe_posterior(result, ci=credMass, centrality = "mean",ci_method="HDI") #get summary
  
  group_bf<-round(exp(result@bayesFactor[["bf"]]),2) ##need to exp transform bc BF is in log space
  
  #direction of BF evidence
  if(group_bf==1){
    group_bf_direction<-"No Evidence"
  }else if(group_bf>1){
    group_bf_direction<-"Alt."
  }else{
    group_bf_direction<-"Null"
  }
  
  # strength of bayes factor evidence. ranges from Lee and Wagenmakers (2014)
  if((group_bf >= 100)){
    group_bf_range <- "Extreme"
  }else if(group_bf >= 30 && group_bf < 100){
    group_bf_range <- "Very Strong"
  }else if(group_bf >= 10 && group_bf < 30){
    group_bf_range <- "Strong"
  }else if(group_bf >= 3 && group_bf < 10){
    group_bf_range <- "Moderate"
  }else if(group_bf >= 1 && group_bf < 3){
    group_bf_range <- "Anecdotal"
  }else if(group_bf == 1){
    group_bf_range <- "No Evidence"
  }else if(group_bf >= 0.33 && group_bf < 1){
    group_bf_range <- "Anecdotal"
  }else if(group_bf >= 0.1 && group_bf < 0.33){
    group_bf_range <- "Moderate"
  }else if(group_bf >= 0.03 && group_bf < 0.1){
    group_bf_range <- "Strong"
  }else if(group_bf >= 0.01 && group_bf < 0.03){
    group_bf_range <- "Very Strong"
  }else if(group_bf < 0.01){
    group_bf_range <- "Extreme"
  }else{
    group_bf_range <- ""
  }
  
  apa_format<-paste("BF=",group_bf," (",group_bf_direction,"/",group_bf_range,")",sep="")
  
  ### POST HOC TESTS
  
  #recombine data
  tmp_data<-data.frame(group1=group1,group2=group2)
  tmp_data<-na.omit(tmp_data)
  
  # Get unique levels of the factor
  unique_levels <- unique(tmp_data$group1)
  
  # Create all unique combinations, conver to string, filter out duplicate combinations
  combinations <- t(combn(unique_levels, 2))
  combinations_strings <- apply(combinations, 1, paste, collapse = "")
  combinations_filtered <- data.frame(combo=combinations_strings[!duplicated(combinations_strings)])
  
  combinations_filtered$group1 <- substr(combinations_filtered$combo, 1, 1)
  combinations_filtered$group2 <- substr(combinations_filtered$combo, 2, 2)
  combinations_filtered<-combinations_filtered[,-1]
  
  factor_levels <- levels(tmp_data$group1) # Get levels of group factor
  
  for (j in 1:nrow(combinations_filtered)){
    
    groupidx1<-as.numeric(combinations_filtered$group1[j])
    groupidx2<-as.numeric(combinations_filtered$group2[j])
    
    group1<-factor_levels[groupidx1]
    group2<-factor_levels[groupidx2]
    
    result <- ttestBF(tmp_data$group2[tmp_data$group1 == group1], tmp_data$group2[tmp_data$group1 == group2])
    output<-describe_posterior(result, ci=credMass, centrality = "mean",ci_method="HDI") #get summary
    
    post_bf<-round(output$BF[which(output$Parameter=="Difference")],2)
    post_ci_lo<-round(output$CI_lo[which(output$Parameter=="Difference")],2)
    post_ci_hi<-round(output$CI_hi[which(output$Parameter=="Difference")],2)
    post_ci<-paste("[",post_ci_lo,", ",post_ci_hi,"]",sep="")
    post_mean<-round(output$Mean[which(output$Parameter=="Difference")],2)
    
    if((post_ci_lo>0&&post_ci_hi>0)|(post_ci_lo<0&&post_ci_hi<0)){ #if difference is credible
      if(post_mean<0){
        tmp_operator<-"<"
      }else{
        tmp_operator<-">"
      }
    }else{
      tmp_operator<-"≈"
    }
    
    posthoc_output<-paste(group1,tmp_operator,group2,sep="")
    
    if(j==1){
      posthoc_output_all<-posthoc_output
    }else{
      posthoc_output_all<-paste(posthoc_output_all,posthoc_output,sep="; ")
    }
  }
  
  chisq_output<-list(apa_format,posthoc_output_all)
  return(chisq_output)
}

bf_footnote<-"* = Credible association (i.e., 95% HDI does not contain zero); Mean = mean posterior estimate of correlation coefficient (equivalent to Pearson R); HDI = 95% HDI of the correlation coefficient; BF = Bayes Factor; BF_Evidence_Strength =  Bayes Factor interpretation scheme based on Lee and Wagenmakers (2014); BF_Evidence_Favors = whether evidence favors the null hypothesis (i.e., the association between variable 1 and 2 is 0) or the alternative hypothesis (i.e., an association between variable 1 and 2 is not 0); N = Number of subjects with complete data included in a given test."

########################################################
# PREPROCESS DATA 
########################################################

#if processed BEHAVIORAL Eye Contact data doesnt exist, process it now
outfile<-paste(datadir,"/schizgaze12_gaze_beh.csv",sep="")
if(file.exists(outfile)==FALSE){

  data2<-paste(datadir,'SG2_gaze_beh.xlsx',sep="")
  sheet_names2 <- excel_sheets(data2)
  
  iteration<-1
  for (sheet in sheet_names2) {
    tmp_data <- data.frame(read_excel(data2, sheet = sheet))
    tmp_data$subj<-sheet #add subj id
    
    tmp_data<-dplyr::select(tmp_data,c(subj,GazeAngle:EyesEndorse))
    if(iteration==1){
      alldata2<-tmp_data
    }else{
      alldata2<-rbind(alldata2,tmp_data)
    }
    iteration<-iteration+1
  }
  
  alldata2$Study<-'schizgaze2'
  
  #exclude 2032, male SZ patient that gave the same response for the entire task
  alldata2<-subset(alldata2,alldata2$subj!=2032)
  #exclude 2042 (only completed half of the task)
  alldata2<-subset(alldata2,alldata2$subj!=2042)
  
  ##combine studies
  #alldata<-rbind(alldata1,alldata2)
  alldata<-alldata2
  
  #remove fixation cross rows
  alldata<-subset(alldata,alldata$Task!="Fixation")
  
  #recode responses.
  ## gender: 2=male, 3=female
  ## gaze (eye contact): 2=yes,3=no
  
  alldata$GazeSlide.RESP <- ifelse(alldata$GazeSlide.RESP == 2, "Y", alldata$GazeSlide.RESP)
  alldata$GazeSlide.RESP <- ifelse(alldata$GazeSlide.RESP == 3, "N", alldata$GazeSlide.RESP)
  alldata$GenderSlide.RESP <- ifelse(alldata$GenderSlide.RESP == 2, "M", alldata$GenderSlide.RESP)
  alldata$GenderSlide.RESP <- ifelse(alldata$GenderSlide.RESP == 3, "F", alldata$GenderSlide.RESP)
  alldata$GenderCorrectResp <- ifelse(alldata$GenderCorrectResp == 2, "M", alldata$GenderCorrectResp)
  alldata$GenderCorrectResp <- ifelse(alldata$GenderCorrectResp == 3, "F", alldata$GenderCorrectResp)
  
  #insert new columns
  alldata$Resp<-alldata$RT<-alldata$CorrectResp<-alldata$Acc<-NA
  
  #code responses into one column
  alldata$Resp<-ifelse(alldata$Task=="Eyes",alldata$GazeSlide.RESP,alldata$Resp)
  alldata$Resp<-ifelse(alldata$Task=="GenderID",alldata$GenderSlide.RESP,alldata$Resp)
  
  #code RT's into one column
  alldata$RT<-ifelse(alldata$Task=="Eyes",alldata$GazeSlide.RT,alldata$RT)
  alldata$RT<-ifelse(alldata$Task=="GenderID",alldata$GenderSlide.RT,alldata$RT)
  
  #code correct responses into one column (gender only)
  alldata$CorrectResp<-ifelse(alldata$Task=="GenderID",alldata$GenderCorrectResp,alldata$CorrectResp)
  
  #code accuracy responses into one column (gender only)
  alldata$Acc<-ifelse(alldata$Task=="GenderID",alldata$GenderAcc,alldata$GenderAcc)
  
  #remove non response trials
  alldata<-subset(alldata,!is.na(alldata$Resp))
  
  alldata1<-subset(alldata,alldata$Task=="Eyes")
  
  alldata<-subset(alldata,alldata$RT>=200&alldata$RT<=4000)
  
  #code response boundary (for ddm; yes and female = upper (1) / no and male = lower (2) )
  alldata$RespBound<-ifelse(alldata$Resp=="Y"|alldata$Resp=="F",1,2)
  alldata$Subj<-alldata$subj
  
  #code diagnostic group
  ## schizgaze2 = 1000s=HC, 2000s= psychosis
  alldata$Group<-alldata$subj
  alldata$Group <- ifelse(alldata$Study=="schizgaze2"&(substr(alldata$Group, 1, 1) == "1"), "HC",alldata$Group)
  alldata$Group <- ifelse(alldata$Study=="schizgaze2"&(substr(alldata$Group, 1, 1) == "2"), "SZ",alldata$Group)
  
  #subset to include only relevant columns
  alldata<-dplyr::select(alldata,c(Study,Group,Subj,GazeAngle,Task,Gender,Resp,RespBound,RT,CorrectResp,Acc))
  outfile<-paste(datadir,"/schizgaze12_gaze_beh.csv",sep="")
  write.csv(alldata,outfile,row.names = FALSE)
  
}else{
  alldata<-read.csv(outfile)
}

# get all subjects with behavioral data
tmp<-subset(alldata,alldata$Study=="schizgaze2")
sg2_subjs<-data.frame(subj=unique(tmp$Subj))

#if processed BEHAVIORAL Visual Integration data doesnt exist, process it now
outfile<-paste(datadir,"/schizgaze2_jovi_beh.csv",sep="")
if(file.exists(outfile)==FALSE){
  
  data2<-paste(datadir,'SG2_jovi_beh.xlsx',sep="")
  sheet_names2 <- excel_sheets(data2)
  
  iteration<-1
  for (sheet in sheet_names2) {

    tmp_data <- data.frame(read_excel(data2, sheet = sheet))
    tmp_data$subj<-sheet #add subj id
    
    tmp_data<-dplyr::select(tmp_data,c(subj,Run:RT))
    if(iteration==1){
      alldata2<-tmp_data
    }else{
      alldata2<-rbind(alldata2,tmp_data)
    }
    iteration<-iteration+1
  }
  
  alldata2$Study<-'schizgaze2'

#remove non-responses (where rt=0) and trials with RTs <200ms
alldata2<-subset(alldata2,alldata2$RT>=200)
alldata2<-subset(alldata2,alldata2$RT!=0)

alldata2<-alldata2[!is.na(alldata2$RT), ]
alldata2$Group<-ifelse(alldata2$subj>1999,"SZ","HC")
alldata2<-subset(alldata2,alldata2$TrialType=="Real")
alldata2$RT<-alldata2$RT/1000
alldata2$RespBound<-ifelse(alldata2$Response=="2",1,2)
alldata2$Direction<-ifelse(alldata2$Direction=="L",1,2)

alldata<-data.frame(Study=alldata2$Study,
                    Group=alldata2$Group,
                    Subj=alldata2$subj,
                    Jitter=alldata2$Jitter,
                    Direction=alldata2$Direction,
                    Resp=alldata2$Response,
                    RespBound=alldata2$RespBound,
                    RT=alldata2$RT,
                    Acc=alldata2$ACC)

alldata <- alldata[order(alldata$Subj), ]

alldata<-subset(alldata,alldata$Subj!="2032")
alldata<-subset(alldata,alldata$Subj!="2042")

for(i in 1:nrow(alldata)){
    if(alldata$Subj[i]=="2057"){
      alldata$Resp[i]<-ifelse(alldata$Resp[i]==2,3,2)
      alldata$RespBound[i]<-ifelse(alldata$RespBound[i]==1,2,1)
      alldata$Acc[i]<-ifelse(alldata$Direction[i]==alldata$RespBound[i],1,0)
    }
  }

alldata$SubjID<-match(alldata$Subj, unique(alldata$Subj)) 

alldata$Jitter<-(max(alldata$Jitter)+1)-alldata$Jitter
alldata$Jitter<-ifelse(alldata$Direction==1,alldata$Jitter,alldata$Jitter*-1) #code strength AND direction
alldata$Jitter<-(alldata$Jitter-mean(alldata$Jitter))/sd(alldata$Jitter)

#remove non response trials and trials that exceeded time limit
alldata<-subset(alldata,!is.na(alldata$Resp))
alldata<-subset(alldata,alldata$RT>=0.2&alldata$RT<=2.5)

outfile<-paste(datadir,"/schizgaze2_jovi_beh.csv",sep="")
write.csv(alldata,outfile,row.names = FALSE)
  
}else{
  alldata<-read.csv(outfile)
}

# #if processed BEHAVIORAL jovi data doesnt exist, process it now
# outfile<-paste(datadir,"/schizgaze2_jovi_beh.csv",sep="")
# if(file.exists(outfile)==FALSE){
#   
#   data2<-paste(datadir,'SG2_jovi_beh.xlsx',sep="")
#   sheet_names2 <- excel_sheets(data2)
#   
#   iteration<-1
#   for (sheet in sheet_names2) {
# 
#     tmp_data <- data.frame(read_excel(data2, sheet = sheet))
#     tmp_data$subj<-sheet #add subj id
#     
#     tmp_data<-dplyr::select(tmp_data,c(subj,Run:RT))
#     if(iteration==1){
#       alldata2<-tmp_data
#     }else{
#       alldata2<-rbind(alldata2,tmp_data)
#     }
#     iteration<-iteration+1
#   }
#   
#   alldata2$Study<-'schizgaze2'
#   
#   #exclude 2032, male SZ patient that gave the same response for the entire GAZE task
#   alldata2<-subset(alldata2,alldata2$subj!=2032)
#   #exclude 2042 (only completed half of the GAZE task)
#   alldata2<-subset(alldata2,alldata2$subj!=2042)
# 
# #remove non-responses (where rt=0) and trials with RTs <200ms
# alldata2<-subset(alldata2,alldata2$RT>=200)
# alldata2<-subset(alldata2,alldata2$RT!=0)
# 
# alldata2<-alldata2[!is.na(alldata2$RT), ]
# alldata2$Group<-ifelse(alldata2$subj>1999,"SZ","HC")
# alldata2<-subset(alldata2,alldata2$TrialType=="Real")
# alldata2$RT<-alldata2$RT/1000
# alldata2$RespBound<-ifelse(alldata2$Response=="2",1,2)
# alldata2$Direction<-ifelse(alldata2$Direction=="L",1,2)
# 
# alldata<-data.frame(Study=alldata2$Study,
#                     Group=alldata2$Group,
#                     Subj=alldata2$subj,
#                     Jitter=alldata2$Jitter,
#                     Direction=alldata2$Direction,
#                     Resp=alldata2$Response,
#                     RespBound=alldata2$RespBound,
#                     RT=alldata2$RT,
#                     Acc=alldata2$ACC)
# 
# alldata$Jitter<-(max(alldata$Jitter)+1)-alldata$Jitter
# alldata$Jitter<-ifelse(alldata$Direction==1,alldata$Jitter,alldata$Jitter*-1) #code strength AND direction
# alldata$Jitter<-(alldata$Jitter-mean(alldata$Jitter))/sd(alldata$Jitter)
# 
# #remove non response trials and trials that exceeded time limit
# alldata<-subset(alldata,!is.na(alldata$Resp))
# alldata<-subset(alldata,alldata$RT>=0.2&alldata$RT<=2.5)
# 
# outfile<-paste(datadir,"/schizgaze2_jovi_beh.csv",sep="")
# write.csv(alldata,outfile,row.names = FALSE)
#   
# }else{
#   alldata<-read.csv(outfile)
# }

#if processed CORRELATES data doesnt exist, process it now
outfile<-paste(datadir,"/schizgaze2_preproc_correlates.csv",sep="")

data<-paste(datadir,'schizgaze2_raw_correlates.xlsx',sep="")
sheet_names <- excel_sheets(data)
iteration<-1

for (sheet in sheet_names) {
  
  tmp_data <- data.frame(read_excel(data, sheet = sheet))#load temp data sheet
  tmp_combine<-merge(sg2_subjs, tmp_data,all.x=TRUE,by.x = names(sg2_subjs), by.y = "subj",no.dups = T)
  
  if(iteration==1){
    sg2_correlates<-tmp_combine
  }else{
    
    sg2_correlates<-cbind(sg2_correlates,tmp_combine)
  }
  iteration<-iteration+1
}

#get rid of extra subject columns
tmp_col<-grep("subj", colnames(sg2_correlates),fixed = TRUE) 
sg2_correlates<-sg2_correlates[,-tmp_col[-1]]

# Replace "NA" with NA 
sg2_correlates[sg2_correlates == "NA"] <- NA

sg2_correlates$has_beh_data<-0

for(i in 1:nrow(sg2_subjs)){
  sg2_correlates$has_beh_data[which(sg2_correlates$subj==sg2_subjs$subj[i])]<-1
}

sg2_correlates<-subset(sg2_correlates,sg2_correlates$has_beh_data==1&sg2_correlates$exclude==0)

colnames(sg2_correlates)<-tolower(colnames(sg2_correlates))

# extract matrics principal component(exclude social cognition scale to avoid overlap w/ MSCEIT)
tmp_data<-data.frame(sop=as.numeric(sg2_correlates$matrics_sop_agt),
                     av=as.numeric(sg2_correlates$matrics_av_agt),
                     wm=as.numeric(sg2_correlates$matrics_wm_agt),
                     verl=as.numeric(sg2_correlates$matrics_verl_agt),
                     visl=as.numeric(sg2_correlates$matrics_visl_agt),
                     reas=as.numeric(sg2_correlates$matrics_reasprsolv_agt))

sg2_correlates$matrics<-pcamethods_bpca(data=tmp_data,npcs=1,scale="uv",center=TRUE,method="pcamethods")$PC1

sg2_correlates$africanamer<-ifelse(sg2_correlates$race=="AfricanAmer",1,0)
sg2_correlates$africanamer<-ifelse(is.na(sg2_correlates$race),NA,sg2_correlates$africanamer)

sg2_correlates$nativeamer<-ifelse(sg2_correlates$race=="NativeAmer",1,0)
sg2_correlates$nativeamer<-ifelse(is.na(sg2_correlates$race),NA,sg2_correlates$nativeamer)

sg2_correlates$white<-ifelse(sg2_correlates$race=="White",1,0)
sg2_correlates$white<-ifelse(is.na(sg2_correlates$race),NA,sg2_correlates$white)

sg2_correlates$asian<-ifelse(sg2_correlates$race=="Asian",1,0)
sg2_correlates$asian<-ifelse(is.na(sg2_correlates$race),NA,sg2_correlates$asian)

sg2_correlates$other<-ifelse(sg2_correlates$race=="Other",1,0)
sg2_correlates$other<-ifelse(is.na(sg2_correlates$race),NA,sg2_correlates$other)

sg2_correlates$multi<-ifelse(sg2_correlates$race=="Multi",1,0)
sg2_correlates$multi<-ifelse(is.na(sg2_correlates$race),NA,sg2_correlates$multi)

sg2_correlates$hispanic<-ifelse(sg2_correlates$ethnicity=="Hispanic",1,0)
sg2_correlates$hispanic<-ifelse(is.na(sg2_correlates$ethnicity),NA,sg2_correlates$hispanic)

sg2_correlates$schizoaffective<-ifelse(sg2_correlates$psychosis_dx=="Schizoaffective",1,0)
sg2_correlates$schizoaffective<-ifelse(is.na(sg2_correlates$psychosis_dx),NA,sg2_correlates$schizoaffective)

sg2_correlates$schizophrenia<-ifelse(sg2_correlates$psychosis_dx=="Schizophrenia",1,0)
sg2_correlates$schizophrenia<-ifelse(is.na(sg2_correlates$psychosis_dx),NA,sg2_correlates$schizophrenia)

sg2_correlates$schizophreniform<-ifelse(sg2_correlates$psychosis_dx=="Schizophreniform",1,0)
sg2_correlates$schizophreniform<-ifelse(is.na(sg2_correlates$psychosis_dx),NA,sg2_correlates$schizophreniform)

# extract social cognition principal component from er40/rme/msceit data with missing values 

#remove and impute er40 and rme for 2057 and 2072 for rme only who performed at chance levels

  sg2_correlates$rme_accuracy[which(sg2_correlates$subj=="2057")]<-NA
  sg2_correlates$rme_rt_word_correct[which(sg2_correlates$subj=="2057")]<-NA
  sg2_correlates$rme_rt_average_word[which(sg2_correlates$subj=="2057")]<-NA
  sg2_correlates$er40_correctresp[which(sg2_correlates$subj=="2057")]<-NA
  sg2_correlates$er40_correctrt[which(sg2_correlates$subj=="2057")]<-NA
  
  sg2_correlates$rme_accuracy[which(sg2_correlates$subj=="2072")]<-NA
  sg2_correlates$rme_rt_word_correct[which(sg2_correlates$subj=="2072")]<-NA
  sg2_correlates$rme_rt_average_word[which(sg2_correlates$subj=="2072")]<-NA

tmp_data<-data.frame(msceit=as.numeric(sg2_correlates$msceit),
                     er40=as.numeric(sg2_correlates$er40_correctresp),
                     er40_rt=as.numeric(sg2_correlates$er40_correctrt),
                     rme=as.numeric(sg2_correlates$rme_accuracy))

sg2_correlates$soc_cog_pc_wrt<-pcamethods_bpca(data=tmp_data,npcs=1,scale="uv",center=T,method="pcamethods")$PC1

write.csv(sg2_correlates,outfile,row.names = F)

```

# Model Implementation

Initial fits of all models were run on a high performance computing cluster using 1500 warmup samples, 1000 postwarmup draws per chain, 10 chains, resulting in 10000 postwarmup draws in cmdstanr. These "initial fits" were used as the basis for all model evaluation steps described below. 

For analyses, each winning model was run using a larger number of samples (1500 warmup samples, 5000 postwarmup draws per chain, 10 chains, resulting in 50000 postwarmup draws). This ensured that our inferences were based on stable posteriors for all parameters.

# Model Specification

__Naming conventions:__ Model 2 (Eye Contact) is the primary model for the eye contact detection task described in the main text, Model 3 (Gender) is the primary model for the gender identification task described in the main text, and Model 5 (Visual Integration) is the model Visual Integration task described in the main text. Note that Model 2 (Eye Contact) and Model 5 (Visual Integration) share the same structure.

__Note on gender model parameterization:__ For the gender models in which gender was allowed to categorically modulate drift rates (Gender Model 3 [winning model from main text] and 4), we originally estimated separate drift rates for male and female faces. To recast those parameters to match the parameterization of the winning Eye Contact Model (Eye Contact: Model 2), which is equivalent to the effects-coding approach described in the main text, we calculated drift sensitivity and drift biases in the gender identification task as follows:

Drift Sensitivity (Gender)* = (Drift Rate<sup>Male</sup> - Drift Rate<sup>Female</sup>)/2
Drift Bias (Gender) = (Drift Rate<sup>Male</sup> + Drift Rate<sup>Female</sup>)/2

As such, the model evaluation steps are performed on the gender models with separate male/female drift rate parameters. It is for the analyses that we recast those parameters in the gender identification task to drift sensitivity (discriminability) and drift bias (as shown above).

## Overview

More complete details of the specification of the winning models is presented in the next section (under "Model Comparisons"). 

```{r id12433333422222, echo=FALSE, fig.height=5, fig.show="hold", fig.width=5, message=FALSE, warning=FALSE, cache=TRUE, dpi=500, out.width='1500px'}
#
model_table<-read.csv("/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/manuscript/figures and tables/all_models.csv")
  
model_table$Condition<-""

model_table %>%
  kbl(caption = "Specifications of All Models Tested",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  pack_rows("Eye Contact Detection", 1, 2) %>%
  pack_rows("Gender Identification", 3, 6) %>%
  pack_rows("Visual Integration", 7, 7) %>%
  kable_styling(full_width=T)

```
  
# Model Comparison

## Eye Contact Models

For the eye contact detection task, the winning model ("Model 2: Eye Contact") assumed that eye contact signals continuously modulated evidence accumulation. This allowed the model to capture two aspects of the evidence accumulation process. Drift sensitivity indexed how evidence accumulation was continuously modulated by physical changes in Eye Contact signals. Those with higher values are more attuned to eye contact signals, as they better translate stronger eye contact signals into more efficient decision-making. Drift bias captured information processing bias for self-referential/non-self-referential decisions, regardless of physical changes in eye contact signals. As “looking at me” and “not looking at me” responses were coded as the top and bottom boundaries, respectively, drift bias > 0 indicates a information processing bias for self-referential decisions, and drift bias < 0 indicates a information processing bias for non-self-referential decisions. The DDM assumes that the remaining processes—threshold separation (response caution), start point (expectancy bias), and NDT (motor/sensory processes)—are set before the stimulus appears. This means that they can only vary based on stimulus features that can be anticipated before the stimulus onset. As such, threshold separation, start point, and NDT operated as trait-level processes that did not vary based on changes in eye contact, which varied trial-by-trial. 
 
```{r id3924321387324, echo=FALSE, cache=TRUE}

outpath <- '/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_'
models<-c('m1_psychophys','m2_psychophys')
groupnames<-c('HC','SZ')
loo_headers<-c('group','model','elpd_loo','delta_elpd','delta_elpd_se')
all_loo<-data.frame(matrix(data=NA,nrow=length(models),ncol=length(loo_headers)))
colnames(all_loo)<-loo_headers

for(h in 1:length(groupnames)){
  group<-h
  groupname<-groupnames[h]
  all_loo_objects<-list() #list to hold loo_psis objects for each model
  
  for (i in 1:length(models)){
    outfile<-paste0(outpath,models[i],"/gaze/log_lik/loglik_group",group,".RData")
    load(outfile)
    all_loo$group<-groupname
    all_loo$model[i]<-models[i]
    all_loo_objects[[models[i]]]<-loo_2
    all_loo$elpd_loo[i]<-round(data.frame(loo_2[["loo"]][["estimates"]])[1,1],2)
  }
  
  #sort table by from lowest to highest loo val
  all_loo <- all_loo[order(all_loo$elpd_loo,decreasing=TRUE),]
  
  #calculate elpd loo diff 
  for (i in 1:length(models)){
    tmp_model<-all_loo$model[i] #current model
    tmp_loo_object<-all_loo_objects[[tmp_model]][["loo"]]
    best_model<-all_loo$model[1] #best fitting model
    best_loo_object<-all_loo_objects[[best_model]][["loo"]]
    if (i > 1){
      compare<-data.frame(loo_compare(tmp_loo_object,best_loo_object))
      all_loo$delta_elpd[i]<-round(compare$elpd_diff[2],2)
      all_loo$delta_elpd_se[i]<-round(compare$se_diff[2],2)
      
    }else{
      all_loo$delta_elpd[i]<-"--"
      all_loo$delta_elpd_se[i]<-"--"
    }
  }
  if(h==1){
    all_loo_combined<-all_loo
  }else{
    all_loo_combined<-rbind(all_loo_combined,all_loo)
  }
}

rownames(all_loo_combined)<-NULL

all_loo_combined$model<-gsub("_psychophys","",all_loo_combined$model)
all_loo_combined$model<-gsub("m","Model ",all_loo_combined$model)
all_loo_combined$model<-paste0(all_loo_combined$model,": Eye Contact")

all_loo_combined %>%
  kbl(caption = "Eye Contact Model Comparison",valign = "t") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  kable_styling(full_width=T) %>%
  footnote("Models are presented in order from best- to worst-fitting based on the expected log pointwise predictive density within each group ('elpd_loo'). Higher ELPD indicates better model fit. Differences in out-of-sample predictive accuracy were assessed relative to the best fitting model based on changes in the ELPD ('delta_elpd'). Uncertainty around ∆ELPD-LOO is captured through the standard error ('delta_elpd_se') of the estimated pointwise ELPD differences. An absolute change in ELPD < 1 SE is weak evidence for improved predictive accuracy.")

```
 
## Gender Models

For the gender identification task, the winning model (“Model 3: Gender”) was one in which evidence accumulation (drift rate) was influenced by the gender of stimuli, but not other task-irrelevant social cues (i.e., Eye Contact). The effect of gender on evidence accumulation was captured in the original models via separate drift rates for male and female stimuli (drift rate<sup>gend-M</sup> and drift rate<sup>gend-F</sup>). As “male” responses were coded as the top boundary and “female” responses were coded as the bottom boundary, higher values for drift rate<sup>gend-M</sup> and lower values for drift rate<sup>gend-F</sup> reflected more efficient evidence accumulation. 

We then recast these parameters into the same parameterization as the eye contact detection model described in the main text, even though gender was manipulated categorically. To achieve this, when calculating trial-level drift sensitivity in the gender identification task using the equation in Figure 1 (main text), we replaced the Eye Contact signal on each trial i with the effects-coded gender of the stimulus on each trial (1=male, -1=female). As a result, during gender identification, drift sensitivity captured the categorical modulation of drift rates by gender, where larger values indicated greater discriminability between male and female faces. Although drift sensitivity parameters were highly similar, they capture slightly different processes: in the gender identification task, they capture evidence accumulation rates and sensitivity to changes in gender cues; in the Eye Contact and Visual Integration tasks, they are a purer measure of sensitivity to changes in signal strength. Additionally, during gender identification, the drift bias reflected a information processing bias for male/female. As “male” and “female” responses were coded as the top and bottom boundaries, respectively, drift bias > 0 indicates a information processing bias for male, and drift bias < 0 indicates a information processing bias for female. All other parameters were the same as in the Eye Contact Model. 

This 'effects coding approach' is equivalent to the following calculations of drift sensitivity and drift bias for the gender identification task, which is how we calculated these parameters in our case:

Drift Sensitivity (Gender)* = (Drift Rate<sup>Male</sup> - Drift Rate<sup>Female</sup>)/2
Drift Bias (Gender) = (Drift Rate<sup>Male</sup> + Drift Rate<sup>Female</sup>)/2
 
```{r id392422321387324, echo=FALSE, cache=TRUE}

outpath <- '/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_'
models<-c('m1_psychophys','m2_psychophys','m3_psychophys','m4_psychophys')
groupnames<-c('HC','SZ')
loo_headers<-c('group','model','elpd_loo','delta_elpd','delta_elpd_se')
all_loo<-data.frame(matrix(data=NA,nrow=length(models),ncol=length(loo_headers)))
colnames(all_loo)<-loo_headers

for(h in 1:length(groupnames)){
  group<-h
  groupname<-groupnames[h]
  
  all_loo_objects<-list() #list to hold loo_psis objects for each model
  
  for (i in 1:length(models)){
    
    outfile<-paste0(outpath,models[i],"/gender/log_lik/loglik_group",group,".RData")
    
    load(outfile)
    all_loo$group<-groupname
    all_loo$model[i]<-models[i]
    all_loo_objects[[models[i]]]<-loo_2
    all_loo$elpd_loo[i]<-round(data.frame(loo_2[["loo"]][["estimates"]])[1,1],2)
    
  }
  
  #sort table by from lowest to highest loo val
  all_loo <- all_loo[order(all_loo$elpd_loo,decreasing=TRUE),]
  
  #calculate elpd loo diff 
  for (i in 1:length(models)){
    
    tmp_model<-all_loo$model[i] #current model
    tmp_loo_object<-all_loo_objects[[tmp_model]][["loo"]]
    
    best_model<-all_loo$model[1] #best fitting model
    best_loo_object<-all_loo_objects[[best_model]][["loo"]]
    
    if (i > 1){
      compare<-data.frame(loo_compare(tmp_loo_object,best_loo_object))
      all_loo$delta_elpd[i]<-round(compare$elpd_diff[2],2)
      all_loo$delta_elpd_se[i]<-round(compare$se_diff[2],2)
      
    }else{
      all_loo$delta_elpd[i]<-"--"
      all_loo$delta_elpd_se[i]<-"--"
    }
  }
  
  if(h==1){
    all_loo_combined<-all_loo
  }else{
    all_loo_combined<-rbind(all_loo_combined,all_loo)
  }

}

rownames(all_loo_combined)<-NULL
all_loo_combined$model<-gsub("_psychophys","",all_loo_combined$model)
all_loo_combined$model<-gsub("m","Model ",all_loo_combined$model)
all_loo_combined$model<-paste0(all_loo_combined$model,": Gender")

all_loo_combined %>%
  kbl(caption = "Gender Model Comparison",valign = "t") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  kable_styling(full_width=T) %>%
  footnote("Models are presented in order from best- to worst-fitting based on the expected log pointwise predictive density within each group ('elpd_loo'). Higher ELPD indicates better model fit. Differences in out-of-sample predictive accuracy were assessed relative to the best fitting model based on changes in the ELPD ('delta_elpd'). Uncertainty around ∆ELPD-LOO is captured through the standard error ('delta_elpd_se') of the estimated pointwise ELPD differences. An absolute change in ELPD < 1 SE is weak evidence for improved predictive accuracy.")

```

## Visual Integration Models

For the Visual Integration task, since the task structure was similar to the Eye Contact Models, we did not perform model comparisons for data from the Visual Integration task. Instead, we made the reasonable assumption that the efficiency of evidence accumulation on the Visual Integration task would be influenced by the jitter level of the stimuli.
As such, for the Visual Integration, we used a model (Model 5) with the same structure as Model 2 of the Eye Contact Detection Task.

## Correlations Between Model Parameters {.tabset}

### Full Sample

```{r id9249, echo=FALSE, fig.height=8, fig.width=8.5, message=FALSE, warning=FALSE, cache=TRUE}
#
all_sub_data<-read.csv("/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/data/schizgaze2_all_sub_params_w_correlates.csv")

tmp_pars<-c('delta_bias','delta_bias_gend','delta_bias_jovi',
            'delta_b1','delta_gend','delta_b1_jovi',
            'beta','beta_gend','beta_jovi',
            'alpha','alpha_gend','alpha_jovi')

tmp_corr_data<-all_sub_data[tmp_pars]

colnames(tmp_corr_data)<-rename_corr_pars(colnames(tmp_corr_data))
p<-bayes_corr_matrix(tmp_corr_data,credMass=.95)
p[["plot"]]

```

### SZ Only

```{r id92491, echo=FALSE, fig.height=8, fig.width=8.5, message=FALSE, warning=FALSE, cache=TRUE}
#
temp<-subset(all_sub_data,all_sub_data$group=="SZ")
tmp_pars<-c('delta_bias','delta_bias_gend','delta_bias_jovi',
            'delta_b1','delta_gend','delta_b1_jovi',
            'beta','beta_gend','beta_jovi',
            'alpha','alpha_gend','alpha_jovi')

tmp_corr_data<-all_sub_data[tmp_pars]
colnames(tmp_corr_data)<-rename_corr_pars(colnames(tmp_corr_data))
p<-bayes_corr_matrix(tmp_corr_data,credMass=.95)
p[["plot"]]

```

### HC Only

```{r id92491221, echo=FALSE, fig.height=8, fig.width=8.5, message=FALSE, warning=FALSE, cache=TRUE}
#
temp<-subset(all_sub_data,all_sub_data$group=="HC")
tmp_pars<-c('delta_bias','delta_bias_gend','delta_bias_jovi',
            'delta_b1','delta_gend','delta_b1_jovi',
            'beta','beta_gend','beta_jovi',
            'alpha','alpha_gend','alpha_jovi')

tmp_corr_data<-all_sub_data[tmp_pars]
colnames(tmp_corr_data)<-rename_corr_pars(colnames(tmp_corr_data))
p<-bayes_corr_matrix(tmp_corr_data,credMass=.95)
p[["plot"]]

```







# Convergence

We assessed convergence for all models by ensuring trace plots showed well-mixed chains, Rhat values were < 1.1 for all parameters, autocorrelation was ~0 by a lag of 30, and there were no divergences. This was true of all parameters for all models tested. For brevity, we only show the convergence check outputs below for the 2 winning models.

## Model 2 (Eye Contact) {.tabset}

Convergence tests for Eye Contact Model described in the main text.

### ESS/Rhat

```{r id342809374873412, echo=FALSE, cache=TRUE}
##
gaze2_all_samples<-read.csv('/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_m2_psychophys/gaze/init_fit/allgroups_init_fit_samples.csv')
chain_id<-gaze2_all_samples$.chain
n_chains<-length(unique(chain_id))
n_iter<-nrow(gaze2_all_samples)/n_chains

gaze_samples<-gaze2_all_samples[,-grep("beta_pr|delta_pr|alpha_pr|ndt_pr|absent_pr|present_pr|draw|iteration|lp|chain",colnames(gaze2_all_samples))]
params<-colnames(gaze_samples)

for(i in 1:length(params)){
  tmp_data<-gaze_samples[,i]
  tmp_matrix<-matrix(tmp_data,nrow=n_iter,ncol=n_chains,byrow=FALSE)
  tmp_summary<-data.frame(
      parameter=params[i],
      mean=round(mean(tmp_matrix),2),
      sd=round(sd(tmp_matrix),2),
      rhat=round(posterior::rhat(tmp_matrix),2),
      bulk_ess=round(posterior::ess_bulk(tmp_matrix),0),
      tail_ess=round(posterior::ess_tail(tmp_matrix),0))
    if(i==1){
      fit_summary<-tmp_summary
    }else{
      fit_summary<-rbind(fit_summary,tmp_summary)
    }
}

params<-rename_gaze_pars(fit_summary$parameter)
fit_summary$parameter<-params
fit_summary

```

### Trace Plots

```{r id211123123122231231, echo=FALSE, fig.height=1.75, fig.width=3.5, message=FALSE, warning=FALSE, cache=TRUE, tidy=TRUE}

tmp_gaze_samples<-gaze_samples[,-grep("^sub_", colnames(gaze_samples)) ]
params<-rename_gaze_pars(colnames(tmp_gaze_samples))
bayesplot_theme_set(theme_default(base_size = 8, base_family = "sans"))

for (i in 1:ncol(tmp_gaze_samples)){
  tmp_data<-tmp_gaze_samples[,i]
  tmp_matrix<-matrix(tmp_data,nrow=n_iter,ncol=n_chains,byrow=FALSE) #make matrix (draws*chains)
  post_draws<-simplify2array(list(tmp_matrix))
  dimnames(post_draws)<-list(Iteration=seq(from=1,to=n_iter,by=1),
                             Chain=seq(from=1,to=n_chains,by=1),
                             Parameter=params[i])
  title<-paste("Model 2 (Eye Contact):\n", params[i], sep="")
  p<-mcmc_trace(post_draws,pars=params[i])+legend_none()+ggtitle(title)+ ylab("Estimate")
  print(p)
}

```

### Autocorrelation

Note: We originally examined all 10 chains for autocorrelation. But here we randomly select 6 and display those (for brevity)

```{r id22312312312212, echo=FALSE, fig.height=2.5, fig.width=3.5, message=FALSE, warning=FALSE, cache=TRUE, tidy=TRUE}

chain_nums<-seq(from=1,to=n_chains,by=1)
subsample_chains<-6 #chains to subsample and display autocorr for
bayesplot_theme_set(theme_default(base_size = 8, base_family = "sans"))

for (i in 1:ncol(tmp_gaze_samples)){
  tmp_data<-tmp_gaze_samples[,i]
  tmp_matrix<-matrix(tmp_data,nrow=n_iter,ncol=n_chains,byrow=FALSE) #make this back into matrix (#draws * #chains)
  rand_chains<-sample(seq(from=1,to=n_chains,by=1),subsample_chains,replace=FALSE) #randomly sample some chains
  tmp_matrix<-tmp_matrix[,rand_chains] #get only data for those chains
  post_draws<-simplify2array(list(tmp_matrix))
  dimnames(post_draws)<-list(Iteration=seq(from=1,to=n_iter,by=1),
                             Chain=rand_chains,
                             Parameter=params[i])
  title<-paste("Model 2 (Eye Contact): Autocorrelation\n", params[i], sep="")
  p<-mcmc_acf(post_draws[,,1], lags=30)+ggtitle(title)
  print(p)
}

rm(list=c('tmp_gaze_samples'))
invisible(gc())

```

## Model 3 (Gender) {.tabset}

Convergence tests for gender model described in the main text.

### ESS/Rhat

```{r id342, echo=FALSE, cache=TRUE}
##
gender3_all_samples<-read.csv('/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_m3_psychophys/gender/init_fit/allgroups_init_fit_samples.csv')
chain_id<-gender3_all_samples$.chain
n_chains<-length(unique(chain_id))
n_iter<-nrow(gender3_all_samples)/n_chains

gender_samples<-gender3_all_samples[,-grep("beta_pr|delta_pr|alpha_pr|ndt_pr|absent_pr|present_pr|draw|iteration|lp|chain",colnames(gender3_all_samples))]
params<-colnames(gender_samples)

for(i in 1:length(params)){
  tmp_data<-gender_samples[,i]
  tmp_matrix<-matrix(tmp_data,nrow=n_iter,ncol=n_chains,byrow=FALSE)
  tmp_summary<-data.frame(
      parameter=params[i],
      mean=round(mean(tmp_matrix),2),
      sd=round(sd(tmp_matrix),2),
      rhat=round(posterior::rhat(tmp_matrix),2),
      bulk_ess=round(posterior::ess_bulk(tmp_matrix),0),
      tail_ess=round(posterior::ess_tail(tmp_matrix),0))
    if(i==1){
      fit_summary<-tmp_summary
    }else{
      fit_summary<-rbind(fit_summary,tmp_summary)
    }
}

params<-rename_gender_pars(fit_summary$parameter)
fit_summary$parameter<-params
fit_summary

```

### Trace Plots

```{r id2231, echo=FALSE, fig.height=1.75, fig.width=3.5, message=FALSE, warning=FALSE, cache=TRUE, tidy=TRUE}

tmp_gender_samples<-gender_samples[,-grep("^sub_", colnames(gender_samples)) ]
params<-rename_gender_pars(colnames(tmp_gender_samples))

bayesplot_theme_set(theme_default(base_size = 8, base_family = "sans"))
for (i in 1:ncol(tmp_gender_samples)){
  tmp_data<-tmp_gender_samples[,i]
  tmp_matrix<-matrix(tmp_data,nrow=n_iter,ncol=n_chains,byrow=FALSE) #make matrix (draws*chains)
  post_draws<-simplify2array(list(tmp_matrix))
  dimnames(post_draws)<-list(Iteration=seq(from=1,to=n_iter,by=1),
                             Chain=seq(from=1,to=n_chains,by=1),
                             Parameter=params[i])
  title<-paste("Model 3 (Gender):\n", params[i], sep="")
  p<-mcmc_trace(post_draws,pars=params[i])+legend_none()+ggtitle(title)+ ylab("Estimate")
  print(p)
}

```

### Autocorrelation

Note: We originally examined all 10 chains for autocorrelation. But here we randomly select 6 and display those (for brevity)

```{r id22311222212, echo=FALSE, fig.height=2.5, fig.width=3, message=FALSE, warning=FALSE, cache=TRUE, tidy=TRUE}

chain_nums<-seq(from=1,to=n_chains,by=1)
subsample_chains<-6 #of of chains to subsample and display autocorr for
bayesplot_theme_set(theme_default(base_size = 8, base_family = "sans"))

for (i in 1:ncol(tmp_gender_samples)){
  tmp_data<-tmp_gender_samples[,i]
  tmp_matrix<-matrix(tmp_data,nrow=n_iter,ncol=n_chains,byrow=FALSE) #make matrix (#draws * #chains)
  rand_chains<-sample(seq(from=1,to=n_chains,by=1),subsample_chains,replace=FALSE) #randomly sample some chains
  tmp_matrix<-tmp_matrix[,rand_chains] #get only data for those chains
  post_draws<-simplify2array(list(tmp_matrix))
  dimnames(post_draws)<-list(Iteration=seq(from=1,to=n_iter,by=1),
                             Chain=rand_chains,
                             Parameter=params[i])
  title<-paste("Model 3 (Gender): Autocorrelation\n", params[i], sep="")
  p<-mcmc_acf(post_draws[,,1], lags=30)+ggtitle(title)
  print(p)
}

rm(list=c('tmp_gender_samples'))
invisible(gc())

```
 
## Model 5 (Visual Integration) {.tabset}

Convergence tests for Visual Integration model described in the main text.

### ESS/Rhat

```{r id32242, echo=FALSE, cache=TRUE}

jovi_all_samples<-read.csv('/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_m5_psychophys/init_fit/allgroups_init_fit_samples.csv')
chain_id<-jovi_all_samples$.chain
n_chains<-length(unique(chain_id))
n_iter<-nrow(jovi_all_samples)/n_chains

jovi_samples<-jovi_all_samples[,-grep("beta_pr|delta_pr|alpha_pr|ndt_pr|_L_pr|_R_pr|draw|iteration|lp|chain",colnames(jovi_all_samples))]
params<-colnames(jovi_samples)

for(i in 1:length(params)){
  tmp_data<-jovi_samples[,i]
  tmp_matrix<-matrix(tmp_data,nrow=n_iter,ncol=n_chains,byrow=FALSE)
  tmp_summary<-data.frame(
      parameter=params[i],
      mean=round(mean(tmp_matrix),2),
      sd=round(sd(tmp_matrix),2),
      rhat=round(posterior::rhat(tmp_matrix),2),
      bulk_ess=round(posterior::ess_bulk(tmp_matrix),0),
      tail_ess=round(posterior::ess_tail(tmp_matrix),0))
    if(i==1){
      fit_summary<-tmp_summary
    }else{
      fit_summary<-rbind(fit_summary,tmp_summary)
    }
}

params<-rename_jovi_pars(fit_summary$parameter)
fit_summary$parameter<-params
fit_summary

```

### Trace Plots

```{r id22231, echo=FALSE, fig.height=1.75, fig.width=3.5, message=FALSE, warning=FALSE, cache=TRUE, tidy=TRUE}

tmp_jovi_samples<-jovi_samples[,-grep("^sub_", colnames(jovi_samples)) ]
params<-rename_jovi_pars(colnames(tmp_jovi_samples))

bayesplot_theme_set(theme_default(base_size = 8, base_family = "sans"))
for (i in 1:ncol(tmp_jovi_samples)){
  tmp_data<-tmp_jovi_samples[,i]
  tmp_matrix<-matrix(tmp_data,nrow=n_iter,ncol=n_chains,byrow=FALSE) #make matrix (draws*chains)
  post_draws<-simplify2array(list(tmp_matrix))
  dimnames(post_draws)<-list(Iteration=seq(from=1,to=n_iter,by=1),
                             Chain=seq(from=1,to=n_chains,by=1),
                             Parameter=params[i])
  title<-paste("Model 5 (Visual Integration):\n", params[i], sep="")
  p<-mcmc_trace(post_draws,pars=params[i])+legend_none()+ggtitle(title)+ ylab("Estimate")
  print(p)
}

```

### Autocorrelation

Note: We originally examined all 10 chains for autocorrelation. But here we randomly select 6 and display those (for brevity)

```{r id22311212, echo=FALSE, fig.height=2.5, fig.width=3, message=FALSE, warning=FALSE, cache=TRUE, tidy=TRUE}

chain_nums<-seq(from=1,to=n_chains,by=1)
subsample_chains<-6 #of of chains to subsample and display autocorr for
bayesplot_theme_set(theme_default(base_size = 8, base_family = "sans"))

for (i in 1:ncol(tmp_jovi_samples)){
  tmp_data<-tmp_jovi_samples[,i]
  tmp_matrix<-matrix(tmp_data,nrow=n_iter,ncol=n_chains,byrow=FALSE) #make matrix (#draws * #chains)
  rand_chains<-sample(seq(from=1,to=n_chains,by=1),subsample_chains,replace=FALSE) #randomly sample some chains
  tmp_matrix<-tmp_matrix[,rand_chains] #get only data for those chains
  post_draws<-simplify2array(list(tmp_matrix))
  dimnames(post_draws)<-list(Iteration=seq(from=1,to=n_iter,by=1),
                             Chain=rand_chains,
                             Parameter=params[i])
  title<-paste("Model 5 (Visual Integration): Autocorrelation\n", params[i], sep="")
  p<-mcmc_acf(post_draws[,,1], lags=30)+ggtitle(title)
  print(p)
}

rm(list=c('tmp_jovi_samples'))
invisible(gc())

```
 




# Parameter Recovery

Parameter recovery was performed for all winning models to determine if parameter values used to simulate choice and RT data could be accurately recovered.

__Procedure:__ For each model, for each group, for each of N = 50 simulations, data of n = 35 subjects were simulated by sampling parameters from distributions matching the means and standard deviations (SD) of the group-level posteriors for each model parameter. This was done using 35 subjects per group because it was slightly less than the number of subjects we had per group, operating under the assumption that if we could recover the parameters in fewer subjects, then we definitely could in more. Sampled parameters were then used to simulate trial-level choice and RT data for each subject using ‘rwiener’ in RWiener R package. The number of trials simulated matched the specifications for each task 
(i.e., 12 trials per signal strength in the Eye Contact Detection Task; 54 trials per gender in the Gender Identification Task; 20 trials per signal strength/direction in the Visual Integration task). These data were then fitted to the winning model to extract simulated parameter values. Correspondence between the generating parameter values and the simulated parameter values was compared by determining the percentage of group-level and subject-level simulated parameters whose 95% highest density interval (HDI) contained the original generating value. Note: NDT was fixed at 0.2 for parameter recovery analyses and scaled the SD of the sampled distribution down slightly to avoid getting extreme, implausible values in our simulated data.

__Results:__ All models showed good posterior coverage for parameters across SZ and HC at the group- (88-100% recovery) and subject-level (88-96% recovery). Recovery of individual differences in parameters at the subject-level (i.e., Pearson R between generating and recovered values) was slightly lower for drift sensitivity<sup>eye_contact</sup> (r’s =0.62-0.73) in the Eye Contact Detection Task than drift rate(s)<sup>gend-M/F</sup> (r’s =0.68-0.79) in the Gender Identification Task, drift sensitivity<sup>jovi</sup> (r’s =0.62-0.73) in the Visual Integration task (r’s =0.9-0.92), and other parameters (r’s=0.77-0.97). However, it was still considered acceptable.

## Model 2 (Eye Contact)

### Group-Level Parameters (Averaged over simulations)

First, we examined how well the group-level generating values were recovered by simulated models fits *on average*. We averaged over the posterior means and 95% HDIs for group-level parameters for all 50 simulations; plotted generating group-level values (red X's in plots below) superimposed over the averaged mean and 95% HDI (points and error bars in figure below); and assessed how well, on average, the group-level generating values were captured by simulated model fits. 

Results below suggest that group level parameters were well-recovered on average because 1) the generating values (red X's in plots below) were contained within the average 95% HDI for all parameters (error bars in plots below), and 2) the generating values are closely aligned with the group-level posterior mean (data points in plots below) of simulated fits on average for all parameters.

```{r id189724310911, echo=FALSE, message=FALSE, warning=FALSE,fig.show="hold",fig.height=3, fig.width=3.25,cache=TRUE}

group_summary<-read.csv("/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_m2_psychophys/gaze/par_recover/parRecovery_groupLvl_summary.csv")

# average over recovered posterior means and HDI's of all simulations (within group)
plot_data<-group_summary %>%
  summarise_all(mean)

#ALPHA
plot_data2<-data.frame(group=c('HC','SZ'),
                       sim_mean=c(plot_data$alpha_grp_sim_mean1,plot_data$alpha_grp_sim_mean2),
                       gen_mean=c(plot_data$alpha_grp_gen1,plot_data$alpha_grp_gen2),
                       sim_hdi_lo=c(plot_data$alpha_grp_sim_hdi_lo1,plot_data$alpha_grp_sim_hdi_lo2),
                       sim_hdi_hi=c(plot_data$alpha_grp_sim_hdi_hi1,plot_data$alpha_grp_sim_hdi_hi2))

p <- ggplot(data = plot_data2, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data2, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data2, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Eye Contact: Threshold Separation\n(Averaged over 50 sims)") + 
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data2$group, value = plot_data2$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)

#BETA
plot_data2<-data.frame(group=c('HC','SZ'),
                       sim_mean=c(plot_data$beta_grp_sim_mean1,plot_data$beta_grp_sim_mean2),
                       gen_mean=c(plot_data$beta_grp_gen1,plot_data$beta_grp_gen2),
                       sim_hdi_lo=c(plot_data$beta_grp_sim_hdi_lo1,plot_data$beta_grp_sim_hdi_lo2),
                       sim_hdi_hi=c(plot_data$beta_grp_sim_hdi_hi1,plot_data$beta_grp_sim_hdi_hi2))


p <- ggplot(data = plot_data2, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data2, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data2, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Eye Contact: Start Point\n(Averaged over 50 sims)") + 
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data2$group, value = plot_data2$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)

#DELTA 1.1
plot_data2<-data.frame(group=c('HC','SZ'),
                       sim_mean=c(plot_data$delta_grp_sim_mean1,plot_data$delta_grp_sim_mean2),
                       gen_mean=c(plot_data$delta_grp_gen1,plot_data$delta_grp_gen2),
                       sim_hdi_lo=c(plot_data$delta_grp_sim_hdi_lo1,plot_data$delta_grp_sim_hdi_lo2),
                       sim_hdi_hi=c(plot_data$delta_grp_sim_hdi_hi1,plot_data$delta_grp_sim_hdi_hi2))


p <- ggplot(data = plot_data2, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data2, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data2, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Eye Contact: Drift Bias\n(Averaged over 50 sims)") + 
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data2$group, value = plot_data2$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)

#B1 (drift sensitivity Parameter)
plot_data2<-data.frame(group=c('HC','SZ'),
                       sim_mean=c(plot_data$b1_grp_sim_mean1,plot_data$b1_grp_sim_mean2),
                       gen_mean=c(plot_data$b1_grp_gen1,plot_data$b1_grp_gen2),
                       sim_hdi_lo=c(plot_data$b1_grp_sim_hdi_lo1,plot_data$b1_grp_sim_hdi_lo2),
                       sim_hdi_hi=c(plot_data$b1_grp_sim_hdi_hi1,plot_data$b1_grp_sim_hdi_hi2))

p <- ggplot(data = plot_data2, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data2, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data2, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Eye Contact: Drift Sensitivity\n(Averaged over 50 sims)") + 
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data2$group, value = plot_data2$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)


```

### Group-Level Parameters (Separate for each simulation)

Second, we examined group-level parameters for the individual simulations to determine what % of the 50 simulations successfully recovered the group-level generating parameter values (vertical black line in plots below). Recovery was "successful" if the 95% HDI of the fitted group-level posterior (error bars in plots below) for a given simulation contained the original subject-level generating value. Data in green in the plots below indicate the group-level parameters were recovered and data in red indicates they were not. This was done separately for each task.

Results below suggest that group level generating parameters were well---recovered because--across all tasks---the generating values (vertical black line in plots below) were contained within the average 95% HDI (error bars in plots below) in 88-100% of simulations for all parameters.

```{r id122189371829311, echo=FALSE, fig.height=5.5, fig.width=2.5, message=FALSE, warning=FALSE,fig.show="hold",cache=TRUE}

#define function to summarize group-level par recovery for separate simulations
## data = dataframe containing summary (one row per simulation) of parameter recovery at the group level. must include group level gen values for each param, simulated posterior mean for each param, the 95% HDI of the simulated posterior for each param, and a column indicating whether the generating value was recovered (1=yes, 0=no)
## par = parameter name in the posterior samples
## parname = how you would like the parameter name to be displayed on resulting plot
## genMean_suffix = suffix of generating mean in data
## simMean_suffix= suffix of simulated mean in data
## simHDI_suffix =  suffix of simulated HDI in data (excluding lo and hi)
## recover_suffix =  suffix in data df that indexing column of data indicating whether par was recovered or not

params<-c('alpha','beta','delta','b1')
parnames<-c('Eye Contact: Threshold Separation','Eye Contact: Start Point','Eye Contact: Drift Bias','Eye Contact: Drift Sensitivity')
groups<-c('HC','SZ')

group_summary$sim_id<-group_summary$sim

for (i in 1:length(groups)){
  for (j in 1:length(params)){
    temp_summary<-group_summary[,grep(paste0("sim|run|", i, "$"),colnames(group_summary))]
    colnames(temp_summary) <- gsub(paste0(i, "$"), "", colnames(temp_summary))
    temp_parname<-paste0(parnames[j]," (",groups[i],")")
    p<-plot_parrecover_group_bysim(data=temp_summary,
                                   par=params[j],
                                   parname=temp_parname,
                                   sim_id<-'sim_id',
                                   genMean_suffix<-'_grp_gen',
                                   simMean_suffix<-'_grp_sim_mean',
                                   simHDI_suffix<-'_grp_sim_hdi_',
                                   recover_suffix<-'_recover')
    print(p)
  }
}

```

### Subject-Level Parameters

Third, we examined how well subject-level generating values were captured by simulated model fits. We determined what % of subject-level generating parameter values were successfully recovered across the 50 simulations. Recovery was "successful" if the 95% HDI of the fitted subject-level posterior contained the original subject-level generating value. Data in green in the plots below indicate the subject-level generating values were recovered and data in red indicates they were not. This was done separately for each  task.

Results below suggest that subject level generating parameters were well-recovered because the generating values (x-axis) were contained within the average 95% HDI of fitted values (error bars in plots below) in 93-95% of simulated participants.

```{r id1313489273421, echo=FALSE, message=FALSE, warning=FALSE, tidy=TRUE,fig.show="hold",cache=TRUE,fig.height=3.5,fig.width=3.35}

subj_summary<-read.csv("/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_m2_psychophys/gaze/par_recover/parRecovery_subjLvl_summary.csv")

#define function to summarize subject-level par recovery for separate simulations
## data = dataframe containing summary (one row per subject per simulation) of parameter recovery at the SUBJECT level. must include subj level gen values for each param, simulated subj posterior mean for each param, the subj-level  95% HDI of the simulated posterior for each param, and a column indicating whether the generating value was recovered (1=yes, 0=no) for each subj for each sim.
## par = parameter name in the posterior samples
## parname = how you would like the parameter name to be displayed on resulting plot
## genMean_suffix = suffix of generating mean in data
## simMean_suffix= suffix of simulated mean in data
## simHDI_suffix =  suffix of simulated HDI in data (excluding lo and hi)
## recover_suffix =  suffix in data df that indexing column of data indicating whether par was recovered or not

params<-c('alpha','beta','delta','b1')
parnames<-c('Eye Contact: Threshold Separation','Eye Contact: Start Point','Eye Contact: Drift Bias','Eye Contact: Drift Sensitivity')
groups<-c('HC','SZ')
subj_summary$sim_id<-subj_summary$sim

for (i in 1:length(groups)){
  for (j in 1:length(params)){
    temp_summary<-subset(subj_summary,subj_summary$group==i)
    temp_parname<-paste0(parnames[j]," (",groups[i],")")
    p<-plot_parrecover_subj(data=temp_summary,
                                   par=params[j],
                                   parname=temp_parname,
                                   sim_id<-'sim_id',
                                   genMean_suffix<-'_sub_gen',
                                   simMean_suffix<-'_sub_sim_mean',
                                   simHDI_suffix<-'_sub_sim_hdi_',
                                   recover_suffix<-'_recover')
    print(p)
  }
}

```




## Model 3 (Gender)

### Group-Level Parameters (Averaged over simulations)

Now we do the same for the gender models. First, we examined how well the group-level generating values were recovered by simulated models fits *on average*. We averaged over the posterior means and 95% HDIs for group-level parameters for all 50 simulations; plotted generating group-level values (red X's in plots below) superimposed over the averaged mean and 95% HDI (points and error bars in figure below); and assessed how well, on average, the group-level generating values were captured by simulated model fits. 

Results below suggest that group level parameters were well-recovered on average because 1) the generating values (red X's in plots below) were contained within the average 95% HDI for all parameters (error bars in plots below), and 2) the generating values are closely aligned with the group-level posterior mean (data points in plots below) of simulated fits on average for all parameters.

```{r id189724377710911, echo=FALSE, message=FALSE, warning=FALSE,fig.show="hold",fig.height=3, fig.width=3.25,cache=TRUE}

group_summary<-read.csv("/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_m3_psychophys/gender/par_recover/parRecovery_groupLvl_summary.csv")

# average over recovered posterior means and HDI's of all simulations (within group)
plot_data<-group_summary %>%
  summarise_all(mean)

#ALPHA
plot_data3<-data.frame(group=c('HC','SZ'),
                       sim_mean=c(plot_data$alpha_grp_sim_mean1,plot_data$alpha_grp_sim_mean2),
                       gen_mean=c(plot_data$alpha_grp_gen1,plot_data$alpha_grp_gen2),
                       sim_hdi_lo=c(plot_data$alpha_grp_sim_hdi_lo1,plot_data$alpha_grp_sim_hdi_lo2),
                       sim_hdi_hi=c(plot_data$alpha_grp_sim_hdi_hi1,plot_data$alpha_grp_sim_hdi_hi2))

p <- ggplot(data = plot_data3, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data3, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data3, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Gender: Threshold Sep\n(Averaged over 50 sims)") + 
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data3$group, value = plot_data3$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)

#BETA
plot_data3<-data.frame(group=c('HC','SZ'),
                       sim_mean=c(plot_data$beta_grp_sim_mean1,plot_data$beta_grp_sim_mean2),
                       gen_mean=c(plot_data$beta_grp_gen1,plot_data$beta_grp_gen2),
                       sim_hdi_lo=c(plot_data$beta_grp_sim_hdi_lo1,plot_data$beta_grp_sim_hdi_lo2),
                       sim_hdi_hi=c(plot_data$beta_grp_sim_hdi_hi1,plot_data$beta_grp_sim_hdi_hi2))


p <- ggplot(data = plot_data3, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data3, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data3, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Gender: Start Point\n(Averaged over 50 sims)") + 
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data3$group, value = plot_data3$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)

#DELTA female
plot_data3<-data.frame(group=c('HC','SZ'),
                       sim_mean=c(plot_data$delta_female_grp_sim_mean1,plot_data$delta_female_grp_sim_mean2),
                       gen_mean=c(plot_data$delta_female_grp_gen1,plot_data$delta_female_grp_gen2),
                       sim_hdi_lo=c(plot_data$delta_female_grp_sim_hdi_lo1,plot_data$delta_female_grp_sim_hdi_lo2),
                       sim_hdi_hi=c(plot_data$delta_female_grp_sim_hdi_hi1,plot_data$delta_female_grp_sim_hdi_hi2))


p <- ggplot(data = plot_data3, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data3, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data3, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Gender: Drift Sensitivity (Female)\n(Averaged over 50 sims)") + 
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data3$group, value = plot_data3$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)

#DELTA male
plot_data3<-data.frame(group=c('HC','SZ'),
                       sim_mean=c(plot_data$delta_male_grp_sim_mean1,plot_data$delta_male_grp_sim_mean2),
                       gen_mean=c(plot_data$delta_male_grp_gen1,plot_data$delta_male_grp_gen2),
                       sim_hdi_lo=c(plot_data$delta_male_grp_sim_hdi_lo1,plot_data$delta_male_grp_sim_hdi_lo2),
                       sim_hdi_hi=c(plot_data$delta_male_grp_sim_hdi_hi1,plot_data$delta_male_grp_sim_hdi_hi2))


p <- ggplot(data = plot_data3, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data3, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data3, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Gender: Drift Sensitivity (Male)\n(Averaged over 50 sims)") + 
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data3$group, value = plot_data3$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)

```

### Group-Level Parameters (Separate for each simulation)

Second, we examined group-level parameters for the individual simulations to determine what % of the 50 simulations successfully recovered the group-level generating parameter values (vertical black line in plots below). Recovery was "successful" if the 95% HDI of the fitted group-level posterior (error bars in plots below) for a given simulation contained the original subject-level generating value. Data in green in the plots below indicate the group-level parameters were recovered and data in red indicates they were not. This was done separately for each task.

Results below suggest that group level generating parameters were well---recovered because--across all   tasks---the generating values (vertical black line in plots below) were contained within the average 95% HDI (error bars in plots below) in 86-98% of simulations for all parameters.

```{r id122133893718229311, echo=FALSE, fig.height=5.5, fig.show="hold", fig.width=2.5, message=FALSE, warning=FALSE, cache=TRUE}

params<-c('alpha','beta','delta_female','delta_male')
parnames<-c('Gender: Threshold Sep','Gender: Start Point','Gender: Drift Female','Gender: Drift Male')
groups<-c('HC','SZ')

group_summary$sim_id<-group_summary$sim

for (i in 1:length(groups)){
  for (j in 1:length(params)){
    temp_summary<-group_summary[,grep(paste0("sim|run|", i, "$"),colnames(group_summary))]
    colnames(temp_summary) <- gsub(paste0(i, "$"), "", colnames(temp_summary))
    temp_parname<-paste0(parnames[j]," (",groups[i],")")
    p<-plot_parrecover_group_bysim(data=temp_summary,
                                   par=params[j],
                                   parname=temp_parname,
                                   sim_id<-'sim_id',
                                   genMean_suffix<-'_grp_gen',
                                   simMean_suffix<-'_grp_sim_mean',
                                   simHDI_suffix<-'_grp_sim_hdi_',
                                   recover_suffix<-'_recover')
    print(p)
  }
}

```

### Subject-Level Parameters

Third, we examined how well subject-level generating values were captured by simulated model fits. We determined what % of subject-level generating parameter values were successfully recovered across the 50 simulations. Recovery was "successful" if the 95% HDI of the fitted subject-level posterior contained the original subject-level generating value. Data in green in the plots below indicate the subject-level generating values were recovered and data in red indicates they were not. This was done separately for each  task.

Results below suggest that subject level generating parameters were well-recovered because the generating values (x-axis) were contained within the average 95% HDI of fitted values (error bars in plots below) in 93-95% of simulated participants.

```{r id131342289273421, echo=FALSE, message=FALSE, warning=FALSE, tidy=TRUE,fig.show="hold",cache=TRUE,fig.height=3.5,fig.width=3.35}

subj_summary<-read.csv("/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_m3_psychophys/gender/par_recover/parRecovery_subjLvl_summary.csv")
params<-c('alpha','beta','delta_female','delta_male')
parnames<-c('Gender: Threshold Sep','Gender: Start Point','Gender: Drift Female','Gender: Drift Male')
groups<-c('HC','SZ')
subj_summary$sim_id<-subj_summary$sim

for (i in 1:length(groups)){
  for (j in 1:length(params)){
    temp_summary<-subset(subj_summary,subj_summary$group==i)
    temp_parname<-paste0(parnames[j]," (",groups[i],")")
    p<-plot_parrecover_subj(data=temp_summary,
                                   par=params[j],
                                   parname=temp_parname,
                                   sim_id<-'sim_id',
                                   genMean_suffix<-'_sub_gen',
                                   simMean_suffix<-'_sub_sim_mean',
                                   simHDI_suffix<-'_sub_sim_hdi_',
                                   recover_suffix<-'_recover')
    print(p)
  }
}

```

## Model 5 (Visual Integration)

### Group-Level Parameters (Averaged over simulations)

First, we examined how well the group-level generating values were recovered by simulated models fits *on average*. We averaged over the posterior means and 95% HDIs for group-level parameters for all 50 simulations; plotted generating group-level values (red X's in plots below) superimposed over the averaged mean and 95% HDI (points and error bars in figure below); and assessed how well, on average, the group-level generating values were captured by simulated model fits. 

Results below suggest that group level parameters were well-recovered on average because 1) the generating values (red X's in plots below) were contained within the average 95% HDI for all parameters (error bars in plots below), and 2) the generating values are closely aligned with the group-level posterior mean (data points in plots below) of simulated fits on average for all parameters.

```{r id18972431033911, echo=FALSE, message=FALSE, warning=FALSE,fig.show="hold",fig.height=3, fig.width=3.25,cache=TRUE}
#
group_summary<-read.csv("/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_m5_psychophys/par_recover/parRecovery_groupLvl_summary.csv")

# average over recovered posterior means and HDI's of all simulations (within group)
plot_data<-group_summary %>%
  summarise_all(mean)

#ALPHA
plot_data2<-data.frame(group=c('HC','SZ'),
                       sim_mean=c(plot_data$alpha_grp_sim_mean1,plot_data$alpha_grp_sim_mean2),
                       gen_mean=c(plot_data$alpha_grp_gen1,plot_data$alpha_grp_gen2),
                       sim_hdi_lo=c(plot_data$alpha_grp_sim_hdi_lo1,plot_data$alpha_grp_sim_hdi_lo2),
                       sim_hdi_hi=c(plot_data$alpha_grp_sim_hdi_hi1,plot_data$alpha_grp_sim_hdi_hi2))

p <- ggplot(data = plot_data2, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data2, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data2, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Visual Integration: Threshold Separation\n(Averaged over 50 sims)") + 
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data2$group, value = plot_data2$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)

#BETA
plot_data2<-data.frame(group=c('HC','SZ'),
                       sim_mean=c(plot_data$beta_grp_sim_mean1,plot_data$beta_grp_sim_mean2),
                       gen_mean=c(plot_data$beta_grp_gen1,plot_data$beta_grp_gen2),
                       sim_hdi_lo=c(plot_data$beta_grp_sim_hdi_lo1,plot_data$beta_grp_sim_hdi_lo2),
                       sim_hdi_hi=c(plot_data$beta_grp_sim_hdi_hi1,plot_data$beta_grp_sim_hdi_hi2))


p <- ggplot(data = plot_data2, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data2, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data2, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Visual Integration: Start Point\n(Averaged over 50 sims)") + 
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data2$group, value = plot_data2$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)

#DELTA 
plot_data2<-data.frame(group=c('HC','SZ'),
                       sim_mean=c(plot_data$delta_grp_sim_mean1,plot_data$delta_grp_sim_mean2),
                       gen_mean=c(plot_data$delta_grp_gen1,plot_data$delta_grp_gen2),
                       sim_hdi_lo=c(plot_data$delta_grp_sim_hdi_lo1,plot_data$delta_grp_sim_hdi_lo2),
                       sim_hdi_hi=c(plot_data$delta_grp_sim_hdi_hi1,plot_data$delta_grp_sim_hdi_hi2))


p <- ggplot(data = plot_data2, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data2, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data2, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Visual Integration: Drift Bias\n(Averaged over 50 sims)") + 
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data2$group, value = plot_data2$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)

#B1 (Drift Sensitivity Parameter)
plot_data2<-data.frame(group=c('HC','SZ'),
                       sim_mean=c(plot_data$b1_grp_sim_mean1,plot_data$b1_grp_sim_mean2),
                       gen_mean=c(plot_data$b1_grp_gen1,plot_data$b1_grp_gen2),
                       sim_hdi_lo=c(plot_data$b1_grp_sim_hdi_lo1,plot_data$b1_grp_sim_hdi_lo2),
                       sim_hdi_hi=c(plot_data$b1_grp_sim_hdi_hi1,plot_data$b1_grp_sim_hdi_hi2))

p <- ggplot(data = plot_data2, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data2, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data2, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Visual Integration: Drift Sensitivity\n(Averaged over 50 sims)") + 
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data2$group, value = plot_data2$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)


```

### Group-Level Parameters (Separate for each simulation)

Second, we examined group-level parameters for the individual simulations to determine what % of the 50 simulations successfully recovered the group-level generating parameter values (vertical black line in plots below). Recovery was "successful" if the 95% HDI of the fitted group-level posterior (error bars in plots below) for a given simulation contained the original subject-level generating value. Data in green in the plots below indicate the group-level parameters were recovered and data in red indicates they were not. This was done separately for each task.

Results below suggest that group level generating parameters were well recovered. Across all tasks and parameters, the generating values (vertical black line in plots below) were contained within the 95% HDI (error bars in plots below) in 94-98% of simulations.

```{r id1221893718329311, echo=FALSE, fig.height=5.5, fig.width=2.5, message=FALSE, warning=FALSE,fig.show="hold",cache=TRUE}
#
params<-c('alpha','beta','delta','b1')
parnames<-c('Visual Integration: Threshold Sep','Visual Integration: Start Point','Visual Integration: Drift Bias','Visual Integration: Drift Sensitivity')
groups<-c('HC','SZ')

group_summary$sim_id<-group_summary$sim

for (i in 1:length(groups)){
  for (j in 1:length(params)){
    temp_summary<-group_summary[,grep(paste0("sim|run|", i, "$"),colnames(group_summary))]
    colnames(temp_summary) <- gsub(paste0(i, "$"), "", colnames(temp_summary))
    temp_parname<-paste0(parnames[j]," (",groups[i],")")
    p<-plot_parrecover_group_bysim(data=temp_summary,
                                   par=params[j],
                                   parname=temp_parname,
                                   sim_id<-'sim_id',
                                   genMean_suffix<-'_grp_gen',
                                   simMean_suffix<-'_grp_sim_mean',
                                   simHDI_suffix<-'_grp_sim_hdi_',
                                   recover_suffix<-'_recover')
    print(p)
  }
}

```

### Subject-Level Parameters

Third, we examined how well subject-level generating values were captured by simulated model fits. We determined what % of subject-level generating parameter values were successfully recovered across the 50 simulations. Recovery was "successful" if the 95% HDI of the fitted subject-level posterior contained the original subject-level generating value. Data in green in the plots below indicate the subject-level generating values were recovered and data in red indicates they were not. This was done separately for each  task.

Results below suggest that subject level generating parameters were well-recovered because the generating values (x-axis) were contained within the average 95% HDI of fitted values (error bars in plots below) in 88-97% of simulated participants.

```{r id1331322489273421, echo=FALSE, message=FALSE, warning=FALSE, tidy=TRUE,fig.show="hold",cache=TRUE,fig.height=3.5,fig.width=3.35}
#
subj_summary<-read.csv("/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_m5_psychophys/par_recover/parRecovery_subjLvl_summary.csv")

params<-c('alpha','beta','delta','b1')
parnames<-c('Visual Integration: Threshold Sep','Visual Integration: Start Point','Visual Integration: Drift Bias','Visual Integration: Drift Sensitivity')
groups<-c('HC','SZ')
subj_summary$sim_id<-subj_summary$sim

subj_summary$group<-ifelse(subj_summary$subj<36,1,2)

for (i in 1:length(groups)){
  for (j in 1:length(params)){
    temp_summary<-subset(subj_summary,subj_summary$group==i)
    temp_parname<-paste0(parnames[j]," (",groups[i],")")
    p<-plot_parrecover_subj(data=temp_summary,
                                   par=params[j],
                                   parname=temp_parname,
                                   sim_id<-'sim_id',
                                   genMean_suffix<-'_sub_gen',
                                   simMean_suffix<-'_sub_sim_mean',
                                   simHDI_suffix<-'_sub_sim_hdi_',
                                   recover_suffix<-'_recover')
    print(p)
  }
}

```

# Posterior Predictions

Posterior predictions were run using all posterior samples from the "Initial Fits" above. For these, models were fit to the real data of participants using 1500 warmup samples, 1000 postwarmup draws per chain, 10 chains, resulting in 10000 postwarmup draws. This was done separately for each model. Posterior means of all group and subject-level parameters were used to simulate trial-level choice proportions and RT distributions for each subject were simulated using values that matched the group and subject level posterior means and SDs of all group level parameters. Note that SD was scaled down slightly to avoid extreme values. 

Results are specified below and generally suggested that all three models were capable of making accurate predictions about choice proportions and RT distributions.

## Model 2 (Eye Contact) 

### Quantile Predictions

For the winning Eye Contact Model, the predicted RT distributions for both groups generally mapped well onto observed RT distributions. The exception was that the model slightly UNDER predicted the tails of the RT distribution at lower signal strengths, and it OVER predicted those at higher signal strengths. This is likely due to relatively few trials in the task (i.e., 12 per signal strength) and suggests that model predictions could be improved by adding more trials. Importantly, the extent of the over and under-prediction of the tails of the RT distribution at higher/lower signal strengths was generally comparable between groups. This means that -- while 

```{r id9924, echo=FALSE, fig.height=3.5, fig.width=4, message=FALSE, warning=FALSE, cache=TRUE}

library(HDInterval)
library(ggplot2)

load(paste0("/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_m2_psychophys/gaze/ppc/","quantile_choice_estimates.RData"))
quantiles<-c(.1,.3,.5,.7,.9)
estim_quantiles<-estim_quantiles_prop[,,1:length(quantiles)]
estim_choice_props<-estim_quantiles_prop[,,length(quantiles)+1]
beh_data_file<-paste(datadir,"/schizgaze12_gaze_beh.csv",sep="")
beh_data<-read.csv(beh_data_file)
beh_data<-subset(beh_data,beh_data$Study=="schizgaze2")
beh_data<-subset(beh_data,beh_data$Task=="Eyes")
beh_data$Resp<-ifelse(beh_data$Resp=="Y",1,2) #recode "yes" as 1 and "no as 2
beh_data$SubjID<-match(beh_data$Subj, unique(beh_data$Subj)) #subj IDs to sequential indexes
beh_data <- beh_data[order(beh_data$SubjID, beh_data$Resp), ]
levels<-unique(beh_data$GazeAngle)
levels<-levels[order(levels)]
groups<-unique(beh_data$Group)
subsample<-1000 #how much to subsample posterior by (helps cut down on ggplot rendering time)

for (l in 1:length(levels)){
  for (m in 1:length(groups)){
  
    #get list of subjects in this group
    current_subj_list<-unique(subset(beh_data,beh_data$Group==groups[m])$SubjID)
    
    #create empty array: #obs x #iterations x #quantiles
    estim_pred_quantiles_over_subjs <- array(numeric(),c(length(current_subj_list),subsample,length(quantiles))) 
    estim_real_quantiles_over_subjs <- array(numeric(),c(length(current_subj_list),length(quantiles))) 
    
    for (n in 1:length(current_subj_list)){

      #get real quantiles for this subj
      tmp_realdata<-subset(beh_data,beh_data$SubjID==current_subj_list[n]&beh_data$GazeAngle==levels[l])
      estim_real_quantiles_over_subjs[n,]<-quantile(tmp_realdata$RT,type=8)
      
      #get predicted quantiles (means) and HDI's
      
      ## get cols from n_obs that correspond to current cond, subj
      tmp_quant_cols<-which(beh_data$SubjID==current_subj_list[n]&beh_data$GazeAngle==levels[l])
      
      #use those to select observations from the predicted quantiles array
      tmp_quantile_est<-estim_quantiles[tmp_quant_cols,,]
      
      #average estimates over trials for this subject (gives matrix = #subsamples*#quantiles)
      if(length(tmp_quant_cols)==0){ #if no trials for current condition for current subject, create array of NA
        estim_pred_quantiles_over_subjs[n,,]<-matrix(NA,nrow=subsample,ncol=length(quantiles))
      }else if(length(tmp_quant_cols)==1){
        estim_pred_quantiles_over_subjs[n,,]<-tmp_quantile_est
      }else{ # if subj has 2 or more trials for this cond
        estim_pred_quantiles_over_subjs[n,,]<-apply(tmp_quantile_est, c(2,3), mean,na.rm=TRUE) 
      }
    }
    
    for (o in 1:length(quantiles)){
      
      tmp_hdi<-HDInterval::hdi(as.vector(estim_pred_quantiles_over_subjs[,,o]),credMass=0.95)
      tmp_hdi<-data.frame(quantile=quantiles[o],
                          real_quantile_mean=mean(estim_real_quantiles_over_subjs[,o],na.rm = TRUE),
                          pred_quantile_mean=mean(estim_pred_quantiles_over_subjs[,,o],na.rm = TRUE)*1000,
                          hdi_lo=tmp_hdi[1]*1000,
                          hdi_hi=tmp_hdi[2]*1000)
      if(o==1){
        all_hdi<-tmp_hdi
      }else{
        all_hdi<-rbind(all_hdi,tmp_hdi)
      }
    }
    
    #setup data in long form
    quantile_data<-data.frame(
      type=c(rep("real",length(quantiles)),rep("pred",length(quantiles))), # type of data (long form)
      quantile=c(quantiles,quantiles), # quantiles
      rt=c(all_hdi$real_quantile_mean,all_hdi$pred_quantile_mean), #type of quantile data
      hdi_lo=c(all_hdi$real_quantile_mean,all_hdi$hdi_lo),
      hdi_hi=c(all_hdi$real_quantile_mean, all_hdi$hdi_hi))
    
    quantile_data$group<-groups[m]
    
    if(m==1){#if group 1
      grp_quantile_data<-quantile_data
      grp1_trials<-dim(subset(beh_data,beh_data$Group==groups[m]&beh_data$GazeAngle==levels[l]))[1]
    }else if(m==2){#if group 2
      tmp_grp_quantile_data<-quantile_data
      tmp_grp_quantile_data$quantile<-tmp_grp_quantile_data$quantile+.03 #shift slightly to right for plotting
      grp_quantile_data<-rbind(tmp_grp_quantile_data,grp_quantile_data)
      grp2_trials<-dim(subset(beh_data,beh_data$Group==groups[m]&beh_data$GazeAngle==levels[l]))[1]
    }
  }
  
  #grp_quantile_data$group<-ifelse(grp_quantile_data$group==1,"HC","SZ")
  grp_quantile_data$group<-as.factor(grp_quantile_data$group)
  
  title<-paste("Eye Contact: Quantile predictions:\n","Signal strength ",levels[l]/max(levels),sep="")
  trials_text<-paste("Trials per group:\nHC=",grp1_trials," SZ=",grp2_trials,sep="")
  
  #plot quantiles (xaxis against all quantiles colored by "types", shapes vary by group)
  plot<-ggplot(grp_quantile_data, aes(x=quantile, y=rt, color=type)) +
    geom_errorbar(aes(ymin=hdi_lo,ymax=hdi_hi),width=0,alpha=.7) +
    geom_point(alpha=.7,aes(shape=group)) +
    ggtitle(title) +
    ylab("RT") +
    xlab("Quantile")+
    theme(plot.title = element_text(size=10),axis.text.y=element_text(size=7))+
    coord_cartesian(ylim = c(0,2500))+
    annotate(geom = "text", x = .5, y = 2450, label = trials_text, hjust = "center",size=3.5,lineheight=.9)
  
  print(plot)

}    

```
 
### Choice Predictions

Predicted choice proportions for the winning Eye Contact Model generally mapped captured patterns of choice proportions across signal strengths in both groups. The exception was that the  model predicted slightly less extreme choice proportions at higher and lower Eye Contact signal strengths than the observed data (likely a by-product of shrinkage due to only 12 trials per signal strength). Despite this, the model accurately predicted the general response pattern over signal strengths, namely the characteristic sigmoid shape of psychophysical data. Note that we tested whether this could be improved by including trial-level variability in the drift rates. This did not improve the choice predictions (and introduced other issues into the data, thus, we opted for the more parsimonious model).

```{r id103123121, echo=FALSE, fig.height=6.5, fig.width=4.25, message=FALSE, warning=FALSE, cache=TRUE}

# need to flip choice probabilities because PPC calculated probability of given choice option, not a "yes" choice (which is what we want)

library(dplyr)
library(ggplot2)

# for (i in 1:nrow(beh_data)){# for i in n_observations
#   tmp_choice<-beh_data$Resp[i] #get current choice
#   if(tmp_choice==2){ #if current choice is lower bound response, flip predicted choice prob
#     estim_choice_props[i,] <- 1-(estim_choice_props[i,])
#   }
# }




for (l in 1:length(levels)){
  for (m in 1:length(groups)){
    
    #get list of subjects in this group
    current_subj_list<-unique(subset(beh_data,beh_data$Group==groups[m])$SubjID)
    
    #create empty array: #subjs in this group x #iterations
    estim_pred_props_over_subjs <- array(numeric(),c(length(current_subj_list),subsample)) 
    estim_real_props_over_subjs <- array(numeric(),c(length(current_subj_list),1)) 
    
    for (n in 1:length(current_subj_list)){
      
      #calculate real YES choice proportion
      tmp_realdata<-subset(beh_data,beh_data$SubjID==current_subj_list[n] & beh_data$GazeAngle==levels[l]) # grab real # of yes reponses
      total_choices<-dim(tmp_realdata)[1] #total choices made for this level for this subj
      total_yes_choice<-length(which(tmp_realdata$RespBound==1))
      estim_real_props_over_subjs[n,1]<-total_yes_choice/total_choices
      
      #calculate predicted YES choice proportion + HDIs for this condition/subj
      tmp_prop_cols<-which(beh_data$SubjID==current_subj_list[n] & beh_data$GazeAngle==levels[l]) #YES coded as 1 in data
      tmp_prop_est<-estim_choice_props[tmp_prop_cols,]
      
      #average estimates over trials for this subject (gives matrix = #subsamples*#quantiles)
      if(length(tmp_prop_cols)==0){ #if no trials for current condition for current subject, create array of NA
        estim_pred_props_over_subjs[n,]<-as.numeric(matrix(NA,nrow=1,ncol=subsample))
      }else if(length(tmp_prop_cols)==1){
        estim_pred_props_over_subjs[n,]<-tmp_prop_est
      }else{ # if subj has 2 or more trials for this cond, average over trials
        estim_pred_props_over_subjs[n,]<-apply(tmp_prop_est, c(2), mean,na.rm=TRUE) 
      }
    }
    
    estim_pred_props_over_subjs<-apply(estim_pred_props_over_subjs, 1, mean)
    
    #calculate predicted YES choice proportion + HDIs
    tmp_hdi_pred<-HDInterval::hdi(as.vector(estim_pred_props_over_subjs),credMass=0.95)
    tmp_hdi_real<-HDInterval::hdi(as.vector(estim_real_props_over_subjs),credMass=0.95)
    
    tmp_cond<-paste0(groups[m]," | ",levels[l]/max(levels))
    
    #setup data in long form (for plots)
    choiceprop_data<-data.frame(
      type=as.factor(c("real","pred")), # type of data (long form)
      mean_choiceprop=as.numeric(c(mean(estim_real_props_over_subjs),mean(estim_pred_props_over_subjs,na.rm = TRUE))),
      pred_hdi_lo=as.numeric(c(tmp_hdi_real[1],tmp_hdi_pred[1])),
      pred_hdi_hi=as.numeric(c(tmp_hdi_real[2],tmp_hdi_pred[2])))
    
    choiceprop_data$group<-as.factor(m)
    choiceprop_data$cond<-tmp_cond
    
    if(l==1&m==1){ #if first iteration
      grp_choiceprop_data<-choiceprop_data   
    }else{
      grp_choiceprop_data<-rbind(grp_choiceprop_data,choiceprop_data)
    }
  }
}

grp_choiceprop_data <- grp_choiceprop_data[order(grp_choiceprop_data$cond),]
beh_data$plot_cond<-paste0(beh_data$Group," | ",beh_data$GazeAngle/max(beh_data$GazeAngle))

plot_beh_data <- beh_data[order(beh_data$plot_cond),]
plot_beh_data$RespBound[plot_beh_data$RespBound == 2] <- 0

plot_beh_data <- plot_beh_data %>% 
  dplyr::group_by(plot_cond,SubjID) %>% 
  dplyr::summarize(Resp = mean(RespBound,na.rm=TRUE))

plot_beh_data$type<-"real"
grp_choiceprop_data$group_name<-factor(grp_choiceprop_data$group, levels = c("1", "2"), labels = c("HC", "SZ"))

title<-paste("Eye Contact: Choice pred (% yes):\n All signal strengths",sep="")
plot<-ggplot(data=grp_choiceprop_data, aes(x=cond, y=mean_choiceprop,color=type)) +
  geom_point(data=grp_choiceprop_data,alpha=1,aes(shape=group_name),position=position_dodge(width = .5),size=3) + theme_bw()+
  geom_errorbar(data=grp_choiceprop_data,aes(ymin=pred_hdi_lo,ymax=pred_hdi_hi),linewidth=.8,
                width=0,alpha=1,position=position_dodge(width = .5)) +
  scale_fill_manual(values=c('#F8766D','#00BFC4'))+
  coord_cartesian(ylim = c(0,1))+
  ggtitle(title) + xlab("") +
  ylab("Choice Proportions")+
  theme(plot.title = element_text(size=10),axis.text.y = element_text(size=7),axis.text.x = element_text(size=7,angle = 90,vjust=1,hjust=1))+
  gghalves::geom_half_point(data=plot_beh_data,aes(x=plot_cond,y=Resp,color=type),
                            size=1.5,
                            side = "r", ## draw jitter on the right
                            range_scale = 0, ## control range of jitter
                            alpha = .3## add some transparency
  ) +
  coord_flip()
print(plot)

```
 
## Model 3 (Gender) 

### Quantile Predictions

For the winning gender model, predicted RT distributions for both groups generally mapped well onto observed RT distributions for correct trials. The exception was that the model slightly underpredicted the tails of the RT distribution of correct trials (likely because performance was near ceiling and lacked sufficient variability to effectively capture these effects) and poorly predicted RT distributions of incorrect trials (as there were few incorrect trials [< 100] to base predictions on). 

```{r id9238429392, echo=FALSE, fig.height=3.5, fig.width=4, message=FALSE, warning=FALSE, cache=TRUE}
####
load(paste0("/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_m3_psychophys/gender/ppc/","quantile_choice_estimates.RData"))
quantiles<-c(.1,.3,.5,.7,.9)

estim_quantiles<-estim_quantiles_yesprop[,,1:length(quantiles)]
estim_choice_props<-estim_quantiles_yesprop[,,length(quantiles)+1]
beh_data_file<-paste(datadir,"/schizgaze12_gaze_beh.csv",sep="")
beh_data<-read.csv(beh_data_file)
beh_data<-subset(beh_data,beh_data$Study=="schizgaze2")
beh_data<-subset(beh_data,beh_data$Task=="GenderID")
beh_data$SubjID<-match(beh_data$Subj, unique(beh_data$Subj)) #subj IDs to sequential indexes 
beh_data <- beh_data[order(beh_data$SubjID, beh_data$Resp), ]
beh_data$Resp<-ifelse(beh_data$Resp=="F",1,2) #recode female as 1 and male as 2
beh_data$Gender<-ifelse(beh_data$Gender=="F",1,2) #recode female gender of stim as 1 and male as 2
beh_data$Group<-match(beh_data$Group, unique(beh_data$Group))
beh_data$Acc<-ifelse(beh_data$Gender==beh_data$Resp,1,0)
gends<-unique(beh_data$Gender)
gend_names<-c("Female","Male")
groups<-unique(beh_data$Group)
group_names<-c("HC","SZ")
acc<-unique(beh_data$Acc)
acc_names<-c("Incorrect","Correct")
subsample<-1000 #how much to subsample posterior by (helps cut down on ggplot rendering time)

for (l in 1:length(gends)){
  for(p in 1:length(acc)){
    for (m in 1:length(groups)){
    
      #get list of subjects in this group
      current_subj_list<-unique(subset(beh_data,beh_data$Group==groups[m])$SubjID)
      
      #create empty array: #obs x #iterations x #quantiles
      estim_pred_quantiles_over_subjs <- array(numeric(),c(length(current_subj_list),subsample,length(quantiles))) 
      estim_real_quantiles_over_subjs <- array(numeric(),c(length(current_subj_list),length(quantiles))) 
      
      for (n in 1:length(current_subj_list)){
        
        #get real quantiles for this subj
        tmp_realdata<-subset(beh_data,beh_data$SubjID==current_subj_list[n]&beh_data$Gender==gends[l]&beh_data$Acc==acc[p])
        estim_real_quantiles_over_subjs[n,]<-quantile(tmp_realdata$RT,type=8)
        
        #get predicted quantiles (means) and HDI's
        
        ## get cols from n_obs that correspond to current cond, subj, acc
        tmp_quant_cols<-which(beh_data$SubjID==current_subj_list[n]&beh_data$Gender==gends[l]&beh_data$Acc==acc[p])
        
        #use those to select observations from the predicted quantiles array
        tmp_quantile_est<-estim_quantiles[tmp_quant_cols,,]
        
        #average estimates over trials for this subject (gives matrix = #subsamples*#quantiles)
        if(length(tmp_quant_cols)==0){ #if no trials for current condition for current subject, create array of NA
          estim_pred_quantiles_over_subjs[n,,]<-matrix(NA,nrow=subsample,ncol=length(quantiles))
        }else if(length(tmp_quant_cols)==1){
          estim_pred_quantiles_over_subjs[n,,]<-tmp_quantile_est
        }else{ # if subj has 2 or more trials for this cond
          estim_pred_quantiles_over_subjs[n,,]<-apply(tmp_quantile_est, c(2,3), mean,na.rm=TRUE) 
        }
      }
      
      for (o in 1:length(quantiles)){
        
        tmp_hdi<-HDInterval::hdi(as.vector(estim_pred_quantiles_over_subjs[,,o]),credMass=0.95)
        tmp_hdi<-data.frame(quantile=quantiles[o],
                            real_quantile_mean=mean(estim_real_quantiles_over_subjs[,o],na.rm = TRUE),
                            pred_quantile_mean=mean(estim_pred_quantiles_over_subjs[,,o],na.rm = TRUE)*1000,
                            hdi_lo=tmp_hdi[1]*1000,
                            hdi_hi=tmp_hdi[2]*1000)
        if(o==1){
          all_hdi<-tmp_hdi
        }else{
          all_hdi<-rbind(all_hdi,tmp_hdi)
        }
      }
      
      #setup data in long form
      quantile_data<-data.frame(
        type=c(rep("real",length(quantiles)),rep("pred",length(quantiles))), # type of data (long form)
        quantile=c(quantiles,quantiles), # quantiles
        rt=c(all_hdi$real_quantile_mean,all_hdi$pred_quantile_mean), #type of quantile data
        hdi_lo=c(all_hdi$real_quantile_mean,all_hdi$hdi_lo),
        hdi_hi=c(all_hdi$real_quantile_mean, all_hdi$hdi_hi))
      
      quantile_data$group<-groups[m]
      
      if(m==1){#if group 1
        grp_quantile_data<-quantile_data
        grp1_trials<-dim(subset(beh_data,beh_data$Group==groups[m]&beh_data$Gender==gends[l]&beh_data$Acc==acc[p]))[1]
      }else if(m==2){#if group 2
        tmp_grp_quantile_data<-quantile_data
        tmp_grp_quantile_data$quantile<-tmp_grp_quantile_data$quantile+.03 #shift slightly to right for plotting
        grp_quantile_data<-rbind(tmp_grp_quantile_data,grp_quantile_data)
        grp2_trials<-dim(subset(beh_data,beh_data$Group==groups[m]&beh_data$Gender==gends[l]&beh_data$Acc==acc[p]))[1]
      }
    }
    
    grp_quantile_data$group<-ifelse(grp_quantile_data$group==1,"HC","SZ")
    grp_quantile_data$group<-as.factor(grp_quantile_data$group)
  
    title<-paste("Gender: Quantile predictions:\n","Gender=",gend_names[l]," / Accuracy=",acc_names[p],sep="")
    trials_text<-paste("Trials per group:\nHC=",grp1_trials," SZ=",grp2_trials,sep="")
    
    #plot quantiles (xaxis against all quantiles colored by "types", shapes vary by group)
    plot<-ggplot(grp_quantile_data, aes(x=quantile, y=rt, color=type)) +
      geom_errorbar(aes(ymin=hdi_lo,ymax=hdi_hi),width=0,alpha=.7) +
      geom_point(alpha=.7,aes(shape=group)) +
      ggtitle(title) +
      ylab("RT") +
      xlab("Quantile")+
      theme(plot.title = element_text(size=10),axis.text.y=element_text(size=7))+
      coord_cartesian(ylim = c(0,2500))+
      annotate(geom = "text", x = .5, y = 2450, label = trials_text, hjust = "center",size=3.5,lineheight=.9)
    
    print(plot)
    
  }
}    

```
 
### Choice Predictions

```{r ra, echo=FALSE, fig.height=3.5, fig.width=5, message=FALSE, warning=FALSE, cache=TRUE}


for (i in 1:nrow(beh_data)){# for i in n_observations
  tmp_choice<-beh_data$Resp[i] #get current choice
  if(tmp_choice==2){ #if current choice is lower bound,flip predicted choice prob
    estim_choice_props[i,] <- 1-(estim_choice_props[i,])
  }
}

for (l in 1:length(gends)){
    for (m in 1:length(groups)){
      
      #get list of subjects in this group
      current_subj_list<-unique(subset(beh_data,beh_data$Group==groups[m])$SubjID)
      
      #create empty array: #subjs in this group x #iterations
      estim_pred_props_over_subjs <- array(numeric(),c(length(current_subj_list),subsample)) 
      estim_real_props_over_subjs <- array(numeric(),c(length(current_subj_list),1)) 
      
      for (n in 1:length(current_subj_list)){
  
        #calculate real FEMALE choice proportion
        tmp_realdata<-subset(beh_data,beh_data$SubjID==current_subj_list[n] & beh_data$Gender==gends[l]) # grab real # of yes reponses
        total_choices<-dim(tmp_realdata)[1] #total choices made for this level for this subj
        total_F_choice<-length(which(tmp_realdata$Resp==1))
        estim_real_props_over_subjs[n,1]<-total_F_choice/total_choices
        
        #calculate predicted YES choice proportion + HDIs for this condition/subj
        tmp_prop_cols<-which(beh_data$SubjID==current_subj_list[n] & beh_data$Gender==gends[l]) #YES coded as 1 in data
        tmp_prop_est<-estim_choice_props[tmp_prop_cols,]
        
        #average estimates over trials for this subject (gives matrix = #subsamples*#quantiles)
        if(length(tmp_prop_cols)==0){ #if no trials for current condition for current subject, create array of NA
          estim_pred_props_over_subjs[n,]<-as.numeric(matrix(NA,nrow=1,ncol=subsample))
        }else if(length(tmp_prop_cols)==1){
          estim_pred_props_over_subjs[n,]<-tmp_prop_est
        }else{ # if subj has 2 or more trials for this cond, average over trials
          estim_pred_props_over_subjs[n,]<-apply(tmp_prop_est, c(2), mean,na.rm=TRUE) 
        }
      }
      
      #calculate predicted YES choice proportion + HDIs
      tmp_hdi_pred<-HDInterval::hdi(as.vector(estim_pred_props_over_subjs),credMass=0.95)
      tmp_hdi_real<-HDInterval::hdi(as.vector(estim_real_props_over_subjs),credMass=0.95)
      tmp_cond<-paste0(groups[m]," | ",gends[l])
      
      #setup data in long form (for plots)
      choiceprop_data<-data.frame(
        type=as.factor(c("real","pred")), # type of data (long form)
        mean_choiceprop=as.numeric(c(mean(estim_real_props_over_subjs,na.rm = TRUE),mean(estim_pred_props_over_subjs,na.rm = TRUE))),
        pred_hdi_lo=as.numeric(c(tmp_hdi_real[1],tmp_hdi_pred[1])),
        pred_hdi_hi=as.numeric(c(tmp_hdi_real[2],tmp_hdi_pred[2])))
      
      choiceprop_data$group<-as.factor(m)
      choiceprop_data$cond<-tmp_cond
      
      if(l==1&m==1){ #if first iteration
        grp_choiceprop_data<-choiceprop_data   
      }else{
        grp_choiceprop_data<-rbind(grp_choiceprop_data,choiceprop_data)
      }
    }
}

grp_choiceprop_data <- grp_choiceprop_data[order(grp_choiceprop_data$cond),]
grp_choiceprop_data$cond <- gsub("1 | ","HC | ",fixed=T,grp_choiceprop_data$cond)
grp_choiceprop_data$cond <- gsub("2 | ","SZ | ",fixed=T,grp_choiceprop_data$cond)
grp_choiceprop_data$cond <- gsub("1","Female",fixed=T,grp_choiceprop_data$cond)
grp_choiceprop_data$cond <- gsub("2","Male",fixed=T,grp_choiceprop_data$cond)
beh_data$Group<-ifelse(beh_data$Group==1,"HC","SZ")
beh_data$Gender<-ifelse(beh_data$Gender==1,"Female","Male")
beh_data$plot_cond<-paste0(beh_data$Group," | ",beh_data$Gender)
plot_beh_data <- beh_data[order(beh_data$plot_cond),]
plot_beh_data$Resp[plot_beh_data$Resp == 2] <- 0

plot_beh_data <- plot_beh_data %>% 
  dplyr::group_by(plot_cond,SubjID) %>% 
  dplyr::summarize(Resp = mean(Resp,na.rm=TRUE))

plot_beh_data$type<-"real"
grp_choiceprop_data$group_name<-factor(grp_choiceprop_data$group, levels = c("1", "2"), labels = c("HC", "SZ"))

title<-paste("Gender: Choice pred (% Female):\n All genders",sep="")
plot<-ggplot(data=grp_choiceprop_data, aes(x=cond, y=mean_choiceprop,color=type)) +
  geom_point(data=grp_choiceprop_data,alpha=1,aes(shape=group_name),position=position_dodge(width = .5),size=3) + theme_bw()+
  geom_errorbar(data=grp_choiceprop_data,aes(ymin=pred_hdi_lo,ymax=pred_hdi_hi),linewidth=.8,
                width=0,alpha=1,position=position_dodge(width = .5)) +
  scale_fill_manual(values=c('#F8766D','#00BFC4'))+
  coord_cartesian(ylim = c(0,1))+
  ggtitle(title) + xlab("") +
  ylab("Choice Proportions")+
  theme(plot.title = element_text(size=10),axis.text.y = element_text(size=7),axis.text.x = element_text(size=7,angle = 90,vjust=1,hjust=1))+
  gghalves::geom_half_point(data=plot_beh_data,aes(x=plot_cond,y=Resp,color=type),
                            size=1.5,
                            side = "r", ## draw jitter on the right
                            range_scale = 0.2, ## control range of jitter
                            alpha = .2## add some transparency
  ) +
  coord_flip()
print(plot)

```
 
## Model 5 (Visual Integration) 

### Quantile Predictions

For the winning Eye Contact Model, predicted RT distributions for both groups mapped closely onto observed RT distributions across all signal strengths.

```{r id9922224, echo=FALSE, fig.height=3.5, fig.width=4, message=FALSE, warning=FALSE, cache=TRUE}

library(HDInterval)
library(ggplot2)

load(paste0("/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_m5_psychophys/ppc/","quantile_choice_estimates.RData"))
quantiles<-c(.1,.3,.5,.7,.9)
estim_quantiles<-estim_quantiles_prop[,,1:length(quantiles)]
estim_choice_props<-estim_quantiles_prop[,,length(quantiles)+1]
beh_data_file<-paste(datadir,"/schizgaze2_jovi_beh.csv",sep="")
beh_data<-read.csv(beh_data_file)
beh_data<-subset(beh_data,beh_data$Study=="schizgaze2")
beh_data <- beh_data[order(beh_data$Subj), ]
beh_data<-subset(beh_data,beh_data$Subj!="2032")
beh_data<-subset(beh_data,beh_data$Subj!="2042")
beh_data$SubjID<-match(beh_data$Subj, unique(beh_data$Subj)) #subj IDs to sequential indexes
beh_data$Resp<-ifelse(beh_data$Resp=="2",1,2) #recode L (2) as 1 and R (3) as 2
beh_data$Group<-match(beh_data$Group, unique(alldata$Group))
beh_data <- beh_data[order(beh_data$SubjID, beh_data$Resp), ]
levels<-unique(beh_data$Jitter)
levels<-levels[order(levels)]
levels_names<-c("Right-0","Right-7","Right-9","Right-11","Right-13","Right-15",
                "Left-15","Left-13","Left-11","Left-9","Left-7","Left-0")

groups<-unique(beh_data$Group)
subsample<-1000 #how much to subsample posterior by (helps cut down on ggplot rendering time)

for (l in 1:length(levels)){
  for (m in 1:length(groups)){
  
    #get list of subjects in this group
    current_subj_list<-unique(subset(beh_data,beh_data$Group==groups[m])$SubjID)
    
    #create empty array: #obs x #iterations x #quantiles
    estim_pred_quantiles_over_subjs <- array(numeric(),c(length(current_subj_list),subsample,length(quantiles))) 
    estim_real_quantiles_over_subjs <- array(numeric(),c(length(current_subj_list),length(quantiles))) 
    
    for (n in 1:length(current_subj_list)){

      #get real quantiles for this subj
      tmp_realdata<-subset(beh_data,beh_data$SubjID==current_subj_list[n]&beh_data$Jitter==levels[l])
      estim_real_quantiles_over_subjs[n,]<-quantile(tmp_realdata$RT,type=8)
      
      #get predicted quantiles (means) and HDI's
      
      ## get cols from n_obs that correspond to current cond, subj
      tmp_quant_cols<-which(beh_data$SubjID==current_subj_list[n]&beh_data$Jitter==levels[l])
      
      #use those to select observations from the predicted quantiles array
      tmp_quantile_est<-estim_quantiles[tmp_quant_cols,,]
      
      #average estimates over trials for this subject (gives matrix = #subsamples*#quantiles)
      if(length(tmp_quant_cols)==0){ #if no trials for current condition for current subject, create array of NA
        estim_pred_quantiles_over_subjs[n,,]<-matrix(NA,nrow=subsample,ncol=length(quantiles))
      }else if(length(tmp_quant_cols)==1){
        estim_pred_quantiles_over_subjs[n,,]<-tmp_quantile_est
      }else{ # if subj has 2 or more trials for this cond
        estim_pred_quantiles_over_subjs[n,,]<-apply(tmp_quantile_est, c(2,3), mean,na.rm=TRUE) 
      }
    }
    
    for (o in 1:length(quantiles)){
      
      tmp_hdi<-HDInterval::hdi(as.vector(estim_pred_quantiles_over_subjs[,,o]),credMass=0.95)
      tmp_hdi<-data.frame(quantile=quantiles[o],
                          real_quantile_mean=mean(estim_real_quantiles_over_subjs[,o],na.rm = TRUE),
                          pred_quantile_mean=mean(estim_pred_quantiles_over_subjs[,,o],na.rm = TRUE),
                          hdi_lo=tmp_hdi[1],
                          hdi_hi=tmp_hdi[2])
      if(o==1){
        all_hdi<-tmp_hdi
      }else{
        all_hdi<-rbind(all_hdi,tmp_hdi)
      }
    }
    
    #setup data in long form
    quantile_data<-data.frame(
      type=c(rep("real",length(quantiles)),rep("pred",length(quantiles))), # type of data (long form)
      quantile=c(quantiles,quantiles), # quantiles
      rt=c(all_hdi$real_quantile_mean,all_hdi$pred_quantile_mean), #type of quantile data
      hdi_lo=c(all_hdi$real_quantile_mean,all_hdi$hdi_lo),
      hdi_hi=c(all_hdi$real_quantile_mean, all_hdi$hdi_hi))
    
    quantile_data$group<-groups[m]
    
    if(m==1){#if group 1
      grp_quantile_data<-quantile_data
      grp1_trials<-dim(subset(beh_data,beh_data$Group==groups[m]&beh_data$Jitter==levels[l]))[1]
    }else if(m==2){#if group 2
      tmp_grp_quantile_data<-quantile_data
      tmp_grp_quantile_data$quantile<-tmp_grp_quantile_data$quantile+.03 #shift slightly to right for plotting
      grp_quantile_data<-rbind(tmp_grp_quantile_data,grp_quantile_data)
      grp2_trials<-dim(subset(beh_data,beh_data$Group==groups[m]&beh_data$Jitter==levels[l]))[1]
    }
  }
  
  grp_quantile_data$group<-ifelse(grp_quantile_data$group==1,"HC","SZ")
  grp_quantile_data$group<-as.factor(grp_quantile_data$group)
  
  title<-paste("Visual Integration: Quantile Predictions:\n","Direction/Jitter: ",levels_names[l]," degrees",sep="")
  trials_text<-paste("Trials per group:\nHC=",grp1_trials," SZ=",grp2_trials,sep="")
  
  #plot quantiles (xaxis against all quantiles colored by "types", shapes vary by group)
  plot<-ggplot(grp_quantile_data, aes(x=quantile, y=rt, color=type)) +
    geom_errorbar(aes(ymin=hdi_lo,ymax=hdi_hi),width=0,alpha=.7) +
    geom_point(alpha=.7,aes(shape=group)) +
    ggtitle(title) +
    ylab("RT") +
    xlab("Quantile")+
    theme(plot.title = element_text(size=10),axis.text.y=element_text(size=7))+
    coord_cartesian(ylim = c(0,2.9))+
    annotate(geom = "text", x = .5, y = 2.7, label = trials_text, hjust = "center",size=3.5,lineheight=.9)
  
  print(plot)

}    

```

### Choice Predictions

Predicted choice proportions for the winning Eye Contact Model generally mapped captured patterns of choice proportions across signal strengths in both groups. The exception was that the Eye Contact Model predicted slightly less extreme choice proportions at higher and lower Eye Contact signal strengths than the observed data (likely a by-product of shrinkage due to only 12 trials per signal strength). Despite this, the model accurately predicted the general response pattern over signal strengths, namely the characteristic sigmoid shape of psychophysical data. Note that we tested whether this could be improved by including trial-level variability in the drift rates. This did not improve the choice predictions (and introduced other issues into the data, thus, we opted for the more parsimonious model).

```{r id10312312221, echo=FALSE, fig.height=6.5, fig.width=4.25, message=FALSE, warning=FALSE, cache=TRUE}

#library(dplyr)
#library(ggplot2)

beh_data$Jitter2 <- levels_names[match(beh_data$Jitter, levels)]
#beh_data$GroupName<-ifelse(beh_data$Group==1,"HC","SZ")
groups_names<-c("HC","SZ")

for (l in 1:length(levels)){
  for (m in 1:length(groups)){
    
    #get list of subjects in this group
    current_subj_list<-unique(subset(beh_data,beh_data$Group==groups[m])$SubjID)
    
    #create empty array: #subjs in this group x #iterations
    estim_pred_props_over_subjs <- array(numeric(),c(length(current_subj_list),subsample)) 
    estim_real_props_over_subjs <- array(numeric(),c(length(current_subj_list),1)) 
    
    for (n in 1:length(current_subj_list)){
      
      #calculate REAL left choice proportion
      tmp_realdata<-subset(beh_data,beh_data$SubjID==current_subj_list[n] & beh_data$Jitter==levels[l]) # grab real # of L reponses
      total_choices<-dim(tmp_realdata)[1] #total L choices made for this jitter for this subj
      total_L_choice<-length(which(tmp_realdata$RespBound==1))
      estim_real_props_over_subjs[n,1]<-total_L_choice/total_choices
      
      #calculate PREDICTED left choice proportion + HDIs for this condition/subj
      tmp_prop_cols<-which(beh_data$SubjID==current_subj_list[n] & beh_data$Jitter==levels[l]) #LEFT coded as 1 
      tmp_prop_est<-estim_choice_props[tmp_prop_cols,]
      
      #average estimates over trials for this subject (gives matrix = #subsamples*#quantiles)
      if(length(tmp_prop_cols)==0){ #if no trials for current condition for current subject, create array of NA
        estim_pred_props_over_subjs[n,]<-as.numeric(matrix(NA,nrow=1,ncol=subsample))
      }else if(length(tmp_prop_cols)==1){
        estim_pred_props_over_subjs[n,]<-tmp_prop_est
      }else{ # if subj has 2 or more trials for this cond, average over trials
        estim_pred_props_over_subjs[n,]<-apply(tmp_prop_est, c(2), mean,na.rm=TRUE) 
      }
    }
    
    estim_pred_props_over_subjs<-apply(estim_pred_props_over_subjs, 1, mean,na.rm=TRUE)
    
    #calculate predicted L choice proportion + HDIs
    tmp_hdi_pred<-HDInterval::hdi(as.vector(estim_pred_props_over_subjs),credMass=0.95)
    tmp_hdi_real<-HDInterval::hdi(as.vector(estim_real_props_over_subjs),credMass=0.95)
    
    #tmp_cond<-paste0(groups[m]," | ",round(levels[l]/max(levels),3))
    tmp_cond<-paste0(groups_names[m]," | ",levels_names[l])
    
    #setup data in long form (for plots)
    choiceprop_data<-data.frame(
      type=as.factor(c("real","pred")), 
      mean_choiceprop=as.numeric(c(mean(estim_real_props_over_subjs),mean(estim_pred_props_over_subjs,na.rm = TRUE))),
      pred_hdi_lo=as.numeric(c(tmp_hdi_real[1],tmp_hdi_pred[1])),
      pred_hdi_hi=as.numeric(c(tmp_hdi_real[2],tmp_hdi_pred[2])))
    
    choiceprop_data$group<-as.factor(m)
    choiceprop_data$cond<-tmp_cond
    
    if(l==1&m==1){ #if first iteration
      grp_choiceprop_data<-choiceprop_data   
    }else{
      grp_choiceprop_data<-rbind(grp_choiceprop_data,choiceprop_data)
    }
  }
}

grp_choiceprop_data <- grp_choiceprop_data[order(grp_choiceprop_data$cond),]
grp_choiceprop_data$group <- ifelse(grp_choiceprop_data$group==1,"HC","SZ")
beh_data$Group<-ifelse(beh_data$Group==1,"HC","SZ")
beh_data$plot_cond<-paste0(beh_data$Group," | ",beh_data$Jitter2)
plot_beh_data <- beh_data[order(beh_data$plot_cond),]
plot_beh_data$RespBound[plot_beh_data$RespBound == 2] <- 0

plot_beh_data <- plot_beh_data %>% 
  dplyr::group_by(plot_cond,SubjID) %>% 
  dplyr::summarize(Resp = mean(RespBound,na.rm=TRUE))

plot_beh_data$type<-"real"
plot_beh_data$plot_cond<-factor(plot_beh_data$plot_cond,levels = c(c("HC | Left-0","HC | Left-7","HC | Left-9","HC | Left-11","HC | Left-13","HC | Left-15","HC | Right-15","HC | Right-13","HC | Right-11" ,"HC | Right-9","HC | Right-7","HC | Right-0","SZ | Left-0","SZ | Left-7","SZ | Left-9","SZ | Left-11","SZ | Left-13","SZ | Left-15","SZ | Right-15","SZ | Right-13","SZ | Right-11" ,"SZ | Right-9","SZ | Right-7","SZ | Right-0")))

grp_choiceprop_data$group_name<-factor(grp_choiceprop_data$group, levels = c("HC", "SZ"))

title<-paste("Visual Integration: Choice pred (% Left):\n All jitters",sep="")
plot<-ggplot(data=grp_choiceprop_data, aes(x=cond, y=mean_choiceprop,color=type)) +
  geom_point(data=grp_choiceprop_data,alpha=1,aes(shape=group_name),position=position_dodge(width = .5),size=3) + theme_bw()+
  geom_errorbar(data=grp_choiceprop_data,aes(ymin=pred_hdi_lo,ymax=pred_hdi_hi),linewidth=.8,
                width=0,alpha=1,position=position_dodge(width = .5)) +
  scale_fill_manual(values=c('#F8766D','#00BFC4'))+
  coord_cartesian(ylim = c(0,1))+
  ggtitle(title) + xlab("") +
  ylab("Choice Proportions")+
  xlab("Group / Stimulus Direction / Jitter (Degrees)")+
  theme(plot.title = element_text(size=10),axis.text.y = element_text(size=7),axis.text.x = element_text(size=7,angle = 90,vjust=1,hjust=1))+
  gghalves::geom_half_point(data=plot_beh_data,aes(x=plot_cond,y=Resp,color=type),
                            size=1.5,
                            side = "r", ## draw jitter on the right
                            range_scale = 0, ## control range of jitter
                            alpha = .3## add some transparency
  ) +
  coord_flip()
print(plot)

```
 
# [ANALYSIS SUPPLEMENT] {.unnumbered}

Note on coding of response boundaries: For modeling, testing, and evaluation -- in the Gender Identification task -- female stimuli were modeled as upper bound responses and male as lower boundary. In the Eye Contact Detection Task, "looking at me" was modeled as upper boundary and "not looking at me" as lower boundary. However, in the task design, participants pressed their INDEX finger for "Yes-looking at me responses" (in the Eye Contact Detection task) and for "male" (in the Gender Identification task). They pressed their MIDDLE finger for "No-not looking at me responses" (in the Eye Contact Detection task) and for "female" (in the Gender Identification task). So, in order to be able to rule out response biases across the two tasks (in the start points, drift biases, etc), we recoded the samples from the gender identification task so that upper boundary responses corresponded to choices made with the index finger (Yes, Looking at me and Male) and lower boundary responses corresponded to choices made with the index finger (No, not looking at me and female). This was done simply by multiplying subject and group-level drift rates in the gender identification task by -1. For this reason, the directionality of drift rate estimates in the gender identification task for the analyses below will be opposite to the directionality of those estimates shown above for model testing.

For the "final fits" this issue was already fixed at the fitting stage

```{r id9933221912, echo=FALSE, cache=T,fig.width=4,fig.height=3.75}

# Preprocess Data
n_subj<-81

# EYE CONTACT Model
gaze2_all_samples<-as.data.frame(fread('/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_m2_psychophys/gaze/final_fit/allgroups_final_fit_samples.csv'))

colnames(gaze2_all_samples) <- gsub("\\[|\\]", ".", colnames(gaze2_all_samples))

for(i in 1:n_subj){
  temp_col1<-paste0("sub_delta.",i,".")
  gaze2_all_samples[[temp_col1]]<-pnorm(gaze2_all_samples[[temp_col1]])*10-5
}

#get subject level variables
sub_alpha<-colMeans(gaze2_all_samples[,grep("sub_alpha.",fixed=T,colnames(gaze2_all_samples))])
sub_beta<-colMeans(gaze2_all_samples[,grep("sub_beta.",fixed=T,colnames(gaze2_all_samples))])
sub_delta_b1<-colMeans(gaze2_all_samples[,grep("sub_b1.",fixed=T,colnames(gaze2_all_samples))])
sub_ndt<-colMeans(gaze2_all_samples[,grep("sub_ndt.",fixed=T,colnames(gaze2_all_samples))])
sub_delta_bias<-colMeans(gaze2_all_samples[,grep("sub_delta.",fixed=T,colnames(gaze2_all_samples))])

sub_samples<-data.frame(alpha=sub_alpha,
                        beta=sub_beta,
                        delta_bias=sub_delta_bias,
                        delta_b1=sub_delta_b1,
                        ndt=sub_ndt)

all_sub_data<-cbind(sg2_correlates,sub_samples)

#GENDER MODEL (in the "final fit" model, the boundaries are already aligned)
gaze3_all_samples<-as.data.frame(fread('/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_m3_psychophys/gender/final_fit/allgroups_final_fit_samples.csv'))

colnames(gaze3_all_samples) <- gsub("\\[|\\]", ".", colnames(gaze3_all_samples))

for(i in 1:n_subj){

  temp_col1<-paste0("sub_delta_male.",i,".")
  temp_col2<-paste0("sub_delta_female.",i,".")

  temp_col3<-paste0("sub_delta_bias.",i,".")
  gaze3_all_samples[[temp_col3]]<-(gaze3_all_samples[[temp_col1]]+(gaze3_all_samples[[temp_col2]]))/2
  
  temp_col4<-paste0("sub_delta.",i,".")
  gaze3_all_samples[[temp_col4]]<-(gaze3_all_samples[[temp_col1]]-(gaze3_all_samples[[temp_col2]]))/2
}

sub_alpha<-colMeans(gaze3_all_samples[,grep("sub_alpha.",fixed=T,colnames(gaze3_all_samples))])
sub_beta<-colMeans(gaze3_all_samples[,grep("sub_beta.",fixed=T,colnames(gaze3_all_samples))])
sub_ndt<-colMeans(gaze3_all_samples[,grep("sub_ndt.",fixed=T,colnames(gaze3_all_samples))])
sub_delta_female<-colMeans(gaze3_all_samples[,grep("sub_delta_female.",fixed=T,colnames(gaze3_all_samples))])
sub_delta_male<-colMeans(gaze3_all_samples[,grep("sub_delta_male.",fixed=T,colnames(gaze3_all_samples))])
sub_delta_bias<-colMeans(gaze3_all_samples[,grep("sub_delta_bias.",fixed=T,colnames(gaze3_all_samples))])
sub_delta<-colMeans(gaze3_all_samples[,grep("sub_delta.",fixed=T,colnames(gaze3_all_samples))])

gaze3_all_samples$mu_grp_delta_bias_pr.1.<-(gaze3_all_samples$mu_grp_delta_male_pr.1.+gaze3_all_samples$mu_grp_delta_female_pr.1.)/2
gaze3_all_samples$mu_grp_delta_bias_pr.2.<-(gaze3_all_samples$mu_grp_delta_male_pr.2.+gaze3_all_samples$mu_grp_delta_female_pr.2.)/2

gaze3_all_samples$mu_grp_delta_pr.1.<-(gaze3_all_samples$mu_grp_delta_male_pr.1.-gaze3_all_samples$mu_grp_delta_female_pr.1.)/2
gaze3_all_samples$mu_grp_delta_pr.2.<-(gaze3_all_samples$mu_grp_delta_male_pr.2.-gaze3_all_samples$mu_grp_delta_female_pr.2.)/2

gaze3_all_samples$mu_delta.1.<-pnorm(gaze3_all_samples$mu_grp_delta_pr.1.)*10-5
gaze3_all_samples$mu_delta.2.<-pnorm(gaze3_all_samples$mu_grp_delta_pr.2.)*10-5

gaze3_all_samples$mu_delta_bias.1.<-pnorm(gaze3_all_samples$mu_grp_delta_bias_pr.1.)*10-5
gaze3_all_samples$mu_delta_bias.2.<-pnorm(gaze3_all_samples$mu_grp_delta_bias_pr.2.)*10-5

sub_samples_gend<-data.frame(alpha_gend=sub_alpha,
                        beta_gend=sub_beta,
                        delta_female_gend=sub_delta_female,
                        delta_male_gend=sub_delta_male,
                        delta_bias_gend=sub_delta_bias,
                        delta_gend=sub_delta,
                        ndt_gend=sub_ndt)

all_sub_data<-cbind(all_sub_data,sub_samples_gend)

#Visual Integration MODEL
jovi_all_samples<-as.data.frame(fread('/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_m5_psychophys/final_fit/allgroups_final_fit_samples.csv'))

colnames(jovi_all_samples) <- gsub("\\[|\\]", ".", colnames(jovi_all_samples))

for(i in 1:n_subj){
  temp_col1<-paste0("sub_delta.",i,".")
  jovi_all_samples[[temp_col1]]<-pnorm(jovi_all_samples[[temp_col1]])*10-5
}

jovi_sub_alpha<-colMeans(jovi_all_samples[,grep("sub_alpha.",fixed=T,colnames(jovi_all_samples))])
jovi_sub_beta<-colMeans(jovi_all_samples[,grep("sub_beta.",fixed=T,colnames(jovi_all_samples))])
jovi_sub_ndt<-colMeans(jovi_all_samples[,grep("sub_ndt.",fixed=T,colnames(jovi_all_samples))])

jovi_sub_delta_bias<-colMeans(jovi_all_samples[,grep("sub_delta.",fixed=T,colnames(jovi_all_samples))])
jovi_sub_delta_b1<-colMeans(jovi_all_samples[,grep("sub_b1.",fixed=T,colnames(jovi_all_samples))])

sub_samples_jovi<-data.frame(alpha_jovi=jovi_sub_alpha,
                             beta_jovi=jovi_sub_beta,
                             delta_bias_jovi=jovi_sub_delta_bias,
                             delta_b1_jovi=jovi_sub_delta_b1)

all_sub_data<-cbind(all_sub_data,sub_samples_jovi)

all_sub_data$sz_dummy<-ifelse(all_sub_data$group=="SZ",1,0)
all_sub_data$female_dummy<-ifelse(all_sub_data$gender=="Female",1,0)

write.csv(all_sub_data,"/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/data/schizgaze2_all_sub_params_w_correlates.csv",row.names = FALSE)

all_sub_data<-read.csv("/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/data/schizgaze2_all_sub_params_w_correlates.csv")

```

# Demographics

Tests below are Bayesian t-tests, ANOVA, and proportion analyses run using ttestBF, anovaBF, and proportion BF in the 'BayesFactor' R package (Morey, 2015). 

```{r id1243333322222, echo=FALSE, fig.height=5, fig.show="hold", fig.width=5, message=FALSE, warning=FALSE, cache=TRUE, dpi=500, out.width='1500px'}

#Demographics
demo_table<-dplyr::select(all_sub_data,c('subj','group','age','female_dummy','edu','parent_edu',
                                         'white','africanamer','asian','nativeamer','multi','other','hispanic',
                                         'schizophrenia','schizoaffective','schizophreniform',
                                         'gpts_socref','gpts_persec',
                                         'antipsychotic_user',
                                         'antidepressant_user','moodstab_user',
                                         'stimulant_user','anticholinergic_user',
                                         'cpzeq',
                                         'matrics','soc_cog_pc_wrt'))

colnames(demo_table)<-c('Subj','Group','Age','Sex (% Female)','Education (Years)','Parent Education (Years)',
                                         'White (%)','African American (%)','Asian (%)',
                        'Native American (%)','Multiracial (%)','Other (%)','Hispanic (%)',
                                         'Schizophrenia (%)','Schizoaffective (%)','Schizophreniform (%)',
                                         'GPTS-Referential Thinking','GPTS-Persecution',
                                         'Antipsychotic User (%)',
                                         'Antidepressant User (%)','Mood Stabilizer User (%)',
                                         'Stimulant User (%)','Anticholinergic User (%)',
                                         'CPZeq',
                                         'General Cognition (MCCB PC)','Social Cognition (MSCEIT/RME/ER40 PC)')

demo_table_hc<-subset(demo_table,demo_table$Group=="HC")
demo_table_sz<-subset(demo_table,demo_table$Group=="SZ")

demo_table_hc_sd<-apply((demo_table_hc[,3:ncol(demo_table_hc)]),2,sd,na.rm=T)
demo_table_sz_sd<-apply((demo_table_sz[,3:ncol(demo_table_sz)]),2,sd,na.rm=T)

demo_table_hc_n<-colSums(!is.na(demo_table_hc[,3:ncol(demo_table_hc)]))
demo_table_sz_n<-colSums(!is.na(demo_table_sz[,3:ncol(demo_table_sz)]))

demo_table_hc<-colMeans(demo_table_hc[,3:ncol(demo_table_hc)],na.rm=T)
demo_table_sz<-colMeans(demo_table_sz[,3:ncol(demo_table_sz)],na.rm=T)

demo_table_all<-data.frame(cbind(demo_table_hc,demo_table_hc_sd,demo_table_hc_n,
                                 demo_table_sz,demo_table_sz_sd,demo_table_sz_n))

colnames(demo_table_all)<-c("HC (M)","HC (SD)","HC (N)",
                            "SZ (M)","SZ (SD)","SZ (N)")

demo_table_all<-round(demo_table_all,2)

demo_table_all[demo_table_all=="NaN"]<-"--"
demo_table_all[is.na(demo_table_all)]<-"--"

# remove SD's for % variables
rows_to_replace <- grep("%", rownames(demo_table_all))
cols_to_replace <- grep("SD", colnames(demo_table_all))
demo_table_all[rows_to_replace, cols_to_replace] <- "--"

rows_with_percent <- grepl("%", rownames(demo_table_all))
demo_table_all$GroupDiff<-"--"
demo_table_all$PostHoc<-"--"

for (i in 1:nrow(demo_table_all)){

  tmp_var<-rownames(demo_table_all)[i]
  
  # skip variables with data only in 1 group
  if(grepl("Hallucination|Delusion|Motivation|Expressive|Schizo|User|Other|CPZ", tmp_var)==FALSE){
    grp_var<-factor(demo_table$Group,levels=c("HC","SZ"))
    tmp_data<-demo_table[[tmp_var]]
    remove<-which(is.na(tmp_data))
    if(length(remove)>0){
      grp_var<-grp_var[-remove]
      tmp_data<-tmp_data[-remove]
    }
    
    if(rows_with_percent[i]==TRUE){#if categorical
      demo_table_all$GroupDiff[i]<-BFchisq_summary(grp_var,tmp_data,credMass=0.95)[[1]]
      demo_table_all$PostHoc[i]<-BFchisq_summary(grp_var,tmp_data,credMass=0.95)[[2]]
    }else{ #if continuous
      demo_table_all$GroupDiff[i]<-BFanova_summary(grp_var,tmp_data,credMass=0.95)[[1]]
      demo_table_all$PostHoc[i]<-BFanova_summary(grp_var,tmp_data,credMass=0.95)[[2]]
    }
  }
}

demo_table_all %>%
  kbl(caption = "Sample Characteristics",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  pack_rows("Demographic", 1, 4) %>%
  pack_rows("Race", 5, 10,hline_before = T,italic=T,bold=F) %>%
  pack_rows("Ethinicity", 11, 11,hline_before = T,italic=T,bold=F) %>%
  pack_rows("Clinical", 12, 22) %>%
  pack_rows("Diagnosis", 12, 14,hline_before = T,italic=T,bold=F) %>%
  pack_rows("Symptoms", 15, 16,hline_before = T,italic=T,bold=F) %>%
  pack_rows("Medications", 17, 22,hline_before = T,italic=T,bold=F) %>%
  pack_rows("General/Social Cognition", 23, 24) %>%
  kable_styling(full_width=T)

```
 
# Group Differences

## Model 2 (Eye Contact)

```{r id9933212322999, echo=FALSE, cache=TRUE,fig.width=4,fig.height=3.75}

plot_posterior_diffs<-function(data,ci,par_prefix,rawpar_prefix,sig_prefix,par_name,group_nums,group_names,sample_sizes,round_digits,showtext){
  library(HDInterval)
  n_groups<-length(group_nums)

  #first define temp data frame to hold plot data based on number of groups
  column_names <- c("group", "mean", "hdi_hi", "hdi_lo")
  plot_data <- data.frame(matrix(NA, nrow = n_groups, ncol = length(column_names)))
  colnames(plot_data) <- column_names

  #define mean and ci of each groups posterior
  for (i in 1:n_groups){
    plot_data$group[i]<-group_names[i]
    plot_data$mean[i]<-mean(data[[paste0(par_prefix,i,".")]])
    plot_data$hdi_lo[i]<-HDInterval::hdi(data[[paste0(par_prefix,i,".")]],credMass = ci)[1]
    plot_data$hdi_hi[i]<-HDInterval::hdi(data[[paste0(par_prefix,i,".")]],credMass = ci)[2]
  }

  #now calculate pairwise group differences
  combinations_nums<-as.data.frame(t(combn(group_nums, 2))) #unique combinations of groups
  combinations_names<-as.data.frame(t(combn(group_names, 2))) #unique combinations of groups

  for(i in 1:nrow(combinations_nums)){
    group1_id<-combinations_nums[i,1]
    group2_id<-combinations_nums[i,2]
    group1_name<-combinations_names[i,1]
    group2_name<-combinations_names[i,2]
    group1_size<-sample_sizes[which(group_names==group1_name)]
    group2_size<-sample_sizes[which(group_names==group2_name)]

    diff_posterior<-data[[paste0(par_prefix,group1_id,".")]]-data[[paste0(par_prefix,group2_id,".")]]
    diff_mean<-round(mean(diff_posterior),round_digits)
    diff_hdi_lo<-as.numeric(HDInterval::hdi(diff_posterior,credMass=ci)[1])
    diff_hdi_hi<-as.numeric(HDInterval::hdi(diff_posterior,credMass=ci)[2])

    group1_rawpar<-data[[paste0(rawpar_prefix,group1_id,".")]]
    group2_rawpar<-data[[paste0(rawpar_prefix,group2_id,".")]]
      
    if(is.na(sig_prefix)){
      cohen_d<-NA
    }else{
      #calculate pooled SD for cohen's d effect sizes
      group1_sigma<-data[[paste0(sig_prefix,group1_id,".")]]
      group2_sigma<-data[[paste0(sig_prefix,group2_id,".")]]

      sigma_pooled <- sqrt(((mean(group1_sigma)^2 + mean(group2_sigma)^2) / 2))#Cohen 1988
      cohen_d<-round(mean(group1_rawpar-group2_rawpar)/sigma_pooled,round_digits)
    }

    if((diff_hdi_hi>0&diff_hdi_lo>0)|(diff_hdi_hi<0&diff_hdi_lo<0)){
      cred_index<-'*'
    }else{
      cred_index<-''
    }

    diff_hdi_lo<-round(diff_hdi_lo,round_digits)
    diff_hdi_hi<-round(diff_hdi_hi,round_digits)
    
    savage_dickey_ratio<-function(posterior,prior,method){ 
  
  # logspline kernel density estimation (as in Wagenmakers 2010)
  if(method=="logspline"){
    library(logspline)
    
    log_spline_density<-invisible(logspline(posterior,silent=TRUE,error.action = 1)) #density estimate for posterior
    if(is.null(log_spline_density)==FALSE){
      posterior_density<-dlogspline(0, log_spline_density, log = F)  #density @ 0
      log_spline_density<-invisible(logspline(prior,silent=TRUE)) #density estimate for prior
      prior_density<-dlogspline(0, log_spline_density, log = F)  #density @ 0
      
    }else{
      library(MASS)
      
      # Fit normal distribution to posterior samples
      fit_posterior <- fitdistr(posterior, "normal")
      posterior_mean <- fit_posterior$estimate[1]
      posterior_sd <- fit_posterior$estimate[2]
      
      # Laplace approximation Bayes Factor calculation using likelihood ratio at 0 (mode of the normal distribution)
      posterior_density <- dnorm(0, mean = posterior_mean, sd = posterior_sd)
      prior_density <- dnorm(0, mean = 0, sd = 1)
    }
    
    # a group effect of 0.09 is ~11 times LESS likely than it was before seeing the data (i.e., 1/.09)
    bf<-prior_density/posterior_density
  }else if(method=="kde"){
    library(stats)
    
    kde_density1 <- density(posterior)
    posterior_density <- approx(kde_density1$x, kde_density1$y, xout = 0)$y
    
    # if density too low to be estimated, then use the smallest representable number that R allows
    if(is.na(posterior_density)){
      posterior_density<-.Machine$double.xmin
    }
    
    kde_density2 <- density(prior)
    prior_density <- approx(kde_density2$x, kde_density2$y, xout = 0)$y
    bf<-prior_density/posterior_density
  }
  
  #direction of BF evidence
  if(bf==1){
    bf_direction<-"No Evidence"
  }else if(bf<1){
    bf_direction<-"Null"
  }else{
    bf_direction<-"Alt"
  }
  
  # strength of bayes factor evidence. ranges from Lee and Wagenmakers (2014)
  if((bf >= 100)){
    bf_range <- "Extreme"
  }else if(bf >= 30 && bf < 100){
    bf_range <- "Very Strong"
  }else if(bf >= 10 && bf < 30){
    bf_range <- "Strong"
  }else if(bf >= 3 && bf < 10){
    bf_range <- "Moderate"
  }else if(bf >= 1 && bf < 3){
    bf_range <- "Anecdotal"
  }else if(bf == 1){
    bf_range <- "No Evidence"
  }else if(bf >= 0.33 && bf < 1){
    bf_range <- "Anecdotal"
  }else if(bf >= 0.1 && bf < 0.33){
    bf_range <- "Moderate"
  }else if(bf >= 0.03 && bf < 0.1){
    bf_range <- "Strong"
  }else if(bf >= 0.01 && bf < 0.03){
    bf_range <- "Very Strong"
  }else if(bf < 0.01){
    bf_range <- "Extreme"
  }else{
    bf_range <- ""
    
  }
  combine<-list(bf=bf,
                bf_range=bf_range,
                bf_direction=bf_direction)
  
  return(combine)
}

    bf<-round(savage_dickey_ratio(group1_rawpar-group2_rawpar,rnorm(length(diff_posterior),0,1),"logspline")[["bf"]],round_digits)
    
    temp_text<-paste0(group1_name,"-",group2_name,": [",diff_hdi_lo,",",diff_hdi_hi,"], M = ",diff_mean,cred_index,"\nBF = ",bf,", d = ",cohen_d)

    #if not the last comparison, add a line break to the text
    if(i != nrow(combinations_nums)){
      temp_text<-paste0(temp_text,"\n")
    }
    if(i==1){
      text<-temp_text
    }else{
      text<-paste0(text,temp_text)
    }
  }

  #after we're done with all the combinations, make the plot and return it

  if(min(plot_data$hdi_lo)<0){
    yaxis_min<-min(plot_data$hdi_lo)*1.1
  }else{
    yaxis_min<-min(plot_data$hdi_lo)-(min(plot_data$hdi_lo)*.1)
  }

  if(max(plot_data$hdi_hi)<0){
    ytext_position<-max(plot_data$hdi_hi)-(max(plot_data$hdi_hi)*.1)
    yaxis_max<-max(plot_data$hdi_hi)-(max(plot_data$hdi_hi)*.15)
  }else{
    ytext_position<-max(plot_data$hdi_hi)+(max(plot_data$hdi_hi)*.1)
    yaxis_max<-max(plot_data$hdi_hi)+(max(plot_data$hdi_hi)*.15)
  }

  xtext_position<-mean(group_nums)

  if(showtext==FALSE){
    text<-""
  }

  p<-ggplot(data=plot_data, aes(x=group, y=mean,color=group,alpha=.4)) +
  geom_point(data=plot_data,alpha=1,position=position_dodge(width = .5),size=4) + theme_bw()+
  geom_errorbar(data=plot_data,aes(ymin=hdi_lo,ymax=hdi_hi),linewidth=1,
                width=0,alpha=1,position=position_dodge(width = .5)) +
  coord_cartesian(ylim = c(yaxis_min,yaxis_max))+
  ggtitle(par_name) + xlab("") +
  scale_x_discrete(limits=group_names)+
  scale_color_manual(values=c('#57b6c9','#966acc'))+
  ylab(par_name)+
  theme(axis.text.x = element_text(angle = 0,vjust=1,hjust=0.5,size=10),
        axis.text.y = element_text(size=10))+
  annotate(geom = "text", x = xtext_position, y = ytext_position, label = text, hjust = "center",size=3.5,lineheight=.9)
  p
  return(p)

}

# calculate and plot group differences
params<-c('mu_alpha.','mu_beta.','mu_ndt.','mu_grp_b1_pr.','mu_delta.')
raw_params<-c('mu_grp_alpha_pr.','mu_grp_beta_pr.','mu_grp_ndt_pr.','mu_grp_b1_pr.','mu_grp_delta_pr.')
sig_params<-c('sig_grp_alpha_pr.','sig_grp_beta_pr.','sig_grp_ndt_pr.','sig_grp_b1_pr.','sig_grp_delta_pr.')
param_names<-c('Threshold Separation','Start Point','NDT','Drift Sensitivity','Drift Bias')

# "data" = data frame of posterior samples. col names should be paste0(par_prefix,group_num,".") to work properly with this function

#loop through and plot/compare group differences
for(i in 1:length(params)){

  p<-plot_posterior_diffs(
    data=gaze2_all_samples,
    ci=.95, #credible interval
    par_prefix=params[i],
    rawpar_prefix=raw_params[i],
    sig_prefix=sig_params[i],
    par_name = param_names[i],
    group_nums = c(1,2),
    group_names=c('HC','SZ'),
    sample_sizes=c(39,42),
    round_digits=3,
    showtext=TRUE)
  print(p)
}

```

## Model 3 (Gender)

```{r id99322, echo=FALSE, cache=TRUE,fig.width=4,fig.height=3.75}

params<-c('mu_alpha.','mu_beta.','mu_ndt.','mu_delta.','mu_delta_bias.')
raw_params<-c('mu_grp_alpha_pr.','mu_grp_beta_pr.','mu_grp_ndt_pr.','mu_grp_delta_pr.','mu_grp_delta_bias_pr.')
sig_params<-c('sig_grp_alpha_pr.','sig_grp_beta_pr.','sig_grp_ndt_pr.','sig_grp_delta_pr.','sig_grp_delta_pr.')
param_names<-c('Threshold Separation','Start Point','NDT','Drift Sensitivity','Drift Bias')

#loop through and plot/compare group differences
for(i in 1:length(params)){

  p<-plot_posterior_diffs(
    data=gaze3_all_samples,
    ci=.95, #credible interval
    par_prefix=params[i],
    rawpar_prefix=raw_params[i],
    sig_prefix=sig_params[i],
    par_name = param_names[i],
    group_nums = c(1,2),
    group_names=c('HC','SZ'),
    sample_sizes=c(39,42),
    round_digits=3,
    showtext = TRUE)
  print(p)
}
#

```

## Model 5 (Visual Integration)

```{r id99332222212322999, echo=FALSE, cache=TRUE,fig.width=4,fig.height=3.75}

params<-c('mu_alpha.','mu_beta.','mu_ndt.','mu_delta.','mu_grp_b1_pr.')
raw_params<-c('mu_grp_alpha_pr.','mu_grp_beta_pr.','mu_grp_ndt_pr.','mu_grp_delta_pr.','mu_grp_b1_pr.')
sig_params<-c('sig_grp_alpha_pr.','sig_grp_beta_pr.','sig_grp_ndt_pr.','sig_grp_delta_pr.','sig_grp_b1_pr.')
param_names<-c('Threshold Separation','Start Point','NDT','Drift Bias','Drift Sensitivity')

#loop through and plot/compare group differences
for(i in 1:length(params)){
  p<-plot_posterior_diffs(
    data=jovi_all_samples,
    ci=.95, #credible interval
    par_prefix=params[i],
    rawpar_prefix=raw_params[i],
    sig_prefix=sig_params[i],
    par_name = param_names[i],
    group_nums = c(1,2),
    group_names=c('HC','SZ'),
    sample_sizes=c(39,42),
    round_digits=3,
    showtext = TRUE)
  print(p)
}

```

```{r id99332122322999, fig.height=3.3, fig.width=1.5, cache=TRUE, dpi=500, include=FALSE}

## Plots for Figure 2

hlines<-c(0,.5,0,0,0)

title_size<-10
tick_size<-9
axis_title_size<-9

# Eye Contact condition
params<-c('mu_alpha.','mu_beta.','mu_ndt.','mu_grp_b1_pr.','mu_delta.')
raw_params<-c('mu_grp_alpha_pr.','mu_grp_beta_pr.','mu_grp_ndt_pr.','mu_grp_b1_pr.','mu_grp_delta_pr.')
sig_params<-c('sig_grp_alpha_pr.','sig_grp_beta_pr.','sig_grp_ndt_pr.','sig_grp_b1_pr.','sig_grp_delta_pr.')
param_names<-c('Threshold Sep','Start Point','NDT','Drift Sensitivity*','Drift Bias')

ylims<-list(c(1.6,2.4),
            c(.45,.58),
            c(.85,.98),
            c(.15,.6),
            c(-2,.5))

for(i in 1:length(params)){
  p<-plot_posterior_diffs(
    data=gaze2_all_samples, 
    ci=.95, 
    par_prefix=params[i],
    rawpar_prefix=raw_params[i],
    sig_prefix=sig_params[i],
    par_name = param_names[i],
    group_nums = c(1,2),
    group_names=c('HC','SZ'),
    sample_sizes=c(39,42),
    round_digits=3,
    showtext=F)
  print(p + coord_cartesian(ylim = ylims[[i]])+
          theme(legend.position = "none",
                plot.title = element_text(size = title_size, hjust = -0.1),
                #plot.title=element_text(size=title_size),
                axis.title.y=element_text(size=axis_title_size),
                axis.text.x=element_text(size=tick_size),
                axis.text.y=element_text(size=tick_size))+
          geom_hline(yintercept=hlines[i],linewidth = 2,alpha=.5))
}

# gender condition
params<-c('mu_alpha.','mu_beta.','mu_ndt.','mu_delta_bias.','mu_delta.')
raw_params<-c('mu_grp_alpha_pr.','mu_grp_beta_pr.','mu_grp_ndt_pr.','mu_grp_delta_bias_pr.','mu_grp_delta_pr.')
sig_params<-c('sig_grp_alpha_pr.','sig_grp_beta_pr.','sig_grp_ndt_pr.','sig_grp_delta_pr.','sig_grp_delta_pr.')
param_names<-c('Threshold Sep','Start Point','NDT','Drift Bias','Drift Sensitivity*')

ylims<-list(c(1.6,2.4),
            c(.45,.58),
            c(.85,.98),
            c(-2.1,.5),
            c(2,3.1))

for(i in 1:length(params)){
  p<-plot_posterior_diffs(
    data=gaze3_all_samples, 
    ci=.95,
    par_prefix=params[i],
    rawpar_prefix=raw_params[i],
    sig_prefix=sig_params[i],
    par_name = param_names[i],
    group_nums = c(1,2),
    group_names=c('HC','SZ'),
    sample_sizes=c(39,42),
    round_digits=3,
    showtext=F)
  
  print(p + coord_cartesian(ylim = ylims[[i]])+
          theme(legend.position = "none",
                plot.title=element_text(size=title_size,hjust = -0.1),
                axis.title.y=element_text(size=axis_title_size),
                axis.text.x=element_text(size=tick_size),
                axis.text.y=element_text(size=tick_size))+
          geom_hline(yintercept=hlines[i],linewidth = 2,alpha=.5))
}

# jovi condition
params<-c('mu_alpha.','mu_beta.','mu_ndt.','mu_grp_b1_pr.','mu_delta.')
raw_params<-c('mu_grp_alpha_pr.','mu_grp_beta_pr.','mu_grp_ndt_pr.','mu_grp_b1_pr.','mu_grp_delta_pr.')
sig_params<-c('sig_grp_alpha_pr.','sig_grp_beta_pr.','sig_grp_ndt_pr.','sig_grp_b1_pr.','sig_grp_delta_pr.')
param_names<-c('Threshold Sep','Start Point','NDT','Drift Sensitivity*','Drift Bias')

ylims<-list(c(1.6,2.4),
            c(.45,.58),
            c(.85,.98),
            c(.15,.6),
            c(-2,.5))

for(i in 1:length(params)){
  p<-plot_posterior_diffs(
    data=jovi_all_samples, 
    ci=.95, 
    par_prefix=params[i],
    rawpar_prefix=raw_params[i],
    sig_prefix=sig_params[i],
    par_name = param_names[i],
    group_nums = c(1,2),
    group_names=c('HC','SZ'),
    sample_sizes=c(39,42),
    round_digits=3,
    showtext=F)
  
  print(p + coord_cartesian(ylim = ylims[[i]])+
          theme(legend.position = "none",
                plot.title=element_text(size=title_size,hjust = -0.1),
                axis.title.y=element_text(size=axis_title_size),
                axis.text.x=element_text(size=tick_size),
                axis.text.y=element_text(size=tick_size))+
          geom_hline(yintercept=hlines[i],linewidth = 2,alpha=.5))
}

```

# Regressions (All) 

__Do model parameters predict clinical correlates dimensionally (after controlling for age and sex) or do these relationships differ credibly between between HC and SZ?__

Bayesian regression models run in brms examined the effects of social cue processing efficiency for DDM parameters, schizophrenia diagnosis (sz_dummy; SZ=1, HC=0), and their interaction on key clinical outcomes (GPTS referential thinking and persecution; social cognition). All measures were z-scored, and evidence was assessed using Bayes factors (BF) via the Savage-Dickey ratio and 95% Highest Density Intervals (HDIs). We used weakly informative priors for predictors (Normal(0,1)) and between-subject variance parameters (student_t(3,0,2.5)).

## Primary {.tabset}

### GPTS-Referential Thinking
 
```{r id9932991112121331, echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE, cache=TRUE}

tmp_pred<-c('delta_bias','delta_bias_gend','delta_bias_jovi',
            'delta_b1','delta_gend','delta_b1_jovi',
            'beta','beta_gend','beta_jovi','sz_dummy',
            'female_dummy','age')

tmp_outcome<-c('gpts_socref')

tmp_data<-subset(all_sub_data,all_sub_data$group=="SZ"|all_sub_data$group=="HC")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])

#define models to test (list predictors in order they should be entered) 
test_models<-list(c("age","female_dummy","sz_dummy"),
                  c("age","female_dummy","sz_dummy","delta_b1","sz_dummy:delta_b1"),
                  c("age","female_dummy","sz_dummy","delta_bias","sz_dummy:delta_bias"),
                  c("age","female_dummy","sz_dummy","beta","sz_dummy:beta"),
                  c("age","female_dummy","sz_dummy","delta_gend","sz_dummy:delta_gend"),
                  c("age","female_dummy","sz_dummy","delta_b1_jovi","sz_dummy:delta_b1_jovi"))

outcome<-"gpts_socref" #outcome variable
outfile<-paste0(dirname,"/analysis/full/saved_brms_models_1a1.rds")

result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=outfile,
                         credMass=.95,
                         nu=4) #student t degrees of freedom

result_glm_ddm$Pred<-rename_corr_pars(result_glm_ddm$Pred)

result_glm_ddm %>%
  kbl(caption = "Bayesian Regression: Predicting GPTS Referential Thinking (Full Sample, Controlling for Age, Sex)",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T) 

```
 
### GPTS-Persecution

```{r id99329911121331121, echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE, cache=TRUE}

tmp_pred<-c('delta_bias','delta_bias_gend','delta_bias_jovi',
            'delta_b1','delta_gend','delta_b1_jovi',
            'beta','beta_gend','beta_jovi','sz_dummy',
            'female_dummy','age')


tmp_outcome<-c('gpts_persec')

tmp_data<-subset(all_sub_data,all_sub_data$group=="SZ"|all_sub_data$group=="HC")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])

#define models to test (list predictors in order they should be entered)
test_models<-list(c("age","female_dummy","sz_dummy"),
                  c("age","female_dummy","sz_dummy","delta_b1","sz_dummy:delta_b1"),
                  c("age","female_dummy","sz_dummy","delta_bias","sz_dummy:delta_bias"),
                  c("age","female_dummy","sz_dummy","beta","sz_dummy:beta"),
                  c("age","female_dummy","sz_dummy","delta_gend","sz_dummy:delta_gend"),
                  c("age","female_dummy","sz_dummy","delta_b1_jovi","sz_dummy:delta_b1_jovi"))

outcome<-"gpts_persec" #outcome
outfile<-paste0(dirname,"/analysis/full/saved_brms_models_2a1.rds")

result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=outfile,
                         credMass=.95,
                         nu=4) #student t df

result_glm_ddm$Pred<-rename_corr_pars(result_glm_ddm$Pred)

result_glm_ddm %>%
  kbl(caption = "Bayesian Regression: Predicting GPTS Persecution (Full Sample, Controlling for Age, Sex)",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T) 

```

### Social Cognition

```{r id99329933121412111211, echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE, cache=TRUE}
########
tmp_pred<-c('delta_bias','delta_bias_gend','delta_bias_jovi',
            'delta_b1','delta_gend','delta_b1_jovi',
            'beta','beta_gend','beta_jovi','sz_dummy',
            'female_dummy','age')

tmp_outcome<-c('soc_cog_pc_wrt')

tmp_data<-subset(all_sub_data,all_sub_data$group=="SZ"|all_sub_data$group=="HC")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])

#define models to test (list predictors in order they should be entered)
test_models<-list(c("age","female_dummy","sz_dummy"),
                  c("age","female_dummy","sz_dummy","delta_b1","sz_dummy:delta_b1"),
                  c("age","female_dummy","sz_dummy","delta_bias","sz_dummy:delta_bias"),
                  c("age","female_dummy","sz_dummy","beta","sz_dummy:beta"),
                  c("age","female_dummy","sz_dummy","delta_gend","sz_dummy:delta_gend"),
                  c("age","female_dummy","sz_dummy","delta_b1_jovi","sz_dummy:delta_b1_jovi"))

outcome<-"soc_cog_pc_wrt" #outcome variable
outfile<-paste0(dirname,"/analysis/full/saved_brms_models_3a1.rds")

result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=outfile,
                         credMass=.95,
                         nu=4)

result_glm_ddm$Pred<-rename_corr_pars(result_glm_ddm$Pred)

result_glm_ddm %>%
  kbl(caption = "Bayesian Regression: Predicting Social Cognition (Full Sample, Controlling for Age, Sex)",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T)

```

## Sensitivity - Controlling for General Cognition (MCCB) {.tabset}

__Re-run those models with credible diagnosis*drift sensitivity interactions while also controlling for general cognitive ability (i.e., a composite of performance on MCCB)__

### GPTS-Referential Thinking

```{r id99329911121921331, echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE, cache=TRUE}

tmp_pred<-c('delta_b1','delta_gend','sz_dummy','female_dummy','age','matrics')
tmp_outcome<-c('gpts_socref')

tmp_data<-subset(all_sub_data,all_sub_data$group=="SZ"|all_sub_data$group=="HC")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])

#define models to test (list predictors in order they should be entered)
test_models<-list(c("age","female_dummy","sz_dummy","matrics"),
                  c("age","female_dummy","sz_dummy","matrics","delta_b1","sz_dummy:delta_b1"),
                  c("age","female_dummy","sz_dummy","matrics","delta_gend","sz_dummy:delta_gend"))

outcome<-"gpts_socref" #outcome variable
outfile<-paste0(dirname,"/analysis/full/saved_brms_models_1a2.rds")

result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=outfile,
                         credMass=.95,
                         nu=4) #student t degrees of freedom

result_glm_ddm$Pred<-rename_corr_pars(result_glm_ddm$Pred)

result_glm_ddm %>%
  kbl(caption = "Bayesian Regression: Predicting GPTS Referential Thinking (Full Sample, Controlling for Age, Sex, General Cognition)",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T)

```

### GPTS-Persecution

```{r id993299111213931121, echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE, cache=TRUE}
#
tmp_pred<-c('delta_b1','delta_gend','matrics','sz_dummy','female_dummy','age')
tmp_outcome<-c('gpts_persec')

tmp_data<-subset(all_sub_data,all_sub_data$group=="SZ"|all_sub_data$group=="HC")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])

#define models to test (list predictors in order they should be entered)
test_models<-list(c("age","female_dummy","sz_dummy","matrics"),
                  c("age","female_dummy","sz_dummy","matrics","delta_b1","sz_dummy:delta_b1"),
                  c("age","female_dummy","sz_dummy","matrics","delta_gend","sz_dummy:delta_gend"))

outcome<-"gpts_persec" #outcome
outfile<-paste0(dirname,"/analysis/full/saved_brms_models_2a2.rds")

result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=outfile,
                         credMass=.95,
                         nu=4) #student t df

result_glm_ddm$Pred<-rename_corr_pars(result_glm_ddm$Pred)

result_glm_ddm %>%
  kbl(caption = "Bayesian Regression: Predicting GPTS Persecution (Full Sample, Controlling for Age, Sex, General Cognition)",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T)

```

## Sensitivity - Controlling for General Perceptual Processing (Visual Integration) {.tabset}

__Re-run those models with credible diagnosis*drift sensitivity interactions while also controlling for general perceptual processing ability (i.e., drift sensitivity parameter from Visual Integration task).__

### GPTS-Referential Thinking

```{r id99329911121ss921331, echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE, cache=TRUE}

tmp_pred<-c('delta_b1','delta_gend','sz_dummy','female_dummy','age','delta_b1_jovi')
tmp_outcome<-c('gpts_socref')

tmp_data<-subset(all_sub_data,all_sub_data$group=="SZ"|all_sub_data$group=="HC")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])

#define models to test (list predictors in order they should be entered)
test_models<-list(c("age","female_dummy","sz_dummy","delta_b1_jovi"),
                  c("age","female_dummy","sz_dummy","delta_b1_jovi","delta_b1","sz_dummy:delta_b1"),
                  c("age","female_dummy","sz_dummy","delta_b1_jovi","delta_gend","sz_dummy:delta_gend"))

outcome<-"gpts_socref" #outcome variable
outfile<-paste0(dirname,"/analysis/full/saved_brms_models_1a2b.rds")

result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=outfile,
                         credMass=.95,
                         nu=4) #student t degrees of freedom

result_glm_ddm$Pred<-rename_corr_pars(result_glm_ddm$Pred)

result_glm_ddm %>%
  kbl(caption = "Bayesian Regression: Predicting GPTS Referential Thinking (Full Sample, Controlling for Age, Sex, General Perceptual Processing [Visual Integration])",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T)

```

### GPTS-Persecution

```{r id9932991112139312121, echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE, cache=TRUE}
#
tmp_pred<-c('delta_b1','delta_gend','delta_b1_jovi','sz_dummy','female_dummy','age')
tmp_outcome<-c('gpts_persec')

tmp_data<-subset(all_sub_data,all_sub_data$group=="SZ"|all_sub_data$group=="HC")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])

#define models to test (list predictors in order they should be entered)
test_models<-list(c("age","female_dummy","sz_dummy","delta_b1_jovi"),
                  c("age","female_dummy","sz_dummy","delta_b1_jovi","delta_b1","sz_dummy:delta_b1"),
                  c("age","female_dummy","sz_dummy","delta_b1_jovi","delta_gend","sz_dummy:delta_gend"))

outcome<-"gpts_persec" #outcome
outfile<-paste0(dirname,"/analysis/full/saved_brms_models_2a2b.rds")

result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=outfile,
                         credMass=.95,
                         nu=4) #student t df

result_glm_ddm$Pred<-rename_corr_pars(result_glm_ddm$Pred)

result_glm_ddm %>%
  kbl(caption = "Bayesian Regression: Predicting GPTS Persecution (Full Sample, Controlling for Age, Sex, General Perceptual Processing [Visual Integration])",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T)

```



# Regressions (SZ Only)

## Primary {.tabset}

__For primary regressions in which DDM parameters showed interactions with SZ diagnosis to predict any of our clinical outcomes of interest, re-run within group__

### GPTS-Referential Thinking

```{r id9932991112, echo=FALSE, fig.height=4, fig.show='asis', fig.width=5, message=FALSE, warning=FALSE, cache=TRUE, results='asis'}

show_modelchange<-TRUE
options(mc.cores = parallel::detectCores())
tmp_pred<-c('delta_b1','delta_gend','delta_b1_jovi','age','female_dummy')
tmp_outcome<-c('gpts_socref')
tmp_data<-subset(all_sub_data,all_sub_data$group=="SZ")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) 
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])
pars<-c("delta_b1","delta_gend",'delta_b1_jovi')
mod_suffix<-c("b","c","d","e")
outcome<-"gpts_socref"
outcome_name<-"GPTS Referential Thinking (SZ Only)"
outcome_prefix<-1

for(i in 1:length(pars)){
  
  #define models to test (list predictors in order they should be entered)
  test_models<-list(c("age","female_dummy"),
                    c("age","female_dummy",pars[i]))
  
  result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=paste0(dirname,
                                        paste0("/analysis/full/saved_brms_models_",
                                                       outcome_prefix,
                                                       mod_suffix[i],".rds")),
                         credMass=.95,
                         nu=4)
  
  if(i!=1){
    startrow<-which(result_glm_ddm$Model==2)
    endrow<-nrow(result_glm_ddm)
    result_glm_ddm<-result_glm_ddm[c(startrow:endrow),]
  }
  
  if(show_modelchange==FALSE){
    startcol<-which(colnames(result_glm_ddm)=="Ref Model")
    endcol<-which(colnames(result_glm_ddm)=="Pred")-1
    result_glm_ddm<-result_glm_ddm[,-c(startcol:endcol)]
    result_glm_ddm$Model[1]<-i
  }
  
  if(i==1){
    all_result_glm_ddm<-result_glm_ddm
  }else{
    all_result_glm_ddm<-rbind(all_result_glm_ddm,result_glm_ddm)
  }
}

rownames(all_result_glm_ddm)<-NULL
all_result_glm_ddm$Pred<-rename_corr_pars(all_result_glm_ddm$Pred)

print(all_result_glm_ddm %>%
  kbl(caption = paste0("Bayesian Regression: Predicting ", outcome_name),valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T))

```

### GPTS-Persecution

```{r id993299111218, echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE, cache=TRUE,fig.show='asis',results='asis'}
#####
show_modelchange<-TRUE
tmp_pred<-c('delta_b1','delta_gend','delta_b1_jovi','age','female_dummy')
tmp_outcome<-c('gpts_persec')
tmp_data<-subset(all_sub_data,all_sub_data$group=="SZ")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])
pars<-c("delta_b1","delta_gend",'delta_b1_jovi')
mod_suffix<-c("b","c","d","e")
outcome<-"gpts_persec"
outcome_name<-"GPTS Persecution (SZ Only)"
outcome_prefix<-2

for(i in 1:length(pars)){
  
  #define models to test (list predictors in order they should be entered)
  test_models<-list(c("age","female_dummy"),
                    c("age","female_dummy",pars[i]))
  
  result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=paste0(dirname,paste0("/analysis/full/saved_brms_models_",
                                                       outcome_prefix,
                                                       mod_suffix[i],".rds")),
                         credMass=.95,
                         nu=4)
  
  if(i!=1){
    startrow<-which(result_glm_ddm$Model==2)
    endrow<-nrow(result_glm_ddm)
    result_glm_ddm<-result_glm_ddm[c(startrow:endrow),]
  }
  
  if(show_modelchange==FALSE){
    startcol<-which(colnames(result_glm_ddm)=="Ref Model")
    endcol<-which(colnames(result_glm_ddm)=="Pred")-1
    result_glm_ddm<-result_glm_ddm[,-c(startcol:endcol)]
    result_glm_ddm$Model[1]<-i
  }
  
  if(i==1){
    all_result_glm_ddm<-result_glm_ddm
  }else{
    all_result_glm_ddm<-rbind(all_result_glm_ddm,result_glm_ddm)
  }
}

rownames(all_result_glm_ddm)<-NULL
all_result_glm_ddm$Pred<-rename_corr_pars(all_result_glm_ddm$Pred)

print(all_result_glm_ddm %>%
  kbl(caption = paste0("Bayesian Regression: Predicting ", outcome_name),valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T))
```

### Social Cognition

```{r id9932991112118, echo=FALSE, fig.height=4, fig.show='asis', fig.width=5, message=FALSE, warning=FALSE, cache=TRUE, results='asis'}
######
show_modelchange<-TRUE
tmp_pred<-c('delta_b1','delta_bias','delta_gend','delta_b1_jovi','age','female_dummy')
tmp_outcome<-c('soc_cog_pc_wrt')
tmp_data<-subset(all_sub_data,all_sub_data$group=="SZ")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])
pars<-c("delta_b1","delta_bias",'delta_gend','delta_b1_jovi')
mod_suffix<-c("b","c","d","e")
outcome<-"soc_cog_pc_wrt"
outcome_name<-"Social Cognition (SZ Only)"
outcome_prefix<-3

for(i in 1:length(pars)){
  
  #define models to test (list predictors in order they should be entered)
  test_models<-list(c("age","female_dummy"),
                    c("age","female_dummy",pars[i]))
  
  result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=paste0(dirname,paste0("/analysis/full/saved_brms_models_",
                                                       outcome_prefix,
                                                       mod_suffix[i],".rds")),
                         credMass=.95,
                         nu=4)
  
  if(i!=1){
    startrow<-which(result_glm_ddm$Model==2)
    endrow<-nrow(result_glm_ddm)
    result_glm_ddm<-result_glm_ddm[c(startrow:endrow),]
  }
  
  if(show_modelchange==FALSE){
    startcol<-which(colnames(result_glm_ddm)=="Ref Model")
    endcol<-which(colnames(result_glm_ddm)=="Pred")-1
    result_glm_ddm<-result_glm_ddm[,-c(startcol:endcol)]
    result_glm_ddm$Model[1]<-i
  }
  
  if(i==1){
    all_result_glm_ddm<-result_glm_ddm
  }else{
    all_result_glm_ddm<-rbind(all_result_glm_ddm,result_glm_ddm)
  }
}

rownames(all_result_glm_ddm)<-NULL
all_result_glm_ddm$Pred<-rename_corr_pars(all_result_glm_ddm$Pred)

print(all_result_glm_ddm %>%
  kbl(caption = paste0("Bayesian Regression: Predicting ", outcome_name),valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T))
```

{-}
 
## Sensitivity - Controlling for General Cognition (MCCB) {.tabset}

__Re-run those within group regressions to see if effects remain after controlling for general cognitive ability (i.e., composite from MCCB).__

### GPTS-Referential Thinking

```{r id99329911129, echo=FALSE, fig.height=4, fig.show='asis', fig.width=5, message=FALSE, warning=FALSE, cache=TRUE, results='asis'}
#####
show_modelchange<-TRUE
options(mc.cores = parallel::detectCores())
tmp_pred<-c('delta_b1','delta_gend','age','female_dummy','matrics','delta_b1_jovi')
tmp_outcome<-c('gpts_socref')
tmp_data<-subset(all_sub_data,all_sub_data$group=="SZ")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])
pars<-c("delta_b1","delta_gend")
mod_suffix<-c("b","c")
outcome<-"gpts_socref"
outcome_name<-"GPTS Referential Thinking (SZ Only)"
outcome_prefix<-"1.1"

for(i in 1:length(pars)){
  
  #define models to test (list predictors in order they should be entered)
  test_models<-list(c("age","female_dummy","matrics"),
                    c("age","female_dummy","matrics",pars[i]))
  
  result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=paste0(dirname,
                                        paste0("/analysis/full/saved_brms_models_",
                                                       outcome_prefix,
                                                       mod_suffix[i],".rds")),
                         credMass=.95,
                         nu=4)
  
    if(i!=1){
    startrow<-which(result_glm_ddm$Model==2)
    endrow<-nrow(result_glm_ddm)
    result_glm_ddm<-result_glm_ddm[c(startrow:endrow),]
  }
  
  if(show_modelchange==FALSE){
    startcol<-which(colnames(result_glm_ddm)=="Ref Model")
    endcol<-which(colnames(result_glm_ddm)=="Pred")-1
    result_glm_ddm<-result_glm_ddm[,-c(startcol:endcol)]
    result_glm_ddm$Model[1]<-i
  }
  
  if(i==1){
    all_result_glm_ddm<-result_glm_ddm
  }else{
    all_result_glm_ddm<-rbind(all_result_glm_ddm,result_glm_ddm)
  }
}

rownames(all_result_glm_ddm)<-NULL
all_result_glm_ddm$Pred<-rename_corr_pars(all_result_glm_ddm$Pred)

print(all_result_glm_ddm %>%
  kbl(caption = paste0("Bayesian Regression: Predicting ", outcome_name),valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T))

```
 
### GPTS-Persecution

```{r id9932991112218, echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE, cache=TRUE,fig.show='asis',results='asis'}
#
show_modelchange<-TRUE
tmp_pred<-c('delta_b1','delta_gend','age','female_dummy','matrics','delta_b1_jovi')
tmp_outcome<-c('gpts_persec')
tmp_data<-subset(all_sub_data,all_sub_data$group=="SZ")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])
pars<-c("delta_b1","delta_gend")
mod_suffix<-c("b","c")
outcome<-"gpts_persec"
outcome_name<-"GPTS Persecution (SZ Only)"
outcome_prefix<-"2.1"

for(i in 1:length(pars)){
  
  #define models to test (list predictors in order they should be entered)
  test_models<-list(c("age","female_dummy","matrics"),
                    c("age","female_dummy","matrics",pars[i]))
  
  result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=paste0(dirname,paste0("/analysis/full/saved_brms_models_",
                                                       outcome_prefix,
                                                       mod_suffix[i],".rds")),
                         credMass=.95,
                         nu=4)
  
    if(i!=1){
    startrow<-which(result_glm_ddm$Model==2)
    endrow<-nrow(result_glm_ddm)
    result_glm_ddm<-result_glm_ddm[c(startrow:endrow),]
  }
  
  if(show_modelchange==FALSE){
    startcol<-which(colnames(result_glm_ddm)=="Ref Model")
    endcol<-which(colnames(result_glm_ddm)=="Pred")-1
    result_glm_ddm<-result_glm_ddm[,-c(startcol:endcol)]
    result_glm_ddm$Model[1]<-i
  }
  
  if(i==1){
    all_result_glm_ddm<-result_glm_ddm
  }else{
    all_result_glm_ddm<-rbind(all_result_glm_ddm,result_glm_ddm)
  }
}

rownames(all_result_glm_ddm)<-NULL
all_result_glm_ddm$Pred<-rename_corr_pars(all_result_glm_ddm$Pred)

print(all_result_glm_ddm %>%
  kbl(caption = paste0("Bayesian Regression: Predicting ", outcome_name),valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T))
```
 
## Sensitivity - Controlling for General Perceptual Processing (Visual Integration) {.tabset}

__Re-run those follow-up within group regressions to see if effects remain after controlling for general perceptual processing ability (i.e., DDM parameters from the Visual Integration task).__

### GPTS-Referential Thinking

```{r id993292222911129, echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE, cache=TRUE,fig.show='asis',results='asis'}
#
show_modelchange<-TRUE
options(mc.cores = parallel::detectCores())
tmp_pred<-c('delta_b1','delta_gend','age','female_dummy','delta_b1_jovi')
tmp_outcome<-c('gpts_socref')
tmp_data<-subset(all_sub_data,all_sub_data$group=="SZ")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])
pars<-c("delta_b1","delta_gend")
mod_suffix<-c("b","c")
outcome<-"gpts_socref"
outcome_name<-"GPTS Referential Thinking (SZ Only)"
outcome_prefix<-"1.1.1"

for(i in 1:length(pars)){
  
  #define models to test (list predictors in order they should be entered)
  test_models<-list(c("age","female_dummy","delta_b1_jovi"),
                    c("age","female_dummy","delta_b1_jovi",pars[i]))
  
  result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=paste0(dirname,
                                        paste0("/analysis/full/saved_brms_models_",
                                                       outcome_prefix,
                                                       mod_suffix[i],".rds")),
                         credMass=.95,
                         nu=4)
  
    if(i!=1){
    startrow<-which(result_glm_ddm$Model==2)
    endrow<-nrow(result_glm_ddm)
    result_glm_ddm<-result_glm_ddm[c(startrow:endrow),]
  }
  
  if(show_modelchange==FALSE){
    startcol<-which(colnames(result_glm_ddm)=="Ref Model")
    endcol<-which(colnames(result_glm_ddm)=="Pred")-1
    result_glm_ddm<-result_glm_ddm[,-c(startcol:endcol)]
    result_glm_ddm$Model[1]<-i
  }
  
  if(i==1){
    all_result_glm_ddm<-result_glm_ddm
  }else{
    all_result_glm_ddm<-rbind(all_result_glm_ddm,result_glm_ddm)
  }
}

rownames(all_result_glm_ddm)<-NULL
all_result_glm_ddm$Pred<-rename_corr_pars(all_result_glm_ddm$Pred)

print(all_result_glm_ddm %>%
  kbl(caption = paste0("Bayesian Regression: Predicting ", outcome_name),valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T))

```
 
### GPTS-Persecution

```{r id9932991112222218, echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE, cache=TRUE,fig.show='asis',results='asis'}
#
show_modelchange<-TRUE
tmp_pred<-c('delta_b1','delta_gend','age','female_dummy','delta_b1_jovi')
tmp_outcome<-c('gpts_persec')
tmp_data<-subset(all_sub_data,all_sub_data$group=="SZ")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])
pars<-c("delta_b1","delta_gend")
mod_suffix<-c("b","c")
outcome<-"gpts_persec"
outcome_name<-"GPTS Persecution (SZ Only)"
outcome_prefix<-"2.1.1"

for(i in 1:length(pars)){
  
  #define models to test (list predictors in order they should be entered)
  test_models<-list(c("age","female_dummy","delta_b1_jovi"),
                    c("age","female_dummy","delta_b1_jovi",pars[i]))
  
  result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=paste0(dirname,paste0("/analysis/full/saved_brms_models_",
                                                       outcome_prefix,
                                                       mod_suffix[i],".rds")),
                         credMass=.95,
                         nu=4)
  
    if(i!=1){
    startrow<-which(result_glm_ddm$Model==2)
    endrow<-nrow(result_glm_ddm)
    result_glm_ddm<-result_glm_ddm[c(startrow:endrow),]
  }
  
  if(show_modelchange==FALSE){
    startcol<-which(colnames(result_glm_ddm)=="Ref Model")
    endcol<-which(colnames(result_glm_ddm)=="Pred")-1
    result_glm_ddm<-result_glm_ddm[,-c(startcol:endcol)]
    result_glm_ddm$Model[1]<-i
  }
  
  if(i==1){
    all_result_glm_ddm<-result_glm_ddm
  }else{
    all_result_glm_ddm<-rbind(all_result_glm_ddm,result_glm_ddm)
  }
}

rownames(all_result_glm_ddm)<-NULL
all_result_glm_ddm$Pred<-rename_corr_pars(all_result_glm_ddm$Pred)

print(all_result_glm_ddm %>%
  kbl(caption = paste0("Bayesian Regression: Predicting ", outcome_name),valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T))
```

# Regressions (HC Only)

__For primary regressions in which DDM parameters showed interactions with SZ diagnosis to predict any of our clinical outcomes of interest, re-run within group__

## Primary {.tabset}

### GPTS-Referential Thinking

```{r id99329911121, echo=FALSE, fig.height=4, fig.show='asis', fig.width=5, message=FALSE, warning=FALSE, cache=TRUE, results='asis'}

show_modelchange<-TRUE
tmp_pred<-c('delta_b1','delta_gend','delta_b1_jovi','age','female_dummy')
tmp_outcome<-c('gpts_socref')
tmp_data<-subset(all_sub_data,all_sub_data$group=="HC")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])
pars<-c("delta_b1","delta_gend",'delta_b1_jovi')
mod_suffix<-c("h","i","j","k")
outcome<-"gpts_socref"
outcome_name<-"GPTS Referential Thinking (HC Only)"
outcome_prefix<-1

for(i in 1:length(pars)){
  
  #define models to test (list predictors in order they should be entered)
  test_models<-list(c("age","female_dummy"),
                    c("age","female_dummy",pars[i]))
  
  result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=paste0(dirname,paste0("/analysis/full/saved_brms_models_",
                                                       outcome_prefix,
                                                       mod_suffix[i],".rds")),
                         credMass=.95,
                         nu=4)
  
  
  if(i!=1){
    startrow<-which(result_glm_ddm$Model==2)
    endrow<-nrow(result_glm_ddm)
    result_glm_ddm<-result_glm_ddm[c(startrow:endrow),]
  }
  
  if(show_modelchange==FALSE){
    startcol<-which(colnames(result_glm_ddm)=="Ref Model")
    endcol<-which(colnames(result_glm_ddm)=="Pred")-1
    result_glm_ddm<-result_glm_ddm[,-c(startcol:endcol)]
    result_glm_ddm$Model[1]<-i
  }
  
  if(i==1){
    all_result_glm_ddm<-result_glm_ddm
  }else{
    all_result_glm_ddm<-rbind(all_result_glm_ddm,result_glm_ddm)
  }
}

rownames(all_result_glm_ddm)<-NULL
all_result_glm_ddm$Pred<-rename_corr_pars(all_result_glm_ddm$Pred)

print(all_result_glm_ddm %>%
  kbl(caption = paste0("Bayesian Regression: Predicting ", outcome_name),valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T))

```
 
### GPTS-Persecution

```{r id9932991112181, echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE, cache=TRUE,fig.show='asis',results='asis'}

show_modelchange<-TRUE
tmp_pred<-c('delta_b1','delta_gend','delta_b1_jovi','age','female_dummy')
tmp_outcome<-c('gpts_persec')
tmp_data<-subset(all_sub_data,all_sub_data$group=="HC")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])
pars<-c("delta_b1","delta_gend",'delta_b1_jovi')
mod_suffix<-c("h","i","j","k")
outcome<-"gpts_persec"
outcome_name<-"GPTS Persecution (HC Only)"
outcome_prefix<-2

for(i in 1:length(pars)){
  
  #define models to test (list predictors in order they should be entered)
  test_models<-list(c("age","female_dummy"),
                    c("age","female_dummy",pars[i]))
  
  result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=paste0(dirname,paste0("/analysis/full/saved_brms_models_",
                                                       outcome_prefix,
                                                       mod_suffix[i],".rds")),
                         credMass=.95,
                         nu=4)
  
  if(i!=1){
    startrow<-which(result_glm_ddm$Model==2)
    endrow<-nrow(result_glm_ddm)
    result_glm_ddm<-result_glm_ddm[c(startrow:endrow),]
  }
  
  if(show_modelchange==FALSE){
    startcol<-which(colnames(result_glm_ddm)=="Ref Model")
    endcol<-which(colnames(result_glm_ddm)=="Pred")-1
    result_glm_ddm<-result_glm_ddm[,-c(startcol:endcol)]
    result_glm_ddm$Model[1]<-i
  }
  
  if(i==1){
    all_result_glm_ddm<-result_glm_ddm
  }else{
    all_result_glm_ddm<-rbind(all_result_glm_ddm,result_glm_ddm)
  }
}

rownames(all_result_glm_ddm)<-NULL
all_result_glm_ddm$Pred<-rename_corr_pars(all_result_glm_ddm$Pred)

print(all_result_glm_ddm %>%
  kbl(caption = paste0("Bayesian Regression: Predicting ", outcome_name),valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T))

```
 
### Social Cognition

```{r id99329911121181, echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE, cache=TRUE,fig.show='asis',results='asis'}
########
show_modelchange<-TRUE
tmp_pred<-c('delta_b1','delta_bias','delta_gend','delta_b1_jovi','age','female_dummy')
tmp_outcome<-c('soc_cog_pc_wrt')
tmp_data<-subset(all_sub_data,all_sub_data$group=="HC")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) 
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])
pars<-c("delta_b1","delta_bias","delta_gend",'delta_b1_jovi')
mod_suffix<-c("h","i","j","k")
outcome<-"soc_cog_pc_wrt"
outcome_name<-"Social Cognition (HC Only)"
outcome_prefix<-3

for(i in 1:length(pars)){
  
  #define models to test (list predictors in order they should be entered)
  test_models<-list(c("age","female_dummy"),
                    c("age","female_dummy",pars[i]))
  
  result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=paste0(dirname,paste0("/analysis/full/saved_brms_models_",
                                                       outcome_prefix,
                                                       mod_suffix[i],".rds")),
                         credMass=.95,
                         nu=4)
  
  if(i!=1){
    startrow<-which(result_glm_ddm$Model==2)
    endrow<-nrow(result_glm_ddm)
    result_glm_ddm<-result_glm_ddm[c(startrow:endrow),]
  }
  
  if(show_modelchange==FALSE){
    startcol<-which(colnames(result_glm_ddm)=="Ref Model")
    endcol<-which(colnames(result_glm_ddm)=="Pred")-1
    result_glm_ddm<-result_glm_ddm[,-c(startcol:endcol)]
    result_glm_ddm$Model[1]<-i
  }
  
  if(i==1){
    all_result_glm_ddm<-result_glm_ddm
  }else{
    all_result_glm_ddm<-rbind(all_result_glm_ddm,result_glm_ddm)
  }
}

rownames(all_result_glm_ddm)<-NULL
all_result_glm_ddm$Pred<-rename_corr_pars(all_result_glm_ddm$Pred)

print(all_result_glm_ddm %>%
  kbl(caption = paste0("Bayesian Regression: Predicting ", outcome_name),valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T))

```

## Sensitivity - Controlling for General Cognition (MCCB) {.tabset}

__Re-run those follow-up within group regressions to see if effects remain after controlling for general cognitive ability (i.e., composite from MCCB).__

### GPTS-Referential Thinking

```{r id993299122231121, echo=FALSE, fig.height=4, fig.show='asis', fig.width=5, message=FALSE, warning=FALSE, cache=TRUE, results='asis'}

show_modelchange<-TRUE
tmp_pred<-c('delta_b1','delta_gend','age','female_dummy','matrics')
tmp_outcome<-c('gpts_socref')
tmp_data<-subset(all_sub_data,all_sub_data$group=="HC")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])
pars<-c("delta_b1","delta_gend")
mod_suffix<-c("h","i")
outcome<-"gpts_socref"
outcome_name<-"GPTS Referential Thinking (HC Only)"
outcome_prefix<-"1.1"

for(i in 1:length(pars)){
  
  #define models to test (list predictors in order they should be entered)
  test_models<-list(c("age","female_dummy","matrics"),
                    c("age","female_dummy","matrics",pars[i]))
  
  result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=paste0(dirname,paste0("/analysis/full/saved_brms_models_",
                                                       outcome_prefix,
                                                       mod_suffix[i],".rds")),
                         credMass=.95,
                         nu=4)
  
  if(i!=1){
    startrow<-which(result_glm_ddm$Model==2)
    endrow<-nrow(result_glm_ddm)
    result_glm_ddm<-result_glm_ddm[c(startrow:endrow),]
  }
  
  if(show_modelchange==FALSE){
    startcol<-which(colnames(result_glm_ddm)=="Ref Model")
    endcol<-which(colnames(result_glm_ddm)=="Pred")-1
    result_glm_ddm<-result_glm_ddm[,-c(startcol:endcol)]
    result_glm_ddm$Model[1]<-i
  }
  
  if(i==1){
    all_result_glm_ddm<-result_glm_ddm
  }else{
    all_result_glm_ddm<-rbind(all_result_glm_ddm,result_glm_ddm)
  }
}

rownames(all_result_glm_ddm)<-NULL
all_result_glm_ddm$Pred<-rename_corr_pars(all_result_glm_ddm$Pred)

print(all_result_glm_ddm %>%
  kbl(caption = paste0("Bayesian Regression: Predicting ", outcome_name),valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T))

```
 
### GPTS-Persecution

```{r id998329911121891, echo=FALSE, fig.height=4, fig.show='asis', fig.width=5, message=FALSE, warning=FALSE, cache=TRUE, results='asis'}

show_modelchange<-TRUE
tmp_pred<-c('delta_b1','delta_gend','age','female_dummy','matrics')
tmp_outcome<-c('gpts_persec')
tmp_data<-subset(all_sub_data,all_sub_data$group=="HC")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])
pars<-c("delta_b1","delta_gend")
mod_suffix<-c("h","i")
outcome<-"gpts_persec"
outcome_name<-"GPTS Persecution (HC Only)"
outcome_prefix<-"2.1"

for(i in 1:length(pars)){
  
  #define models to test (list predictors in order they should be entered)
  test_models<-list(c("age","female_dummy","matrics"),
                    c("age","female_dummy","matrics",pars[i]))
  
  result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=paste0(dirname,paste0("/analysis/full/saved_brms_models_",
                                                       outcome_prefix,
                                                       mod_suffix[i],".rds")),
                         credMass=.95,
                         nu=4)
  
  if(i!=1){
    startrow<-which(result_glm_ddm$Model==2)
    endrow<-nrow(result_glm_ddm)
    result_glm_ddm<-result_glm_ddm[c(startrow:endrow),]
  }
  
  if(show_modelchange==FALSE){
    startcol<-which(colnames(result_glm_ddm)=="Ref Model")
    endcol<-which(colnames(result_glm_ddm)=="Pred")-1
    result_glm_ddm<-result_glm_ddm[,-c(startcol:endcol)]
    result_glm_ddm$Model[1]<-i
  }
  
  if(i==1){
    all_result_glm_ddm<-result_glm_ddm
  }else{
    all_result_glm_ddm<-rbind(all_result_glm_ddm,result_glm_ddm)
  }
}

rownames(all_result_glm_ddm)<-NULL
all_result_glm_ddm$Pred<-rename_corr_pars(all_result_glm_ddm$Pred)

print(all_result_glm_ddm %>%
  kbl(caption = paste0("Bayesian Regression: Predicting ", outcome_name),valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T))

```
 
 
## Sensitivity - Controlling for General Perceptual Processing  (Visual Integration) {.tabset}

__Re-run those follow-up within group regressions to see if effects remain after controlling for general perceptual processing ability (i.e., DDM parameters from Visual Integration task).__

### GPTS-Referential Thinking

```{r id993299131121, echo=FALSE, fig.height=4, fig.show='asis', fig.width=5, message=FALSE, warning=FALSE, cache=TRUE, results='asis'}

show_modelchange<-TRUE
tmp_pred<-c('delta_b1','delta_gend','age','female_dummy','delta_b1_jovi')
tmp_outcome<-c('gpts_socref')
tmp_data<-subset(all_sub_data,all_sub_data$group=="HC")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])
pars<-c("delta_b1","delta_gend")
mod_suffix<-c("h","i")
outcome<-"gpts_socref"
outcome_name<-"GPTS Referential Thinking (HC Only)"
outcome_prefix<-"1.1.1"

for(i in 1:length(pars)){
  
  #define models to test (list predictors in order they should be entered)
  test_models<-list(c("age","female_dummy","delta_b1_jovi"),
                    c("age","female_dummy","delta_b1_jovi",pars[i]))
  
  result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=paste0(dirname,paste0("/analysis/full/saved_brms_models_",
                                                       outcome_prefix,
                                                       mod_suffix[i],".rds")),
                         credMass=.95,
                         nu=4)
  
  if(i!=1){
    startrow<-which(result_glm_ddm$Model==2)
    endrow<-nrow(result_glm_ddm)
    result_glm_ddm<-result_glm_ddm[c(startrow:endrow),]
  }
  
  if(show_modelchange==FALSE){
    startcol<-which(colnames(result_glm_ddm)=="Ref Model")
    endcol<-which(colnames(result_glm_ddm)=="Pred")-1
    result_glm_ddm<-result_glm_ddm[,-c(startcol:endcol)]
    result_glm_ddm$Model[1]<-i
  }
  
  if(i==1){
    all_result_glm_ddm<-result_glm_ddm
  }else{
    all_result_glm_ddm<-rbind(all_result_glm_ddm,result_glm_ddm)
  }
}

rownames(all_result_glm_ddm)<-NULL
all_result_glm_ddm$Pred<-rename_corr_pars(all_result_glm_ddm$Pred)

print(all_result_glm_ddm %>%
  kbl(caption = paste0("Bayesian Regression: Predicting ", outcome_name),valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T))

```
 
### GPTS-Persecution

```{r id9983299111212222891, echo=FALSE, fig.height=4, fig.show='asis', fig.width=5, message=FALSE, warning=FALSE, cache=TRUE, results='asis'}

show_modelchange<-TRUE
tmp_pred<-c('delta_b1','delta_gend','age','female_dummy','delta_b1_jovi')
tmp_outcome<-c('gpts_persec')
tmp_data<-subset(all_sub_data,all_sub_data$group=="HC")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])
pars<-c("delta_b1","delta_gend")
mod_suffix<-c("h","i")
outcome<-"gpts_persec"
outcome_name<-"GPTS Persecution (HC Only)"
outcome_prefix<-"2.1.1"

for(i in 1:length(pars)){
  
  #define models to test (list predictors in order they should be entered)
  test_models<-list(c("age","female_dummy","delta_b1_jovi"),
                    c("age","female_dummy","delta_b1_jovi",pars[i]))
  
  result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=paste0(dirname,paste0("/analysis/full/saved_brms_models_",
                                                       outcome_prefix,
                                                       mod_suffix[i],".rds")),
                         credMass=.95,
                         nu=4)
  
  if(i!=1){
    startrow<-which(result_glm_ddm$Model==2)
    endrow<-nrow(result_glm_ddm)
    result_glm_ddm<-result_glm_ddm[c(startrow:endrow),]
  }
  
  if(show_modelchange==FALSE){
    startcol<-which(colnames(result_glm_ddm)=="Ref Model")
    endcol<-which(colnames(result_glm_ddm)=="Pred")-1
    result_glm_ddm<-result_glm_ddm[,-c(startcol:endcol)]
    result_glm_ddm$Model[1]<-i
  }
  
  if(i==1){
    all_result_glm_ddm<-result_glm_ddm
  }else{
    all_result_glm_ddm<-rbind(all_result_glm_ddm,result_glm_ddm)
  }
}

rownames(all_result_glm_ddm)<-NULL
all_result_glm_ddm$Pred<-rename_corr_pars(all_result_glm_ddm$Pred)

print(all_result_glm_ddm %>%
  kbl(caption = paste0("Bayesian Regression: Predicting ", outcome_name),valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T))

```

{-}


# References

Cohen. (1988). Dedekind completion of positive concatenation structures: Necessary and sufficient conditions. Journal of Mathematical Psychology, 32(1), 64–71. https://doi.org/10.1016/0022-2496(88)90038-7

Freeman, D., Loe, B. S., Kingdon, D., Startup, H., Molodynski, A., Rosebrock, L., Brown, P., Sheaves, B., Waite, F., & Bird, J. C. (2019). The revised Green et al., Paranoid Thoughts Scale (R-GPTS): Psychometric properties, severity ranges, and clinical cut-offs. Psychological Medicine, 51(2), 244–253. https://doi.org/10.1017/S0033291719003155

Green, C. E. L., Freeman, D., Kuipers, E., Bebbington, P., Fowler, D., Dunn, G., & Garety, P. A. (2007). Measuring ideas of persecution and social reference: The Green et al Paranoid Thought Scales (GPTS). Psychological Medicine, 38(1), 101–111. https://doi.org/10.1017/s0033291707001638

Keefe, R. S. E., Fox, K. H., Harvey, P. D., Cucchiaro, J., Siu, C., & Loebel, A. (2011). Characteristics of the MATRICS Consensus Cognitive Battery in a 29-site antipsychotic schizophrenia clinical trial. Schizophrenia Research, 125(2–3), 161–168. https://doi.org/10.1016/j.schres.2010.09.015

Kohler, C. G., Turner, T. H., Gur, R. E., & Gur, R. C. (2004). Recognition of Facial Emotions in Neuropsychiatric Disorders. CNS Spectrums, 9(4), 267–274. https://doi.org/10.1017/S1092852900009202

Lasagna, C., McLaughlin, M., Deng, W., Whiting, E., & Tso, I. (2020). Deconstructing eye contact perception: Measuring perceptual precision and self-referential tendency using an online psychophysical eye contact detection task. PLoS ONE, 15(3), 1–20. https://doi.org/10.1371/journal.pone.0230258

Mayer, Salovey, & Caruso. (2002). Mayer-Salovey-Caruso Emotional Intelligence Test (MSCEIT) User’s Manual. MHS Publishers.

Morey, R., & Rouder, J. (2015). BayesFactor: Computation of Bayes Factors for common  designs. R package version 0.9.12–12.

Silverstein SM, Keane BP, Barch DM, Carter CS, Gold JM, Kovács I, MacDonald A 3rd, Ragland JD, Strauss ME. (2012). Optimization and validation of a visual integration test for schizophrenia research. Schizophrenia Bulletin. doi: 10.1093/schbul/sbr141. 

Stacklies, W., Redestig, H., Scholz, M., Walther, D., & Selbig, J. (2007). pcaMethods—A bioconductor package providing PCA methods for incomplete data. Bioinformatics, 23(9), 1164–1167. https://doi.org/10.1093/bioinformatics/btm069

