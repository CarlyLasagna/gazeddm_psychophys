tmp_sig_grp_alpha_pr<-tmp_posterior[[paste("sig_grp_alpha_pr",start_del,tmp_group,end_del,sep="")]]
tmp_sig_grp_beta_pr<-tmp_posterior[[paste("sig_grp_beta_pr",start_del,tmp_group,end_del,sep="")]]
tmp_sig_grp_alpha_pr<-tmp_posterior[[paste("sig_grp_alpha_pr",start_del,tmp_group,end_del,sep="")]]
tmp_sig_grp_beta_pr<-tmp_posterior[[paste("sig_grp_beta_pr",start_del,tmp_group,end_del,sep="")]]
tmp_sig_grp_delta_pr<-tmp_posterior[[paste("sig_grp_delta_pr",start_del,tmp_group,end_del,sep="")]]
tmp_sig_grp_ndt_pr<-tmp_posterior[[paste("sig_grp_ndt_pr",start_del,tmp_group,end_del,sep="")]]
#draw subj level values
tmp_sub_alpha_pr<-tmp_posterior[[paste("sub_alpha_pr",start_del,tmp_subj,end_del,sep="")]]
tmp_sub_beta_pr<-tmp_posterior[[paste("sub_beta_pr",start_del,tmp_subj,end_del,sep="")]]
tmp_sub_ndt_pr<-tmp_posterior[[paste("sub_ndt_pr",start_del,tmp_subj,end_del,sep="")]]
tmp_sub_delta_pr<-tmp_posterior[[paste("sub_delta_",tmp_gendername,"pr",start_del,tmp_subj,end_del,sep="")]]
# if b1 parameters exist, get them. otherwise set to 0
if(length(grep("b1",colnames(posterior)))>0){
tmp_mu_grp_b1_pr<-tmp_posterior[[paste("mu_grp_b1_pr",start_del,tmp_group,end_del,sep="")]]
tmp_sig_grp_b1_pr<-tmp_posterior[[paste("sig_grp_b1_pr",start_del,tmp_group,end_del,sep="")]]
tmp_sub_b1_pr<-tmp_posterior[[paste("sub_b1_pr",start_del,tmp_subj,end_del,sep="")]]
#do transform for non-centered parameterization
tmp_sub_b1 <- tmp_mu_grp_b1_pr + tmp_sig_grp_b1_pr*tmp_sub_b1_pr
}else{
tmp_sub_b1 <- rep(0,subsample)
}
# if sub delta variance parameters exist, get them. otherwise set to 0
if(length(grep("sig_sub_delta",colnames(posterior)))>0){
tmp_sig_sub_delta_pr<-tmp_posterior[[paste("sig_sub_delta_pr",start_del,tmp_subj,end_del,sep="")]]
}else{
tmp_sig_sub_delta_pr <- rep(0,subsample)
}
# do transformations for non centered parameterization (scale raw subject par by group mean and variance)
tmp_alpha<-pnorm(tmp_mu_grp_alpha_pr + tmp_sub_alpha_pr*tmp_sig_grp_alpha_pr)*3.9+0.1
tmp_beta<-pnorm(tmp_mu_grp_beta_pr + tmp_sub_beta_pr*tmp_sig_grp_beta_pr)
tmp_ndt<-pnorm(tmp_mu_grp_ndt_pr + tmp_sub_ndt_pr*tmp_sig_grp_ndt_pr)*tmp_minRT*0.98
tmp_minRT
tmp_sig_grp_ndt_pr
tmp_sub_ndt_pr
tmp_mu_grp_ndt_pr
#randomly draw group mean, variance values from appropriate posterior
tmp_mu_grp_alpha_pr<-tmp_posterior[[paste("mu_grp_alpha_pr",start_del,tmp_group,end_del,sep="")]]
tmp_mu_grp_beta_pr<-tmp_posterior[[paste("mu_grp_beta_pr",start_del,tmp_group,end_del,sep="")]]
tmp_mu_grp_ndt_pr<-tmp_posterior[[paste("mu_grp_ndt_pr",start_del,tmp_group,end_del,sep="")]]
#group<-group_id<-1
initdir<-"/N/slate/clasagn/gazeddm_psychophys/output/hddm_m3_psychophys/gender/final_fit/"
loglikdir<-"/N/slate/clasagn/gazeddm_psychophys/output/hddm_m3_psychophys/gender/log_lik/"
modelname<-'hddm_m3_psychophys'
task<-'gender'
files <- list.files(paste0(initdir),
pattern = "^hddm_m3", full.names = TRUE)
library(data.table)
library(dplyr)
library(stringr)
cores <- 8           # number of cores to use (default=1)
setDTthreads(threads=cores)
chains<-10
warmup <- 1500        # number of warmup samples (2500)
iter <- 5000          # number of samples per chains (total postwarmup draws = (iter-warmup)*chains) (2000)
options(mc.cores = cores)
for(i in 1:length(files)){
print(i)
# Read CSV while skipping the first 50 rows
tmp_data <- fread(files[i], skip = 52,nrows = iter,nThread = cores)
if(i==1){
alldata<-tmp_data
}else{
alldata<-rbind(alldata,tmp_data)
}
}
#get col names
cols <- read.csv(files[i], fill=TRUE)
startrow<-which(cols$X..stan_version_major...2=="lp__")
endrow<-which(cols$X..stan_version_major...2=="# Adaptation terminated")-1
endrow
cols$X..stan_version_major...2[endrow]
cols$X..stan_version_major...2[endrow]-2
cols$X..stan_version_major...2[endrow-2]
startrow<-which(cols$X..stan_version_major...2=="lp__")
endrow<-which(cols$X..stan_version_major...2=="# Adaptation terminated")-1
col_names<-as.character(cols$X..stan_version_major...2[startrow:endrow])
alldata<-data.frame(alldata)
colnames(alldata)<-col_names
# Function to modify column names
modify_colnames <- function(df) {
colnames(df) <- colnames(df) %>%
ifelse(!str_detect(., "\\.$") & !str_detect(., "__$"), paste0(., "."), .)
return(df)
}
alldata <- modify_colnames(alldata)
all_mcmc <- alldata
all_mcmc$'.chain'<-rep(1:10, each = 5000)
## save samples for this group
#this_subset<-alldata[,-grep("log_lik", colnames(alldata))]
sample_file<-paste0(initdir,'allgroups_final_fit_samples.csv')
initdir
## save samples for this group
#this_subset<-alldata[,-grep("log_lik", colnames(alldata))]
sample_file<-paste0(initdir,'allgroups_final_fit_samples.csv')
## save samples for this group
#this_subset<-alldata[,-grep("log_lik", colnames(alldata))]
sample_file<-paste0(initdir,'allgroups_final_fit_samples.csv')
fwrite(all_mcmc,sample_file,row.names = F)
#group<-group_id<-1
initdir<-"/N/slate/clasagn/gazeddm_psychophys/output/hddm_m2_psychophys/gaze/final_fit/"
loglikdir<-"/N/slate/clasagn/gazeddm_psychophys/output/hddm_m2_psychophys/gaze/log_lik/"
modelname<-'hddm_m2_psychophys'
task<-'gaze'
files <- list.files(paste0(initdir),
pattern = "^hddm_m2", full.names = TRUE)
library(data.table)
library(dplyr)
library(stringr)
cores <- 8           # number of cores to use (default=1)
setDTthreads(threads=cores)
chains<-10
warmup <- 1500        # number of warmup samples (2500)
iter <- 5000          # number of samples per chains (total postwarmup draws = (iter-warmup)*chains) (2000)
options(mc.cores = cores)
for(i in 1:length(files)){
print(i)
# Read CSV while skipping the first 50 rows
tmp_data <- fread(files[i], skip = 52,nrows = iter,nThread = cores)
if(i==1){
alldata<-tmp_data
}else{
alldata<-rbind(alldata,tmp_data)
}
}
#get col names
cols <- read.csv(files[i], fill=TRUE)
startrow<-which(cols$X..stan_version_major...2=="lp__")
endrow<-which(cols$X..stan_version_major...2=="# Adaptation terminated")-1
col_names<-as.character(cols$X..stan_version_major...2[startrow:endrow])
alldata<-data.frame(alldata)
colnames(alldata)<-col_names
# Function to modify column names
modify_colnames <- function(df) {
colnames(df) <- colnames(df) %>%
ifelse(!str_detect(., "\\.$") & !str_detect(., "__$"), paste0(., "."), .)
return(df)
}
alldata <- modify_colnames(alldata)
all_mcmc <- alldata
all_mcmc$'.chain'<-rep(1:10, each = 5000)
#group<-group_id<-1
initdir<-"/N/slate/clasagn/gazeddm_psychophys/output/hddm_m2_psychophys/gaze/final_fit/"
loglikdir<-"/N/slate/clasagn/gazeddm_psychophys/output/hddm_m2_psychophys/gaze/log_lik/"
modelname<-'hddm_m2_psychophys'
task<-'gaze'
files <- list.files(paste0(initdir),
pattern = "^hddm_m2", full.names = TRUE)
library(data.table)
library(dplyr)
library(stringr)
cores <- 8           # number of cores to use (default=1)
setDTthreads(threads=cores)
chains<-10
warmup <- 1500        # number of warmup samples (2500)
iter <- 5000          # number of samples per chains (total postwarmup draws = (iter-warmup)*chains) (2000)
options(mc.cores = cores)
for(i in 1:length(files)){
print(i)
# Read CSV while skipping the first 50 rows
tmp_data <- fread(files[i], skip = 52,nrows = iter,nThread = cores)
if(i==1){
alldata<-tmp_data
}else{
alldata<-rbind(alldata,tmp_data)
}
}
#get col names
cols <- read.csv(files[i], fill=TRUE)
startrow<-which(cols$X..stan_version_major...2=="lp__")
endrow<-which(cols$X..stan_version_major...2=="# Adaptation terminated")-1
cols$X..stan_version_major...2[endrow]
endrow<-which(cols$X..stan_version_major...2=="mu_grp_ndt.pr.1.")
endrow<-which(cols$X..stan_version_major...2=="mu_grp_ndt.pr.1")
run<-1
task<-'gaze'
modelname<-'hddm_m2_psychophys'
seed<-42+run # different runs get different rand seeds
set.seed(seed)
library(readxl)
library(dplyr)
library(ggplot2)
library(cmdstanr)
library(posterior)
library(bayesplot)
library(data.table)
library(loo)
#dirname<-'/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm'
dirname<-'/N/slate/clasagn/gazeddm_psychophys' ## IU cluster
datadir<-paste0(dirname,'/data')
scriptsdir<-paste0(dirname,'/scripts')
behfile<-paste0(datadir,"/schizgaze12_gaze_beh.csv")
outdir<-paste0(dirname,'/output')
modeldir<-paste0(dirname,'/output/',modelname,'/',task)
initdir<-paste0(modeldir,'/init_fit')
loglikdir<-paste0(modeldir,'/log_lik')
ppcdir<-paste0(modeldir,'/ppc')
parrecoverdir<-paste0(modeldir,'/par_recover')
finaldir<-paste0(modeldir,'/final_fit')
warmup <- 1500        # number of warmup samples (1500)
cores <- 10           # number of cores to use (default=1) (10)
iter <- 5000          # number of samples per chain (total postwarmup draws = (iter-warmup)*chains) (1000)
chains <- 10          # number of chains  (10)
adapt_delta <- 0.95   # default=.95 (target mean proposal acceptance probability during adaptation period)
max_treedepth <- 10   # default=10 (when max_treedepth reached, sampler terminates prematurely)
stepsize <- 1         # default=1 (discretization interval)
options(mc.cores = cores)
setDTthreads(threads=cores)
# load preprocessed behavioral data and subset to schizgaze2 only
alldata<-read.csv(behfile)
alldata<-subset(alldata,alldata$Study=="schizgaze2")
# sort subj id's in order (hc's [1000s] will come first, then sz [2000s])
# (do this so that when we assign sequential index id's, they will be the same for gender and gaze tasks)
alldata <- alldata[order(alldata$Subj), ]
#subset to get the appropriate task data
if(task=='gaze'){
alldata<-subset(alldata,alldata$Task=="Eyes")
alldata$SubjID<-match(alldata$Subj, unique(alldata$Subj)) #subj IDs to sequential indexes
alldata$Resp<-ifelse(alldata$Resp=="Y",1,2) #recode "yes" as 1 and "no as 2
alldata$Gender<-ifelse(alldata$Gender=="F",1,2) #recode female gender of stim as 1 and male as 2
alldata$Group<-match(alldata$Group, unique(alldata$Group))
alldata <- alldata[order(alldata$SubjID, alldata$Resp), ]
}else if(task=='gender'){
alldata<-subset(alldata,alldata$Task=="GenderID")
alldata$SubjID<-match(alldata$Subj, unique(alldata$Subj)) #subj IDs to sequential indexes
alldata$Resp<-ifelse(alldata$Resp=="F",1,2) #recode female as 1 and male as 2
alldata$Gender<-ifelse(alldata$Gender=="F",1,2) #recode female gender of stim as 1 and male as 2
alldata$Group<-match(alldata$Group, unique(alldata$Group))
alldata <- alldata[order(alldata$SubjID, alldata$Resp), ]
}
minRT<-alldata %>%
dplyr::group_by(SubjID) %>%
dplyr::summarise(minRT = min(RT)) %>%
dplyr::ungroup()
# prep data for stan
data_stan<-list(
N_obs=nrow(alldata), # number of observations [integer]
N_subj=length(unique(alldata$Subj)),# Number of subjects [integer]
N_groups=length(unique(alldata$Group)),# Number of groups [integer]
N_levels=length(unique(alldata$GazeAngle)), #number of stimulus strength levels
N_choice=length(unique(alldata$Resp)), #number of choices [integer]
RT=alldata$RT/1000, # RT in seconds for each observation [vector of doubles of  length 'N_obs']
choice=alldata$Resp, # choice for each observation [integer vector of length 'N_obs']
gender=alldata$Gender, # gender for each observation [integer vector of length 'N_obs'] 1=female, 2=male
subj=match(alldata$SubjID, unique(alldata$SubjID)), # subject id for each observation [integer vector length 'N_obs']
group=alldata$Group, # group id for each observation
level=(alldata$GazeAngle-mean(alldata$GazeAngle))/sd(alldata$GazeAngle), # zscored signal strength for each observation (will transform back to original units post-fitting)
minRT=minRT$minRT/1000, #min rt in seconds for each subject
rtBound=0.0001) #lower bound on RT in seconds
stanmodelname=paste(scriptsdir,"/",modelname,".stan",sep="")
# run the model
print("Running model in Stan")
model <- cmdstan_model(stanmodelname)
#dirname<-'/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm'
dirname<-'/N/slate/clasagn/gazeddm_psychophys' ## IU cluster
datadir<-paste0(dirname,'/data')
scriptsdir<-paste0(dirname,'/scripts')
behfile<-paste0(datadir,"/schizgaze12_gaze_beh.csv")
outdir<-paste0(dirname,'/output')
modeldir<-paste0(dirname,'/output/',modelname,'/',task)
initdir<-paste0(modeldir,'/init_fit')
loglikdir<-paste0(modeldir,'/log_lik')
ppcdir<-paste0(modeldir,'/ppc')
parrecoverdir<-paste0(modeldir,'/par_recover')
finaldir<-paste0(modeldir,'/final_fit')
warmup <- 150        # number of warmup samples (1500)
cores <- 1           # number of cores to use (default=1) (10)
iter <- 500          # number of samples per chain (total postwarmup draws = (iter-warmup)*chains) (1000)
chains <- 1          # number of chains  (10)
adapt_delta <- 0.95   # default=.95 (target mean proposal acceptance probability during adaptation period)
max_treedepth <- 10   # default=10 (when max_treedepth reached, sampler terminates prematurely)
stepsize <- 1         # default=1 (discretization interval)
options(mc.cores = cores)
setDTthreads(threads=cores)
# load preprocessed behavioral data and subset to schizgaze2 only
alldata<-read.csv(behfile)
alldata<-subset(alldata,alldata$Study=="schizgaze2")
# sort subj id's in order (hc's [1000s] will come first, then sz [2000s])
# (do this so that when we assign sequential index id's, they will be the same for gender and gaze tasks)
alldata <- alldata[order(alldata$Subj), ]
#subset to get the appropriate task data
if(task=='gaze'){
alldata<-subset(alldata,alldata$Task=="Eyes")
alldata$SubjID<-match(alldata$Subj, unique(alldata$Subj)) #subj IDs to sequential indexes
alldata<-subset(alldata,alldata$SubjID<4)
alldata$Resp<-ifelse(alldata$Resp=="Y",1,2) #recode "yes" as 1 and "no as 2
alldata$Gender<-ifelse(alldata$Gender=="F",1,2) #recode female gender of stim as 1 and male as 2
alldata$Group<-match(alldata$Group, unique(alldata$Group))
alldata <- alldata[order(alldata$SubjID, alldata$Resp), ]
}else if(task=='gender'){
alldata<-subset(alldata,alldata$Task=="GenderID")
alldata$SubjID<-match(alldata$Subj, unique(alldata$Subj)) #subj IDs to sequential indexes
alldata$Resp<-ifelse(alldata$Resp=="F",1,2) #recode female as 1 and male as 2
alldata$Gender<-ifelse(alldata$Gender=="F",1,2) #recode female gender of stim as 1 and male as 2
alldata$Group<-match(alldata$Group, unique(alldata$Group))
alldata <- alldata[order(alldata$SubjID, alldata$Resp), ]
}
minRT<-alldata %>%
dplyr::group_by(SubjID) %>%
dplyr::summarise(minRT = min(RT)) %>%
dplyr::ungroup()
# prep data for stan
data_stan<-list(
N_obs=nrow(alldata), # number of observations [integer]
N_subj=length(unique(alldata$Subj)),# Number of subjects [integer]
N_groups=length(unique(alldata$Group)),# Number of groups [integer]
N_levels=length(unique(alldata$GazeAngle)), #number of stimulus strength levels
N_choice=length(unique(alldata$Resp)), #number of choices [integer]
RT=alldata$RT/1000, # RT in seconds for each observation [vector of doubles of  length 'N_obs']
choice=alldata$Resp, # choice for each observation [integer vector of length 'N_obs']
gender=alldata$Gender, # gender for each observation [integer vector of length 'N_obs'] 1=female, 2=male
subj=match(alldata$SubjID, unique(alldata$SubjID)), # subject id for each observation [integer vector length 'N_obs']
group=alldata$Group, # group id for each observation
level=(alldata$GazeAngle-mean(alldata$GazeAngle))/sd(alldata$GazeAngle), # zscored signal strength for each observation (will transform back to original units post-fitting)
minRT=minRT$minRT/1000, #min rt in seconds for each subject
rtBound=0.0001) #lower bound on RT in seconds
stanmodelname=paste(scriptsdir,"/",modelname,".stan",sep="")
# run the model
print("Running model in Stan")
model <- cmdstan_model(stanmodelname)
fit <- model$sample(data=data_stan,
iter_warmup=warmup,
iter_sampling=iter,
chains=chains,
parallel_chains=cores, #num cores
save_warmup = FALSE,
adapt_delta=adapt_delta,
max_treedepth=max_treedepth,
step_size=stepsize,
init=0,
refresh=100,
seed=seed,
output_dir=finaldir,
diagnostics = c("divergences", "treedepth", "ebfmi"))
posterior_df <- as_draws_df(fit$draws())
stanmodelname=paste(scriptsdir,"/",modelname,".stan",sep="")
stanmodelname
# run the model
print("Running model in Stan")
model <- cmdstan_model(stanmodelname)
fit <- model$sample(data=data_stan,
iter_warmup=warmup,
iter_sampling=iter,
chains=chains,
parallel_chains=cores, #num cores
save_warmup = FALSE,
adapt_delta=adapt_delta,
max_treedepth=max_treedepth,
step_size=stepsize,
init=0,
refresh=100,
seed=seed,
output_dir=finaldir,
diagnostics = c("divergences", "treedepth", "ebfmi"))
# Define input models, fit models, and runs
input_models <- c('2')
runs <- c(1)
type<-c('ppc')#par_recovery or #init_fit, ppc, par_recover, or final_fit. init fit includes loglik extract and loo calc
task<-c('gaze')
groups<-c('all')
run<-c(1) #1=run batch scripts after generating; 0=1 dont run batch scripts
fitpath<-'/N/slate/clasagn/gazeddm_psychophys'
# Iterate over combinations to generate and run batch scripts for initial model fit
for (i in input_models) {
for (j in runs) {
for (k in groups){
for (l in type){
for(m in task){
setwd(current_dir<-paste0(fitpath,'/output/hddm_m',i,'_psychophys/',m,'/',l,'/'))
script_name <- paste0("batch",i,"_",l, "_g",k,"_r", j,".sh")
script_content <- paste0("#!/bin/bash",
"\n#SBATCH -J PPm", i, "r", j, "_",m,l,
"\n#SBATCH -A r00914",
"\n#SBATCH -o jobname_%j.txt",
"\n#SBATCH -e jobname_%j.err",
"\n#SBATCH --nodes=1",
"\n#SBATCH --ntasks=10",
"\n#SBATCH --cpus-per-task=1",
"\n#SBATCH --time=0-6:00:00",
"\n#SBATCH --mail-user=clasagna@umich.edu",
"\n#SBATCH --mail-type=BEGIN,FAIL,END",
"\n#SBATCH --mem=100G",
"\n#SBATCH --partition=general",
"\n#note: mem is total memory for entire job (quartz has limit of ~500 GB per job). mem-per-cpu is memory for each core",
"\n\n#Set up environment",
"\npwd; hostname; date",
"\necho \"Running ",l," ",m," m", i," group ",k," ",l," for m", i, '_run',j, " on IU Quartz HPC\"",
"\n\nmodule load r/4.3",
"\nexport LD_LIBRARY_PATH=\"/geode2/soft/hps/rhel8/gcc/12.2.0/lib64/:$LD_LIBRARY_PATH\"",
"\n\n#Define variables",
"\nrun=", j,
"\ngroup=",k,
"\nmodelname='hddm_m", i, "_psychophys'",
"\ntask=",m,
"\n\n#Commands to run",
"\nRscript /N/slate/clasagn/gazeddm_psychophys/output/run2_ppc.R $run $group $modelname $task")
# Write to file
writeLines(script_content, script_name)
if(run==1){
# sbatch run job script
system(paste0("sbatch ",current_dir,script_name),wait=FALSE)
}
}
}
}
}
}
# Define input models, fit models, and runs
input_models <- c(2)
runs <- c(1)
type<-c('final_fit')#par_recovery or #init_fit, ppc, par_recover, or final_fit. init fit includes loglik extract and loo calc
task<-c('gaze')
groups<-c('all')
run<-c(1) #1=run batch scripts after generating; 0=1 dont run batch scripts
fitpath<-'/N/slate/clasagn/gazeddm_psychophys'
# Iterate over combinations to generate and run batch scripts for initial model fit
for (i in input_models) {
for (j in runs) {
for (k in groups){
for (l in type){
for(m in task){
setwd(current_dir<-paste0(fitpath,'/output/hddm_m',i,'_psychophys/',m,'/',l,'/'))
script_name <- paste0("batch",i,"_",l, "_g",k,"_r", j,".sh")
script_content <- paste0("#!/bin/bash",
"\n#SBATCH -J FFm", i, "r", j, "_",m,l,
"\n#SBATCH -A r00914",
"\n#SBATCH -o jobname_%j.txt",
"\n#SBATCH -e jobname_%j.err",
"\n#SBATCH --nodes=1",
"\n#SBATCH --ntasks=10",
"\n#SBATCH --cpus-per-task=1",
"\n#SBATCH --time=0-7:00:00",
"\n#SBATCH --mail-user=clasagna@umich.edu",
"\n#SBATCH --mail-type=BEGIN,FAIL,END",
"\n#SBATCH --mem=75G",
"\n#SBATCH --partition=general",
"\n#note: mem is total memory for entire job (quartz has limit of ~500 GB per job). mem-per-cpu is memory for each core",
"\n\n#Set up environment",
"\npwd; hostname; date",
"\necho \"Running init fit m", i," group ",k," ",l," for m", i, '_run',j, " on IU Quartz HPC\"",
"\n\nmodule load r/4.3",
"\nexport LD_LIBRARY_PATH=\"/geode2/soft/hps/rhel8/gcc/12.2.0/lib64/:$LD_LIBRARY_PATH\"",
"\n\n#Define variables",
"\nrun=", j,
"\ngroup=",k,
"\nmodelname='hddm_m", i, "_psychophys'",
"\ntask=",m,
"\n\n#Commands to run",
"\nRscript /N/slate/clasagn/gazeddm_psychophys/output/run5_final_fit.R $run $group $modelname $task")
# Write to file
writeLines(script_content, script_name)
if(run==1){
# sbatch run job script
system(paste0("sbatch ",current_dir,script_name),wait=FALSE)
}
}
}
}
}
}
# Define input models, fit models, and runs
input_models <- c('2')
runs <- c(1)
type<-c('ppc')#par_recovery or #init_fit, ppc, par_recover, or final_fit. init fit includes loglik extract and loo calc
task<-c('gaze')
groups<-c('all')
run<-c(1) #1=run batch scripts after generating; 0=1 dont run batch scripts
fitpath<-'/N/slate/clasagn/gazeddm_psychophys'
# Iterate over combinations to generate and run batch scripts for initial model fit
for (i in input_models) {
for (j in runs) {
for (k in groups){
for (l in type){
for(m in task){
setwd(current_dir<-paste0(fitpath,'/output/hddm_m',i,'_psychophys/',m,'/',l,'/'))
script_name <- paste0("batch",i,"_",l, "_g",k,"_r", j,".sh")
script_content <- paste0("#!/bin/bash",
"\n#SBATCH -J PPm", i, "r", j, "_",m,l,
"\n#SBATCH -A r00914",
"\n#SBATCH -o jobname_%j.txt",
"\n#SBATCH -e jobname_%j.err",
"\n#SBATCH --nodes=1",
"\n#SBATCH --ntasks=10",
"\n#SBATCH --cpus-per-task=1",
"\n#SBATCH --time=0-2:00:00",
"\n#SBATCH --mail-user=clasagna@umich.edu",
"\n#SBATCH --mail-type=BEGIN,FAIL,END",
"\n#SBATCH --mem=50G",
"\n#SBATCH --partition=general",
"\n#note: mem is total memory for entire job (quartz has limit of ~500 GB per job). mem-per-cpu is memory for each core",
"\n\n#Set up environment",
"\npwd; hostname; date",
"\necho \"Running ",l," ",m," m", i," group ",k," ",l," for m", i, '_run',j, " on IU Quartz HPC\"",
"\n\nmodule load r/4.3",
"\nexport LD_LIBRARY_PATH=\"/geode2/soft/hps/rhel8/gcc/12.2.0/lib64/:$LD_LIBRARY_PATH\"",
"\n\n#Define variables",
"\nrun=", j,
"\ngroup=",k,
"\nmodelname='hddm_m", i, "_psychophys'",
"\ntask=",m,
"\n\n#Commands to run",
"\nRscript /N/slate/clasagn/gazeddm_psychophys/output/run2_ppc.R $run $group $modelname $task")
# Write to file
writeLines(script_content, script_name)
if(run==1){
# sbatch run job script
system(paste0("sbatch ",current_dir,script_name),wait=FALSE)
}
}
}
}
}
}
