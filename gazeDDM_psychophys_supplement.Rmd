---
title: 'Supplement for Lasagna et al. (2025)'
subtitle: 'Drift Diffusion Modeling of Self-Referential Processing in Schizophrenia'
author: "Carly A. Lasagna"
output:
  html_document:
    toc: yes
    toc_float: true
    toc_collapsed: true
    number_sections: yes
    fig_width: 4
    fig_height: 3
    df_print: paged
    toc_depth: 4
geometry: margin=2cm
classoption: 6pt
header-includes:
  - \usepackage{amsmath}
---

\newpage

__Note: Throughout the supplement, Model 2 (Gaze) is the primary gaze model described in the main text and Model 3 (Gender) is the primary gender model described in the main text.__

# Citation

Code and outputs in this document are supplemental material for:

Lasagna, C. A., Pleskac, T. J., Weigard, A., & Tso, I. F. (2025). Modeling Self-Reference in Schizophrenia: The Role of Evidence Accumulation in Social Perception and Paranoia. Preprint:  [(https://doi.org/10.31234/osf.io/9b23k_v1)](https://doi.org/10.31234/osf.io/9b23k_v1)

# Code Availability

**Code used to perform all modeling, simulation, and analysis is available on OSF and GitHub:**

-   OSF [(https://osf.io/e3sxu/?view_only=eaf31c32ae7f48a2ab21305ee2bcfe2a)](https://osf.io/e3sxu/?view_only=eaf31c32ae7f48a2ab21305ee2bcfe2a)
-   GitHub [(https://github.com/CarlyLasagna/gazeddm_psychophys)](https://github.com/CarlyLasagna/gazeddm_psychophys).

# [METHODS SUPPLEMENT] {.unnumbered}

# Study Sample

Participants were recruited from local clinics and community advertisements. All met the following inclusion criteria: aged 18 to 60, no history of conditions with neurological sequelae, visual acuity ≥ 20/30, and no recent substance use disorder (SZ had no dependence in the past year or abuse in the last month; HC had no abuse in the last five years). SZ met DSM-IV-TR criteria for schizophrenia, schizoaffective, or schizophreniform disorder based on the Structured Clinical Interview for DSM-IV-TR (First et al., 2002) (SCID-IV). HC had no history of DSM-IV-TR Axis I disorders and no first-degree relatives with bipolar or psychotic disorders. Two additional SZ were excluded for failing to complete or providing invalid responses on the primary behavioral task (i.e., responded “yes” to every trial).

# Measures

Below are more detailed descriptions as well as reliability and validity information.

## Behavioral Task

Performance in the self-referential gaze perception task shows good convergent validity with social cognitive measures and moderate-to-acceptable test-retest reliability (intra-class-correlation [ICC]: 0.68-0.92) (Lasagna et al., 2020).

## Paranoia (GPTS)

The Green Paranoid Thought Scales (GPTS) (Green et al., 2007) is a self-report measure of paranoia comprising two 16-item subscales: social reference (i.e., experiencing environmental social signals as self-related or self-directed) and persecution (i.e., believing that others intend to cause the individual harm). The original subscales showed good test-retest reliability (ICCs 0.81-0.88) and internal consistency (α > 0.90), but the original two-factor model showed inadequate fit. Thus, subscales were summed using the revised factor structure with enhanced psychometric properties (Freeman et al., 2019).

## General & Social Cognition (RME, ER40, MSCEIT, MCCB)

Social cognition was assessed using the Reading the Mind in the Eyes test (RME) (Baron-Cohen et al., 2001), the Penn Emotion Recognition task (ER40) (Kohler et al., 2004), and the Mayer-Caruso Emotional Intelligence Test (MSCEIT) (Mayer et al., 2002). RME tests the ability to infer mental states of actors from the eye region across 36 images. ER40 tests the ability to process the facial emotions of actors in 40 images. MSCEIT tests the ability to perceive, understand, use, and manage emotions across 141 test items. All show acceptable test-retest reliability (M=0.75-0.86), and questionable-to-good internal consistency (α=0.65-0.91) (Mayer et al., 2002; Pinkham, Penn, et al., 2016). General cognition was assessed using the MATRICS Consensus Cognitive Battery (MCCB), ten tests assessing nine domains of neurocognitive dysfunction in SZ (August et al., 2012). Composite scores show good construct validity, good test-retest reliability (ICC=0.88) (Keefe et al., 2011), and acceptable internal consistency (α=0.76) (Burton et al., 2013). RME and ER40 data of two SZ were removed for performing at chance levels. For dimensionality reduction, we extracted a social cognition principal component from ER40-accuracy, ER40-RT, RME-accuracy, and age- and gender-corrected MSCEIT total, using pcaMethods with Bayesian imputation for missing data (Stacklies et al., 2007). ER40 RT was included to capture similar information as captured by DDM parameters, which incorporate choice and RT information concurrently. This was also done to extract a general cognition principal component from age- and gender-corrected T-scores of the MCCB subscales (excluding the social cognition subscale to avoid overlapping constructs with other measures).

## Other Data Collected 

Additional measures were collected in the present study that were not analyzed because they fell outside of the scope of the current study. Those included:

__Additional behavioral tasks__

* Jitter Orientation Visual Integration Task (JOVI; Silverstein, 2011)

__Clinician-rated symptom measures__

* Brief Psychiatric Rating Scale (BPRS; Overall, 1962).
* Scale for the Assessment of Negative Symptoms (SANS; Andreasen, 1989)
* Scale for the assessment of positive symptoms (SAPS; Andreasen, 1984)

__Intelligence__

* Wide range achievement test (WRAT-Reading; Wilkinson & Robertson, 1993)

__Functional assessments__

* Global Functioning Social/Role Scales (Cornblatt et al., 2007)
* UCSD Performance-based Skills Assessment (UPSA; Patterson et al., 2001a)
* Social Skills Performance Assessment (SSPA; Patterson et al. 2001b)

__Self-reports__

* Questionnaire of Cognitive and Affective Empathy (QCAE; Reniers et al. 2011)
* State-Trait Anxiety Inventory (STAI; Spielberger et al. 1983)
* Psychological Stress Index (PSI; Tso et al. 2012)
* Success and Resource Appraisals Questionnaire (SARA-Q; Couture et al. 2007)
* Beck Depression Inventory (BDI; Beck et al. 1961)

__Neuroimaging__

* Functional magnetic resonance imaging (fMRI): task-based fMRI during the gaze perception and JOVI tasks; and resting-state fMRI.
* Structural magnetic resonance imaging (MRI)
* Diffusion Tensor Imaging (DTI) 

# [MODELING SUPPLEMENT] {.unnumbered}

# Behavioral Data Preprocess

When preprocessing the data, task responses were recoded as follows: (During the gaze condition) "Yes" responses were recoded as 1 (upper boundary responses), "No responses were recoded as 2 (lower boundary responses); during the gender condition, "female" responses were recoded as 1 (upper boundary responses) and "male" responses were recoded as 2 (lower boundary responses). Note: the orientation of choice responses for the gender condition were later flipped for analyses (so Male=upper boundary, Female=lower boundary) to match the buttons that the choice options corresponded to during the task. Thus, for the final analysis, choice options across both tasks that corresponded to the index finger (Yes/Male) were upper boundary and choice options across both tasks that corresponded to the middle finger (No/Female) were lower boundary. 

For the psychophysical aspects of the behavioral task (i.e., gaze signal strength) during the gaze condition, signal strengths were first recoded into 9 integers from 1 to 9 where 1 was the weakest signal strength (predominantly "no" responses) and 9 was the strongest signal strength (predominantly yes looking at me responses). Prior to fitting the models in Stan, these signal strengths were z-scored over the entire sample. 

```{r id1, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE, out.width='1500px'}

########################################################
# SETUP ENVIRONMENT
########################################################
#
library(readxl)
library(dplyr)
library(ggplot2)
library(cmdstanr)
library(posterior)
library(bayesplot)
library(data.table)
library(loo)
library(kableExtra)
library(rstan)
library(HDInterval)
library(GGally)
library(gridExtra)
library(pcaMethods)
library(psych)
library(DescTools)

colors<-c('#287c8e','#482475','#addc30')

bayesplot_theme_set(theme_default(base_size = 8, base_family = "sans"))

#parent directory
dirname<-'/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm'
datadir<-paste(dirname,'/data/',sep="")

########################################################
# CUSTOM FUNCTIONS
########################################################
rename_corr_pars<-function(pars){
  pars <- gsub("alpha_gend", "Threshold Sep (Gender)", pars)
  pars <- gsub("beta_gend", "Start Point (Gender)", pars)
  pars <- gsub("delta_bias_gend", "Drift Bias (Gender)", pars)
  pars <- gsub("delta_female_gend", "Drift Rate-F (Gender)", pars)
  pars <- gsub("delta_male_gend", "Drift Rate-M (Gender)", pars)
  pars <- gsub("delta_gend", "Drift Sensitivity (Gender)", pars)
  pars <- gsub("delta_bias", "Drift Bias (Gaze)", pars)
  pars <- gsub("delta_b1", "Drift Sensitivity (Gaze)", pars)
  pars <- gsub("alpha", "Threshold Sep (Gaze)", pars)
  pars <- gsub("beta", "Start Point (Gaze)", pars)
  pars <- gsub("age", "Age", pars)
  pars <- gsub("female_dummy", "Female", pars)
  pars <- gsub("sz_dummy", "SZ", pars)
  pars <- gsub("matrics", "General Cognition (MCCB)", pars)
  
  return(pars)
}
rename_gaze_pars<-function(pars){
  pars <- gsub("alpha", "threshold_sep", pars)
  pars <- gsub("beta", "start_point", pars)
  pars <- gsub("delta", "drift_bias", pars)
  pars <- gsub("_b1", "_drift_sensitiv", pars)
  pars <- gsub("grp_", "", pars)
  pars <- gsub("mu_", "group_", pars)
  pars <- gsub("_pr.", "_untransformed.", pars)
  
  pars <- ifelse(
  grepl("^sub_", pars), # Check if the string starts with 'sub_'
  pars, # If true, keep the string as is
  gsub("\\.1\\.", "_hc", gsub("\\.2\\.", "_sz", pars)) # Otherwise, replace .1. and .2.
  )
  return(pars)
}
rename_gender_pars<-function(pars){
  pars <- gsub("alpha", "threshold_sep", pars)
  pars <- gsub("beta", "start_point", pars)
  
  pars <- gsub("delta", "drift_rate", pars)
  pars <- gsub("_present", "_female", pars)
  pars <- gsub("_absent", "_male", pars)
  pars <- gsub("grp_", "", pars)
  pars <- gsub("mu_", "group_", pars)
  pars <- gsub("_pr.", "_untransformed.", pars)
  
  pars <- ifelse(
  grepl("^sub_", pars), # Check if the string starts with 'sub_'
  pars, # If true, keep the string as is
  gsub("\\.1\\.", "_hc", gsub("\\.2\\.", "_sz", pars)) # Otherwise, replace .1. and .2.
  )
  return(pars)
}
zscore_cols <- function(data) {
  
  # Loop through each column in the data frame
  for (col in names(data)) {
    # Calculate mean and standard deviation of the column
    col_mean <- mean(data[[col]])
    col_sd <- sd(data[[col]])
    
    # Calculate z-score for each value in the column
    z_scores <- (data[[col]] - col_mean) / col_sd
    
    # Replace the column with z-scores
    data[[col]] <- z_scores
  }
  return(data)
} 
brms_robust_glm_compare<-function(data,test_models,outcome,outfile,credMass,nu){
  
  library(brms)
  library(loo)
  library(stats)
  
  fits<-list()
  
  if(file.exists(outfile)){
    fits<-readRDS(outfile)
  }else{
    for(i in 1:length(test_models)){
      
      formula<-paste(outcome, paste(test_models[[i]],collapse=" + "),sep=" ~ ")
      brms_formula<-bf(formula,nu=nu)
      predictors<-test_models[[i]] #get b predictors (excluding intercept/sigma)
      
      #function dynamically sets priors on standardized b predictors (input to function), as well as intercept and sigma terms, for brms models
      
      dynamic_prior<-function(predictors){ 
        #set prior for intercept and sigma
        ## note: intercept = mean centered b_Intercept = original scale. 
        prior_combine<-brms::prior_string(paste0("normal(0,1)"), class = "Intercept")+
          brms::prior_string(paste0("student_t(3, 0, 2.5)"), class = "sigma") #default prior in brms
        
        #set priors on b coefs
        for (k in 1:length(predictors)){
          tmp_prior<-brms::prior_string(paste0("normal(0,1)"), class = "b",coef=predictors[k])
          prior_combine<-prior_combine+tmp_prior
        }
        return(prior_combine)
      }
      
      fit <- brm(
        formula=brms_formula,
        data = clean_data,
        sample_prior = TRUE,
        family = student(), 
        save_pars = save_pars(all = TRUE),
        chains = 8, cores = 8,seed=42,iter = 5000, warmup = 1000, thin = 1,
        prior = dynamic_prior(predictors),
        control = list(adapt_delta = 0.95)
        )
      
      fits[[i]] <- add_criterion(fit, "loo")
    }
    saveRDS(fits, outfile)
  }
  
  for (i in 1:length(test_models)){

    #get posterior samples
    samples <- as.data.frame(fits[[i]]) 
    
    #extract prior samples
    prior_samples<-samples[,grep("prior", names(samples), ignore.case = TRUE)]
    
    # remove extra columns (interpret b_Intercept, not Intercept,priors)
    samples <- samples[, !(names(samples) %in% c("Intercept","sigma", "prior", "lp__"))]
    samples <- samples[,-grep("prior", names(samples), ignore.case = TRUE)]
    samples <- samples[,-grep("nu", names(samples), ignore.case = TRUE)]
    
    n_pars<-ncol(samples) #define number of parameters in current model
    
    tmp_loo<-data.frame(loo(fits[[i]])[['estimates']])
    elpd_loo<-round(tmp_loo$Estimate[1],2)
    
    # calculate frequentist values (model R2)
    r2_1 <- mean(bayes_R2(fits[[1]]))
    r2_2 <- mean(bayes_R2(fits[[i]]))
    
    if(i==1){
      delta_elpd_se<-"--"
      delta_elpd<-"--"
      model_bf<-"--"
      model_bf_direction<-"--"
      model_bf_range<-"--"
      r2_change<-"--"
      cohen_f2<-"--"
    }else{
      tmp_loo_compare<-data.frame(loo_compare(fits[[i]],fits[[1]]))#compare current model to model1 
      tmp_bf_compare<-capture.output(brms::bayes_factor(fits[[i]],fits[[1]],log=FALSE)) #model-level bayes factor
      tmp_bf_compare<-tmp_bf_compare[length(tmp_bf_compare)]
      model_bf <- round(as.numeric(sub(".+\\s(\\d+\\.?\\d*)$", "\\1", tmp_bf_compare)),2) #bf of current model over model1
      
      # calculate frequentist values: r squared change and cohen's f2
      r2_change<-round(r2_2-r2_1,3)
      cohen_f2 <- round((r2_2 - r2_1) / (1 - r2_2),2)
      
      #direction of BF evidence
      if(model_bf==1){
        model_bf_direction<-"No Evidence"
      }else if(model_bf>1){
        model_bf_direction<-"Alt"
      }else{
        model_bf_direction<-"Null"
      }
      
      # strength of bayes factor evidence. ranges from Lee and Wagenmakers (2014)
      if((model_bf >= 100)){
        model_bf_range <- "Extreme"
      }else if(model_bf >= 30 && model_bf < 100){
        model_bf_range <- "Very Strong"
      }else if(model_bf >= 10 && model_bf < 30){
        model_bf_range <- "Strong"
      }else if(model_bf >= 3 && model_bf < 10){
        model_bf_range <- "Moderate"
      }else if(model_bf >= 1 && model_bf < 3){
        model_bf_range <- "Anecdotal"
      }else if(model_bf == 1){
        model_bf_range <- "No Evidence"
      }else if(model_bf >= 0.33 && model_bf < 1){
        model_bf_range <- "Anecdotal"
      }else if(model_bf >= 0.1 && model_bf < 0.33){
        model_bf_range <- "Moderate"
      }else if(model_bf >= 0.03 && model_bf < 0.1){
        model_bf_range <- "Strong"
      }else if(model_bf >= 0.01 && model_bf < 0.03){
        model_bf_range <- "Very Strong"
      }else if(model_bf < 0.01){
        model_bf_range <- "Extreme"
      }else{
        model_bf_range <- ""
      }
      
      #if current model = better fit than model 1, then it will appear in row1.
      ## then we need to get the elpd diff from row 2 and flip it.
      if(rownames(tmp_loo_compare)[1]=="fits[[i]]"){ 
        
        # get difference from row2 and flip it
        delta_elpd<-round(tmp_loo_compare$elpd_diff[2],2)*-1
        delta_elpd_se<-round(tmp_loo_compare$se_diff[2],2)
        
      }else{  #if current model = worse fit than model 1, then it will appear in row2
        delta_elpd<-round(tmp_loo_compare$elpd_diff[2],2)
        delta_elpd_se<-round(tmp_loo_compare$se_diff[2],2)
      }
    }
    
    #generate model summary
    tmp_mod_summary<-data.frame(Model=i,
                                Ref_Model=1,
                                elpd_loo=elpd_loo,
                                delta_elpd=paste0(delta_elpd," (",delta_elpd_se,")"),
                                model_bf=paste0(model_bf," (",model_bf_direction,
                                                                  "/",model_bf_range,")"),
                                r2=paste0(round(r2_2,3)," (",r2_change,")"),
                                f2=cohen_f2,
                                Predictors="", #predictor name
                                pred_bf="",#predictor bf
                                Mean_and_HDI="") #mean and 95% HDI of posterior
    
    for (j in 1:n_pars){
      tmp_par<-colnames(samples)[j]
      tmp_par <- sub("^b_", "", tmp_par)

      post_par<-paste("b_",tmp_par,sep="") #par name formatted as posterior pars
      
      if(post_par=="b_Intercept"){
        prior_par<-paste("prior_",tmp_par,sep="") #par name formatted as prior pars
      }else{
        prior_par<-paste("prior_b_",tmp_par,sep="") #par name formatted as prior pars
      }

      # CALCULATE BAYES FACTORS FOR PREDICTORS W/ SAVAGE DICKEY DENSITY RATIO (WAGENMAKERS 2010)

      ## for function, enter posterior samples, prior samples, and estimation method (we use logspline)
      
      ## we calculate the ratio to be consistent with the rest of the paper,
      ## such that values >1 favor the alternative and values <1 favor the null
      savage_dickey_ratio<-function(posterior,prior,method){ 
      
        ### logspline kernel density estimation (as in Wagenmakers 2010)
        if(method=="logspline"){
          library(logspline)
          
          log_spline_density<-invisible(logspline(posterior,silent=TRUE)) #density estimate for posterior
          posterior_density<-dlogspline(0, log_spline_density, log = F)  #density @ 0
          
          log_spline_density<-invisible(logspline(prior,silent=TRUE)) #density estimate for prior
          prior_density<-dlogspline(0, log_spline_density, log = F)  #density @ 0
          
          # a group effect of 0 is ~11 times LESS likely than it was before seeing the data (i.e., 1/.09)
          bf<-round(prior_density/posterior_density,2)
        }
        
        #direction of BF evidence
        if(bf==1){
          bf_direction<-"No Evidence"
        }else if(bf>1){
          bf_direction<-"Alt"
        }else{
          bf_direction<-"Null"
        }
        
        # strength of bayes factor evidence. ranges from Lee and Wagenmakers (2014)
        if((bf >= 100)){
          bf_range <- "Extreme"
        }else if(bf >= 30 && bf < 100){
          bf_range <- "Very Strong"
        }else if(bf >= 10 && bf < 30){
          bf_range <- "Strong"
        }else if(bf >= 3 && bf < 10){
          bf_range <- "Moderate"
        }else if(bf >= 1 && bf < 3){
          bf_range <- "Anecdotal"
        }else if(bf == 1){
          bf_range <- "No Evidence"
        }else if(bf >= 0.33 && bf < 1){
          bf_range <- "Anecdotal"
        }else if(bf >= 0.1 && bf < 0.33){
          bf_range <- "Moderate"
        }else if(bf >= 0.03 && bf < 0.1){
          bf_range <- "Strong"
        }else if(bf >= 0.01 && bf < 0.03){
          bf_range <- "Very Strong"
        }else if(bf < 0.01){
          bf_range <- "Extreme"
        }else{
          bf_range <- ""
          
        }
        combine<-list(bf=bf,
                      bf_range=bf_range,
                      bf_direction=bf_direction)
        
          return(combine)
      }
      
      # reflects how likely are data under alternative hypothesis (vs null hypothesis that effect = 0)
      ## values >1 favor the alternative; values < 1 favor the null
      pred_bf<-savage_dickey_ratio(samples[[post_par]],prior_samples[[prior_par]],"logspline")
      
      #calculate mean and HDI of given set of samples, for a given credible mass
      mean_and_hdi<-function(samples, credMass){
        library(HDInterval)
        lower<-HDInterval::hdi(samples,credMass=credMass)[1]
        upper<-HDInterval::hdi(samples,credMass=credMass)[2]
        mean<-mean(samples)
        
        if(lower>0&&upper>0|lower<0&&upper<0){
          credible<-"*"
        }else{
          credible<-""
        }
        
        summary<-paste(round(mean,2)," [",round(lower,3),", ",round(upper,3),"]",credible,sep="")
        return(summary)
      } 

      #generate predictor summary
      tmp_pred_summary<-data.frame(Model="",
                                   Ref_Model="",
                                   elpd_loo="",
                                   delta_elpd="",
                                   model_bf="",
                                   r2="",
                                   f2="",
                                   Predictors=tmp_par, #predictor name
                                   pred_bf=paste0(pred_bf[["bf"]]," (", pred_bf[["bf_direction"]],
                                                                    "/",pred_bf[["bf_range"]],")"), #predictor bf strength/direction
                                   Mean_and_HDI=mean_and_hdi(samples=samples[[post_par]],credMass = credMass)) 
      
      if(j==1){ #append to predictor summary dataframe
        running_summary<-tmp_pred_summary
      }else{
        running_summary<-rbind(running_summary,tmp_pred_summary)
      }
    }
    
    #combine model summary and predictor summary
    combined_summary<-rbind(tmp_mod_summary,running_summary)
    
    if(i==1){
      all_summary<-combined_summary
    }else{
      all_summary<-rbind(all_summary,combined_summary)
    }
  }
  
  colnames(all_summary)<-c("Model","Ref Model","ELPD",
                        paste("\u0394","ELPD (SE)",sep=""),
                        "Model BF",paste("R2 (","\u0394","R2)",sep=""),"f2","Pred",
                        "Pred BF",
                        paste0("Pred Mean [",as.character(credMass*100),"% HDI]"))
  return(all_summary)
}
print_diff_mean_hdi<-function(samples, credMass,digits){
        
        # samples: should be dataframe with nrows=nposterior samples and 2 columns
        # col names should be names of groups you want printed. diff will be
        # calculated as col 1 minus col2.
        
        #credMass = what credible interval do you want to use for HDI?
        
        #digits= how many digits to round output to
        
        library(HDInterval)
        diff<-samples[,1]-samples[,2]
        
        lower<-HDInterval::hdi(diff,credMass=credMass)[1]
        upper<-HDInterval::hdi(diff,credMass=credMass)[2]
        mean<-mean(diff)
        
        if(lower>0&&upper>0|lower<0&&upper<0){
          credible<-"*"
        }else{
          credible<-""
        }
        
        group1<-colnames(samples)[1]
        group2<-colnames(samples)[2]
        
        summary<-paste0(group1,"-",group2,": ",round(mean,digits)," [",
                        round(lower,digits),", ",
                        round(upper,digits),"]",credible)
        
        return(summary)
      }
bayes_corr_matrix<-function(data,credMass){

  library(bayestestR)
  library(BayesFactor)
  options(scipen = 999) #deters r from using scientific notation
  
  n_vars<-ncol(data)
  vars<-colnames(data)
  
  # create empty correlation matrix to hold outputs
  corr_mat<-as.data.frame(matrix(data="",ncol=n_vars,nrow=n_vars))
  rownames(corr_mat)<-colnames(corr_mat)<-vars
  
  #create correlation matrices to hold mean rho estimates, bayes factors, rho ci's, and bf ranges
  corr_mat_credible<-corr_mat_rho<-corr_mat_bf<-corr_mat_ci<-corr_mat_bf_range<-corr_mat
  
  iteration<-0
  
  for (i in 1:n_vars){ #var1 / rows
    for (j in 1:n_vars){ #var2 / cols
      
      var1<-vars[i] #name of variable1
      var2<-vars[j] #name of variable2
      data1<-as.numeric(data[[var1]]) #get data for variable1
      data2<-as.numeric(data[[var2]]) #get data for variable2
      
      n_obs<-nrow(na.omit(cbind(data1,data2)))#number of complete observations
      
      if(var1==var2){ #if variables match, set rho (corr coefficient) to 1, otherwise run bayes correlations
        corr_mat_rho[i,j]<-1
      }else{
        
        if(n_obs>2){ #if at least 3 observations, set rho to NA and skip
          iteration<-iteration+1
          
          result <- correlationBF(data1, data2) #calculate BF
          output<-describe_posterior(result, ci=credMass, centrality = "mean",ci_method="HDI") #get output 
          rho<-round(output$Mean,2) #mean of rho (correlation coefficient estimate)
          ci_lo<-round(output$CI_low,2) #lower bound of credible interval around rho
          ci_hi<-round(output$CI_high,2) #upper bound of credible interval around rho
          bf<-output$BF #bayes factor
          bf_direction<-ifelse(bf<1,"Null","Alternative")
          
          # ranges from Lee and Wagenmakers (2014)
          if(bf >= 100){
            bf_range <- "Extreme"
          }else if(bf >= 30 && bf < 100){
            bf_range <- "Very Strong"
          }else if(bf >= 10 && bf < 30){
            bf_range <- "Strong"
          }else if(bf >= 3 && bf < 10){
            bf_range <- "Moderate"
          }else if(bf >= 1 && bf < 3){
            bf_range <- "Anecdotal"
          }else if(bf == 1){
            bf_range <- "No Evidence"
          }else if(bf >= 0.33 && bf < 1){
            bf_range <- "Anecdotal"
          }else if(bf >= 0.1 && bf < 0.33){
            bf_range <- "Moderate"
          }else if(bf >= 0.03 && bf < 0.1){
            bf_range <- "Strong"
          }else if(bf >= 0.01 && bf < 0.03){
            bf_range <- "Very Strong"
          }else if(bf < 0.01){
            bf_range <- "Extreme"
          }else{
            bf_range <- ""
          }
          
          if((ci_lo>0&&ci_hi>0)|(ci_lo<0&&ci_hi<0)){
            credible<-"*"
          }else{
            credible<-""
          }
          
          tmp_long_summary<-data.frame(Variable1=var1,
                                       Variable2=var2,
                                       Mean=rho,
                                       HDI=paste("[",ci_lo,",",ci_hi,"]",credible,sep=""),
                                       BF=round(bf,2),
                                       BF_Evidence_Strength=bf_range,
                                       BF_Evidence_Favors=bf_direction,
                                       N=n_obs)
          
          corr_mat_rho[i,j]<-rho
          corr_mat_bf[i,j]<-bf
          corr_mat_bf_range[i,j]<-bf_range
          corr_mat_ci[i,j]<-paste("[",ci_lo,",",ci_hi,"]",sep="")
          corr_mat_credible[i,j]<-credible
          
          if(iteration==1){
            all_long_summary<-tmp_long_summary
          }else{
            all_long_summary<-rbind(all_long_summary,tmp_long_summary)
          }
        }else{
          corr_mat_rho[i,j]<-NA #if fewer than 3 observations, set rho to NA
          corr_mat_credible[i,j]<-""
        }
      }
    }
  }
  
  var_list<-data.frame(Variable=rownames(corr_mat_rho))
  
  # Convert dataframe to long format for Rho values
  corr_mat_rho<-cbind(var_list,corr_mat_rho)
  df_long <- melt(corr_mat_rho,id="Variable",varnames = c("Variable","value"))
  colnames(df_long)<-c("Variable1","Variable2","Mean")
  df_long$Mean<-as.numeric(df_long$Mean)
  
  #do the same for CI's
  corr_mat_ci<-cbind(var_list,corr_mat_ci)
  df_long_ci <- melt(corr_mat_ci,id="Variable",varnames = c("Variable","value"))
  colnames(df_long_ci)<-c("Variable1","Variable2","CI")
  
  #do the same for BF
  corr_mat_bf<-cbind(var_list,corr_mat_bf)
  df_long_bf <- melt(corr_mat_bf,id="Variable",varnames = c("Variable","value"))
  colnames(df_long_bf)<-c("Variable1","Variable2","BF")
  
  #do the same for BF range
  corr_mat_bf_range<-cbind(var_list,corr_mat_bf_range)
  df_long_bf_range <- melt(corr_mat_bf_range,id="Variable",varnames = c("Variable","value"))
  colnames(df_long_bf_range)<-c("Variable1","Variable2","BF_Range")
  
  #do the same for credible
  corr_mat_credible<-cbind(var_list,corr_mat_credible)
  df_long_credible <- melt(corr_mat_credible,id="Variable",varnames = c("Variable","value"))
  colnames(df_long_credible)<-c("Variable1","Variable2","Credible")
  
  # Create the plot with separate labels for Rho and corr_mat_credible
  p <- ggplot(df_long, aes(x = factor(Variable1, levels = vars), y = factor(Variable2, levels = vars))) +
    geom_tile(aes(fill = Mean), color = "black") +
    geom_text(aes(label = round(Mean, 2)), color = "black", vjust = 0.3) +  # Add Rho values
    geom_text(data = df_long_credible, aes(label = Credible), color = "black", vjust = 1.5,size=6) +  # Add credible values
    scale_fill_gradient2(low = "steelblue", high = "firebrick", midpoint = 0, na.value = "transparent", guide = "legend") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(title = "Correlation Matrix",
         x = "", y = "",
         fill = "Correlation\nStrength")
  
  rownames(all_long_summary)<-NULL
  
  combined<-list(long_summary=all_long_summary,
                 corr_mat_rho=corr_mat_rho,
                 corr_mat_ci=corr_mat_ci,
                 corr_mat_bf_range=corr_mat_bf_range,
                 corr_mat_bf=corr_mat_bf,
                 corr_mat_credible=corr_mat_credible,
                 plot=p)

  return(combined)
}
plot_parrecover_group_bysim<-function(data,par,parname,sim_id,genMean_suffix,simMean_suffix, simHDI_suffix,recover_suffix){
  text<-paste(round(mean(data[[paste0(par,recover_suffix)]]),2)*100,"% Recovery",sep="")
  data[[paste0(par,recover_suffix)]] <- factor(data[[paste0(par,recover_suffix)]], levels = c("1", "0"), labels = c("recovered", "not_recovered"))
  
  n_sims<-max(data[[sim_id]])
  data[[sim_id]]<-factor(data[[sim_id]])
  
  title<-paste0(parname,"\nParameter Recovery")
  xlabel<-"Parameter Value"
  
  xaxis_min<-min(data[[paste0(par,simHDI_suffix,"lo")]])-(min(data[[paste0(par,simHDI_suffix,"lo")]])*.1)
  xaxis_max<-min(data[[paste0(par,simHDI_suffix,"hi")]])+(max(data[[paste0(par,simHDI_suffix,"hi")]])*.1)
  
  text_position<-mean(data[[paste0(par,genMean_suffix)]])
    
  p <- ggplot(data = data, aes(x = .data[[paste0(par,simMean_suffix)]],y=.data[[sim_id]], color=.data[[paste0(par,recover_suffix)]])) +
  geom_point(data = data, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(data[[paste0(par,genMean_suffix)]]))+
  theme_bw() +
  geom_errorbar(data = data, aes(xmin = .data[[paste0(par,simHDI_suffix,"lo")]], xmax = .data[[paste0(par,simHDI_suffix,"hi")]]), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,n_sims+10)) +
  ggtitle(title) +
  xlab(xlabel) +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "label", x = text_position, y =n_sims+8, label = text, hjust = "center",size=3,lineheight=.9)
  return(p)

}
plot_parrecover_subj<-function(data,par,parname,sim_id,genMean_suffix,simMean_suffix, simHDI_suffix,recover_suffix){
  
  cor_coef<-cor(data[[paste0(par,simMean_suffix)]],data[[paste0(par,genMean_suffix)]],method = c("pearson"))
  text<-paste0(round(mean(data[[paste0(par,recover_suffix)]]),2)*100,"% Recovery; r = ",
              round(cor_coef,2))
  
  data[[paste0(par,recover_suffix)]] <- factor(data[[paste0(par,recover_suffix)]], levels = c("1", "0"), labels = c("recovered", "not_recovered"))
  
  n_sims<-max(data[[sim_id]])
  data[[sim_id]]<-factor(data[[sim_id]])
  
  title<-paste0(parname,"\nParameter Recovery")
  xlabel<-paste0("Parameter Value (",parname,")")
  
  axis_min<-min(data[[paste0(par,simHDI_suffix,"lo")]])-(min(data[[paste0(par,simHDI_suffix,"lo")]])*.05)
  axis_max<-max(data[[paste0(par,simHDI_suffix,"hi")]])+(max(data[[paste0(par,simHDI_suffix,"hi")]])*.05)
  
  text_position_x<-text_position_y<-(axis_max+axis_min)/2
  
  #text_position_x<-mean(data[[paste0(par,genMean_suffix)]])-(mean(data[[paste0(par,genMean_suffix)]])*.2)
  #text_position_y<-mean(data[[paste0(par,simMean_suffix)]])+(mean(data[[paste0(par,genMean_suffix)]])*.2)
    
  p <- ggplot(data = data, aes(x = .data[[paste0(par,genMean_suffix)]],y=.data[[paste0(par,simMean_suffix)]], color=.data[[paste0(par,recover_suffix)]])) +
  geom_point(data = data, alpha = .2) + 
  geom_errorbar(data = data, aes(ymin=.data[[paste0(par,simHDI_suffix,"lo")]],ymax=.data[[paste0(par,simHDI_suffix,"hi")]]),alpha=.15)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(axis_min,axis_max),ylim = c(axis_min, axis_max))+theme_bw()+
  ggtitle(title) +
  theme(legend.position="none",plot.title = element_text(size=12))+
  xlab("Generating Parameter Value")+
  ylab("Estimated Parameter Value")+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "label", x = text_position_x, y =text_position_y, label = text, hjust = "center",size=3,lineheight=.9)
  
  return(p)
  
}
pcamethods_bpca<-function(data,npcs,scale,center,method){
  library(pcaMethods)
  library(psych)
  library(stats)
  
  n_var<-ncol(data)
  
  if(method=="pcamethods"){
      na_rows <- which(apply(data, 1, function(x) all(is.na(x))))
      if(length(na_rows)!=0){
        data_no_na <- data[-na_rows, ]
      }else{
        data_no_na<-data
      }
      pc <- scores(pcaMethods::pca(data_no_na, nPcs = npcs, method = "bpca",scale=scale,center=center))
      
      for(i in 1:npcs){
        tmp_col<-paste0("PC",i)
        data[[tmp_col]]<-NA
        if(length(na_rows)!=0){
          data[[tmp_col]][-na_rows] <- pc[,i]
        }else{
          data[[tmp_col]] <- pc[,i]
        }
      }

      # if more that half of the correlations between PC and orig vars is negative, then *-1
      for(i in 1:npcs){
        tmp_col<-paste0("PC",i)
        corMat<-ifelse(cor(data,use="complete.obs")>0,1,0)
        tmp_mat_row<-which(rownames(corMat)==tmp_col)
        n_pos<-sum(corMat[tmp_mat_row,1:n_var]) 
        if(n_pos/n_var<.5){
          data[[tmp_col]]<-data[[tmp_col]]*-1
        }
      }
      
  }
  
  if(method=="psych"){
      na_rows <- which(apply(data, 1, function(x) all(is.na(x))))
      data_no_na <- data[-na_rows, ]
      imputed <- completeObs(pcaMethods::pca(data_no_na, nPcs = npcs, method = "bpca",scale=scale,center=center))
      
      pc<-psych::pca(imputed,npcs=npcs)$scores
      
      for(i in 1:npcs){
        tmp_col<-paste0("PC",i)
        data[[tmp_col]]<-NA
        data[[tmp_col]][-na_rows] <- pc[,i]
      }
  }
  
  

  return(data)
}
plot_posterior_diffs<-function(data,ci,par_prefix,rawpar_prefix,sig_prefix,par_name,group_nums,group_names,sample_sizes,round_digits,showtext){
  library(HDInterval)
  n_groups<-length(group_nums)

  #first define temp data frame to hold plot data based on number of groups
  column_names <- c("group", "mean", "hdi_hi", "hdi_lo")
  plot_data <- data.frame(matrix(NA, nrow = n_groups, ncol = length(column_names)))
  colnames(plot_data) <- column_names

  #define mean and ci of each groups posterior
  for (i in 1:n_groups){
    plot_data$group[i]<-group_names[i]
    plot_data$mean[i]<-mean(data[[paste0(par_prefix,i,".")]])
    plot_data$hdi_lo[i]<-HDInterval::hdi(data[[paste0(par_prefix,i,".")]],credMass = ci)[1]
    plot_data$hdi_hi[i]<-HDInterval::hdi(data[[paste0(par_prefix,i,".")]],credMass = ci)[2]
  }

  #now calculate pairwise group differences
  combinations_nums<-as.data.frame(t(combn(group_nums, 2))) #unique combinations of groups
  combinations_names<-as.data.frame(t(combn(group_names, 2))) #unique combinations of groups

  for(i in 1:nrow(combinations_nums)){
    group1_id<-combinations_nums[i,1]
    group2_id<-combinations_nums[i,2]
    group1_name<-combinations_names[i,1]
    group2_name<-combinations_names[i,2]
    group1_size<-sample_sizes[which(group_names==group1_name)]
    group2_size<-sample_sizes[which(group_names==group2_name)]

    diff_posterior<-data[[paste0(par_prefix,group1_id,".")]]-data[[paste0(par_prefix,group2_id,".")]]
    diff_mean<-round(mean(diff_posterior),round_digits)
    diff_hdi_lo<-as.numeric(HDInterval::hdi(diff_posterior,credMass=ci)[1])
    diff_hdi_hi<-as.numeric(HDInterval::hdi(diff_posterior,credMass=ci)[2])

    group1_rawpar<-data[[paste0(rawpar_prefix,group1_id,".")]]
    group2_rawpar<-data[[paste0(rawpar_prefix,group2_id,".")]]
      
    if(is.na(sig_prefix)){
      cohen_d<-NA
    }else{
      #calculate pooled SD for cohen's d effect sizes
      group1_sigma<-data[[paste0(sig_prefix,group1_id,".")]]
      group2_sigma<-data[[paste0(sig_prefix,group2_id,".")]]

      sigma_pooled <- sqrt(((mean(group1_sigma)^2 + mean(group2_sigma)^2) / 2))#Cohen 1988
      cohen_d<-round(mean(group1_rawpar-group2_rawpar)/sigma_pooled,round_digits)
    }

    if((diff_hdi_hi>0&diff_hdi_lo>0)|(diff_hdi_hi<0&diff_hdi_lo<0)){
      cred_index<-'*'
    }else{
      cred_index<-''
    }

    diff_hdi_lo<-round(diff_hdi_lo,round_digits)
    diff_hdi_hi<-round(diff_hdi_hi,round_digits)
    
    savage_dickey_ratio<-function(posterior,prior,method){ 
  
  # logspline kernel density estimation (as in Wagenmakers 2010)
  if(method=="logspline"){
    library(logspline)
    
    log_spline_density<-invisible(logspline(posterior,silent=TRUE,error.action = 1)) #density estimate for posterior
    if(is.null(log_spline_density)==FALSE){
      posterior_density<-dlogspline(0, log_spline_density, log = F)  #density @ 0
      log_spline_density<-invisible(logspline(prior,silent=TRUE)) #density estimate for prior
      prior_density<-dlogspline(0, log_spline_density, log = F)  #density @ 0
      
    }else{
      library(MASS)
      
      # Fit normal distribution to posterior samples
      fit_posterior <- fitdistr(posterior, "normal")
      posterior_mean <- fit_posterior$estimate[1]
      posterior_sd <- fit_posterior$estimate[2]
      
      # Laplace approximation Bayes Factor calculation using likelihood ratio at 0 (mode of the normal distribution)
      posterior_density <- dnorm(0, mean = posterior_mean, sd = posterior_sd)
      prior_density <- dnorm(0, mean = 0, sd = 1)
    }
    
    # a group effect of 0.09 is ~11 times LESS likely than it was before seeing the data (i.e., 1/.09)
    bf<-prior_density/posterior_density
  }else if(method=="kde"){
    library(stats)
    
    kde_density1 <- density(posterior)
    posterior_density <- approx(kde_density1$x, kde_density1$y, xout = 0)$y
    
    # if density too low to be estimated, then use the smallest representable number that R allows
    if(is.na(posterior_density)){
      posterior_density<-.Machine$double.xmin
    }
    
    kde_density2 <- density(prior)
    prior_density <- approx(kde_density2$x, kde_density2$y, xout = 0)$y
    bf<-prior_density/posterior_density
  }
  
  #direction of BF evidence
  if(bf==1){
    bf_direction<-"No Evidence"
  }else if(bf<1){
    bf_direction<-"Null"
  }else{
    bf_direction<-"Alt"
  }
  
  # strength of bayes factor evidence. ranges from Lee and Wagenmakers (2014)
  if((bf >= 100)){
    bf_range <- "Extreme"
  }else if(bf >= 30 && bf < 100){
    bf_range <- "Very Strong"
  }else if(bf >= 10 && bf < 30){
    bf_range <- "Strong"
  }else if(bf >= 3 && bf < 10){
    bf_range <- "Moderate"
  }else if(bf >= 1 && bf < 3){
    bf_range <- "Anecdotal"
  }else if(bf == 1){
    bf_range <- "No Evidence"
  }else if(bf >= 0.33 && bf < 1){
    bf_range <- "Anecdotal"
  }else if(bf >= 0.1 && bf < 0.33){
    bf_range <- "Moderate"
  }else if(bf >= 0.03 && bf < 0.1){
    bf_range <- "Strong"
  }else if(bf >= 0.01 && bf < 0.03){
    bf_range <- "Very Strong"
  }else if(bf < 0.01){
    bf_range <- "Extreme"
  }else{
    bf_range <- ""
    
  }
  combine<-list(bf=bf,
                bf_range=bf_range,
                bf_direction=bf_direction)
  
  return(combine)
}

    bf<-round(savage_dickey_ratio(group1_rawpar-group2_rawpar,rnorm(length(diff_posterior),0,1),"logspline")[["bf"]],round_digits)
    
    temp_text<-paste0(group1_name,"-",group2_name,": [",diff_hdi_lo,",",diff_hdi_hi,"], M = ",diff_mean,cred_index,"\nBF = ",bf,", d = ",cohen_d)

    #if not the last comparison, add a line break to the text
    if(i != nrow(combinations_nums)){
      temp_text<-paste0(temp_text,"\n")
    }
    if(i==1){
      text<-temp_text
    }else{
      text<-paste0(text,temp_text)
    }
  }

  #after we're done with all the combinations, make the plot and return it

  if(min(plot_data$hdi_lo)<0){
    yaxis_min<-min(plot_data$hdi_lo)*1.1
  }else{
    yaxis_min<-min(plot_data$hdi_lo)-(min(plot_data$hdi_lo)*.1)
  }

  if(max(plot_data$hdi_hi)<0){
    ytext_position<-max(plot_data$hdi_hi)-(max(plot_data$hdi_hi)*.1)
    yaxis_max<-max(plot_data$hdi_hi)-(max(plot_data$hdi_hi)*.15)
  }else{
    ytext_position<-max(plot_data$hdi_hi)+(max(plot_data$hdi_hi)*.1)
    yaxis_max<-max(plot_data$hdi_hi)+(max(plot_data$hdi_hi)*.15)
  }

  xtext_position<-mean(group_nums)

  if(showtext==FALSE){
    text<-""
  }

  p<-ggplot(data=plot_data, aes(x=group, y=mean,color=group,alpha=.4)) +
  geom_point(data=plot_data,alpha=1,position=position_dodge(width = .5),size=4) + theme_bw()+
  geom_errorbar(data=plot_data,aes(ymin=hdi_lo,ymax=hdi_hi),linewidth=1,
                width=0,alpha=1,position=position_dodge(width = .5)) +
  coord_cartesian(ylim = c(yaxis_min,yaxis_max))+
  ggtitle(par_name) + xlab("") +
  scale_x_discrete(limits=group_names)+
  scale_color_manual(values=c('#57b6c9','#966acc'))+
  ylab(par_name)+
  theme(axis.text.x = element_text(angle = 0,vjust=1,hjust=0.5,size=10),
        axis.text.y = element_text(size=10))+
  annotate(geom = "text", x = xtext_position, y = ytext_position, label = text, hjust = "center",size=3.5,lineheight=.9)
  p
  return(p)

}
custom_scatter <- function(data, mapping, ...) {
  ggplot(data, mapping) +
    geom_point(aes(color = group), alpha = 0.5, size = 1) + # Semi-transparent points
    geom_smooth(aes(color = group), method = "lm", se = FALSE, 
                linetype = "solid", size = 0.8, alpha = 0.6) + # Semi-transparent trend lines
    scale_color_manual(
      values = c("HC" = "#1f78b4", "SZ" = "#33a02c"), # Softer colors
      name = "Group" # Legend title
    ) +
    theme(legend.position = "bottom") 
}
anova_summary<-function(group,dv){
  tmp_data<-data.frame(iv=as.factor(group),dv=dv)
  tmp_groups<-unique(tmp_data$iv)
  anova_result <- aov(dv ~ iv, data = tmp_data)
  summary<-summary(anova_result)

  # Extract p-value, df, and F-value
  p_value <-summary[[1]]$Pr[1]
  df <- summary[[1]]$Df[1]
  f_value <- summary[[1]]$F[1]
  sig<-ifelse(p_value<.05,'*',ifelse(p_value<.01,'**',ifelse(p_value<.001,"***","")))

  # Report in APA format
  apa_format <- paste("F(", df, ")=", format(round(f_value, 2), nsmall = 2), ", p=", round(p_value, 3),sig,sep="")
  
  if(sig==""){
    posthoc_output_all<-"--"
  }else{
    posthoc<-data.frame(TukeyHSD(anova_result)[["iv"]])
    posthoc_rows<-rownames(posthoc)
  
    for (i in 1:nrow(posthoc)){#post hoc pairwise
      tmp_compare<-posthoc_rows[i]
      grp1 <- str_extract(tmp_compare, "^[^-]*")
      grp2 <- str_extract(tmp_compare, "(?<=-)[^-]*$")
      if(posthoc$p.adj[i]>.05){
        tmp_operator<-"≈"
      }else{
        if(posthoc$diff[i]<0){
          tmp_operator<-"<"
        }else{
          tmp_operator<-">"
        }
      }
      posthoc_output<-paste(grp1,tmp_operator,grp2,sep="")
      if(i==1){
        posthoc_output_all<-posthoc_output
      }else{
        posthoc_output_all<-paste(posthoc_output_all,posthoc_output,sep="; ")
      }
    }
  }
  
  anova_output<-list(apa_format,posthoc_output_all)
  return(anova_output)
}
BFanova_summary<-function(group,dv,credMass){
  
  library(BayesFactor)
  library(bayestestR)
  
  tmp_data<-data.frame(iv=group,dv=dv)
  tmp_data<-na.omit(tmp_data)
  tmp_groups<-unique(tmp_data$iv)
  
  #anova output 
  result<-anovaBF(dv~iv,data=tmp_data,progress=FALSE) #calculate anova BF
  output<-describe_posterior(result, ci=credMass, centrality = "mean",ci_method="HDI") #get summary
  group_bf<-round(exp(result@bayesFactor[["bf"]]),2) #have to exp transform bc bayesFactor is in log space
  
  #direction of BF evidence
  if(group_bf==1){
    group_bf_direction<-"No Evidence"
  }else if(group_bf>1){
    group_bf_direction<-"Alt."
  }else{
    group_bf_direction<-"Null"
  }
  
  # strength of bayes factor evidence. ranges from Lee and Wagenmakers (2014)
  if((group_bf >= 100)){
    group_bf_range <- "Extreme"
  }else if(group_bf >= 30 && group_bf < 100){
    group_bf_range <- "Very Strong"
  }else if(group_bf >= 10 && group_bf < 30){
    group_bf_range <- "Strong"
  }else if(group_bf >= 3 && group_bf < 10){
    group_bf_range <- "Moderate"
  }else if(group_bf >= 1 && group_bf < 3){
    group_bf_range <- "Anecdotal"
  }else if(group_bf == 1){
    group_bf_range <- "No Evidence"
  }else if(group_bf >= 0.33 && group_bf < 1){
    group_bf_range <- "Anecdotal"
  }else if(group_bf >= 0.1 && group_bf < 0.33){
    group_bf_range <- "Moderate"
  }else if(group_bf >= 0.03 && group_bf < 0.1){
    group_bf_range <- "Strong"
  }else if(group_bf >= 0.01 && group_bf < 0.03){
    group_bf_range <- "Very Strong"
  }else if(group_bf < 0.01){
    group_bf_range <- "Extreme"
  }else{
    group_bf_range <- ""
  }
  
  apa_format<-paste("BF=",group_bf," (",group_bf_direction,"/",group_bf_range,")",sep="")
  
  ### POST HOC TESTS
  
  # Get unique levels of the factor
  unique_levels <- unique(tmp_data$iv)
  
  # Create all unique combinations, conver to string, filter out duplicate combinations
  combinations <- t(combn(unique_levels, 2))
  combinations_strings <- apply(combinations, 1, paste, collapse = "")
  combinations_filtered <- data.frame(combo=combinations_strings[!duplicated(combinations_strings)])
  
  combinations_filtered$group1 <- substr(combinations_filtered$combo, 1, 1)
  combinations_filtered$group2 <- substr(combinations_filtered$combo, 2, 2)
  combinations_filtered<-combinations_filtered[,-1]
  
  factor_levels <- levels(tmp_data$iv) # Get levels of group factor
  
  for (j in 1:nrow(combinations_filtered)){
    
    groupidx1<-as.numeric(combinations_filtered$group1[j])
    groupidx2<-as.numeric(combinations_filtered$group2[j])
    
    group1<-factor_levels[groupidx1]
    group2<-factor_levels[groupidx2]
    
    result <- ttestBF(tmp_data$dv[tmp_data$iv == group1], tmp_data$dv[tmp_data$iv == group2])
    output<-describe_posterior(result, ci=credMass, centrality = "mean",ci_method="HDI") #get summary
    
    post_bf<-round(output$BF[which(output$Parameter=="Difference")],2)
    post_ci_lo<-round(output$CI_lo[which(output$Parameter=="Difference")],2)
    post_ci_hi<-round(output$CI_hi[which(output$Parameter=="Difference")],2)
    post_ci<-paste("[",post_ci_lo,", ",post_ci_hi,"]",sep="")
    post_mean<-round(output$Mean[which(output$Parameter=="Difference")],2)
    
    if((post_ci_lo>0&&post_ci_hi>0)|(post_ci_lo<0&&post_ci_hi<0)){ #if difference is credible
      if(post_mean<0){
        tmp_operator<-"<"
      }else{
        tmp_operator<-">"
      }
    }else{
      tmp_operator<-"≈"
    }
    
    posthoc_output<-paste(group1,tmp_operator,group2,sep="")
    
    if(j==1){
      posthoc_output_all<-posthoc_output
    }else{
      posthoc_output_all<-paste(posthoc_output_all,posthoc_output,sep="; ")
    }
  }
  
  anova_output<-list(apa_format,posthoc_output_all)
  return(anova_output)
}
chisq_summary<-function(group1,group2){
  
  tmp_data <- table(group1, group2)
  chi_sq_result <- chisq.test(tmp_data)
    
  x_value <- chi_sq_result$statistic
  p_value <- chi_sq_result$p.value
  df <- chi_sq_result$parameter
  sig<-ifelse(p_value<.05,'*',ifelse(p_value<.01,'**',ifelse(p_value<.001,"***","")))
  apa_format <- paste("X^2(", df, ")=", round(x_value,2), ", p=", round(p_value,3),sig, sep = "")

  # posthoc tests
  if(sig==""){
    posthoc_output_all<-"--"
  }else{
    posthoc <- pairwise.prop.test(tmp_data,p.adjust.method="none")
    p_values <- posthoc$p.value
    col_names<-colnames(p_values)
    row_names<-rownames(p_values)
  
    for (i in 1:length(row_names)){#post hoc pairwise
     for (j in 1:length(col_names)){
       
       grp1<-row_names[i]
       grp2<-col_names[j]
       
         if(grp1!=grp2){
           if(p_values[i,j]!='NaN'|!is.na(p_values[i,j])){
             if(p_values[i,j]>.05){
              tmp_operator<-""
             }else{
              tmp_operator<-"*"
             }
             posthoc_output<-paste(grp1,"vs",grp2,tmp_operator,sep="")
             if(i==1&j==1){
               posthoc_output_all<-posthoc_output
             }else{
               posthoc_output_all<-paste(posthoc_output_all,posthoc_output,sep="; ")
             }
           }
         }
       }
     } 
  }
  
  
  chisq_output<-list(apa_format,posthoc_output_all)
  return(chisq_output)
}
BFchisq_summary<-function(group1,group2,credMass){
  
  library(BayesFactor)
  library(bayestestR)
  
  tmp_data<-data.frame(group1=group1,group2=group2)
  tmp_data<-na.omit(tmp_data)
  
  tmp_data <- table(tmp_data$group1, tmp_data$group2)
  
  #calculate chi-squared(ish) BF
  result<-contingencyTableBF(tmp_data,sampleType="indepMulti",fixedMargin="rows",progress=FALSE) 
  output<-describe_posterior(result, ci=credMass, centrality = "mean",ci_method="HDI") #get summary
  
  group_bf<-round(exp(result@bayesFactor[["bf"]]),2) ##need to exp transform bc BF is in log space
  
  #direction of BF evidence
  if(group_bf==1){
    group_bf_direction<-"No Evidence"
  }else if(group_bf>1){
    group_bf_direction<-"Alt."
  }else{
    group_bf_direction<-"Null"
  }
  
  # strength of bayes factor evidence. ranges from Lee and Wagenmakers (2014)
  if((group_bf >= 100)){
    group_bf_range <- "Extreme"
  }else if(group_bf >= 30 && group_bf < 100){
    group_bf_range <- "Very Strong"
  }else if(group_bf >= 10 && group_bf < 30){
    group_bf_range <- "Strong"
  }else if(group_bf >= 3 && group_bf < 10){
    group_bf_range <- "Moderate"
  }else if(group_bf >= 1 && group_bf < 3){
    group_bf_range <- "Anecdotal"
  }else if(group_bf == 1){
    group_bf_range <- "No Evidence"
  }else if(group_bf >= 0.33 && group_bf < 1){
    group_bf_range <- "Anecdotal"
  }else if(group_bf >= 0.1 && group_bf < 0.33){
    group_bf_range <- "Moderate"
  }else if(group_bf >= 0.03 && group_bf < 0.1){
    group_bf_range <- "Strong"
  }else if(group_bf >= 0.01 && group_bf < 0.03){
    group_bf_range <- "Very Strong"
  }else if(group_bf < 0.01){
    group_bf_range <- "Extreme"
  }else{
    group_bf_range <- ""
  }
  
  apa_format<-paste("BF=",group_bf," (",group_bf_direction,"/",group_bf_range,")",sep="")
  
  ### POST HOC TESTS
  
  #recombine data
  tmp_data<-data.frame(group1=group1,group2=group2)
  tmp_data<-na.omit(tmp_data)
  
  # Get unique levels of the factor
  unique_levels <- unique(tmp_data$group1)
  
  # Create all unique combinations, conver to string, filter out duplicate combinations
  combinations <- t(combn(unique_levels, 2))
  combinations_strings <- apply(combinations, 1, paste, collapse = "")
  combinations_filtered <- data.frame(combo=combinations_strings[!duplicated(combinations_strings)])
  
  combinations_filtered$group1 <- substr(combinations_filtered$combo, 1, 1)
  combinations_filtered$group2 <- substr(combinations_filtered$combo, 2, 2)
  combinations_filtered<-combinations_filtered[,-1]
  
  factor_levels <- levels(tmp_data$group1) # Get levels of group factor
  
  for (j in 1:nrow(combinations_filtered)){
    
    groupidx1<-as.numeric(combinations_filtered$group1[j])
    groupidx2<-as.numeric(combinations_filtered$group2[j])
    
    group1<-factor_levels[groupidx1]
    group2<-factor_levels[groupidx2]
    
    result <- ttestBF(tmp_data$group2[tmp_data$group1 == group1], tmp_data$group2[tmp_data$group1 == group2])
    output<-describe_posterior(result, ci=credMass, centrality = "mean",ci_method="HDI") #get summary
    
    post_bf<-round(output$BF[which(output$Parameter=="Difference")],2)
    post_ci_lo<-round(output$CI_lo[which(output$Parameter=="Difference")],2)
    post_ci_hi<-round(output$CI_hi[which(output$Parameter=="Difference")],2)
    post_ci<-paste("[",post_ci_lo,", ",post_ci_hi,"]",sep="")
    post_mean<-round(output$Mean[which(output$Parameter=="Difference")],2)
    
    if((post_ci_lo>0&&post_ci_hi>0)|(post_ci_lo<0&&post_ci_hi<0)){ #if difference is credible
      if(post_mean<0){
        tmp_operator<-"<"
      }else{
        tmp_operator<-">"
      }
    }else{
      tmp_operator<-"≈"
    }
    
    posthoc_output<-paste(group1,tmp_operator,group2,sep="")
    
    if(j==1){
      posthoc_output_all<-posthoc_output
    }else{
      posthoc_output_all<-paste(posthoc_output_all,posthoc_output,sep="; ")
    }
  }
  
  chisq_output<-list(apa_format,posthoc_output_all)
  return(chisq_output)
}

bf_footnote<-"* = Credible association (i.e., 95% HDI does not contain zero); Mean = mean posterior estimate of correlation coefficient (equivalent to Pearson R); HDI = 95% HDI of the correlation coefficient; BF = Bayes Factor; BF_Evidence_Strength =  Bayes Factor interpretation scheme based on Lee and Wagenmakers (2014); BF_Evidence_Favors = whether evidence favors the null hypothesis (i.e., the association between variable 1 and 2 is 0) or the alternative hypothesis (i.e., an association between variable 1 and 2 is not 0); N = Number of subjects with complete data included in a given test."

########################################################
# PREPROCESS DATA 
########################################################

#if processed BEHAVIORAL data doesnt exist, process it now
outfile<-paste(datadir,"/schizgaze12_gaze_beh.csv",sep="")
if(file.exists(outfile)==FALSE){
  
  # PROBLEMS -- schizgaze1 cuts off rts slower than stim response so we cant model it.
  # original preprocessing script for schizgaze2 removes trials where RT is <400 which is WAY too conservative.
  ### this is going to influence threshold and minRT

  data1<-paste(datadir,'SG1_gaze_beh.xlsx',sep="")
  sheet_names1 <- excel_sheets(data1)
  
  iteration<-1
  for (sheet in sheet_names1) {
    tmp_data <- data.frame(read_excel(data1, sheet = sheet))
    tmp_data$subj<-sheet #add subj id
    
    tmp_data<-dplyr::select(tmp_data,c(subj,GazeAngle:EyesEndorse))
    if(iteration==1){
      alldata1<-tmp_data
    }else{
      alldata1<-rbind(alldata1,tmp_data)
    }
    iteration<-iteration+1
  }
  
  alldata1$Study<-'schizgaze1'
  
  data2<-paste(datadir,'SG2_gaze_beh.xlsx',sep="")
  sheet_names2 <- excel_sheets(data2)
  
  iteration<-1
  for (sheet in sheet_names2) {
    tmp_data <- data.frame(read_excel(data2, sheet = sheet))
    tmp_data$subj<-sheet #add subj id
    
    tmp_data<-dplyr::select(tmp_data,c(subj,GazeAngle:EyesEndorse))
    if(iteration==1){
      alldata2<-tmp_data
    }else{
      alldata2<-rbind(alldata2,tmp_data)
    }
    iteration<-iteration+1
  }
  
  alldata2$Study<-'schizgaze2'
  
  #exclude 2032, male SZ patient that gave the same response for the entire task
  alldata2<-subset(alldata2,alldata2$subj!=2032)
  #exclude 2042 (only completed half of the task)
  alldata2<-subset(alldata2,alldata2$subj!=2042)
  
  #combine studies
  alldata<-rbind(alldata1,alldata2)
  
  #remove fixation cross rows
  alldata<-subset(alldata,alldata$Task!="Fixation")
  
  #recode responses.
  ## gender: 2=male, 3=female
  ## gaze: 2=yes,3=no
  
  alldata$GazeSlide.RESP <- ifelse(alldata$GazeSlide.RESP == 2, "Y", alldata$GazeSlide.RESP)
  alldata$GazeSlide.RESP <- ifelse(alldata$GazeSlide.RESP == 3, "N", alldata$GazeSlide.RESP)
  alldata$GenderSlide.RESP <- ifelse(alldata$GenderSlide.RESP == 2, "M", alldata$GenderSlide.RESP)
  alldata$GenderSlide.RESP <- ifelse(alldata$GenderSlide.RESP == 3, "F", alldata$GenderSlide.RESP)
  alldata$GenderCorrectResp <- ifelse(alldata$GenderCorrectResp == 2, "M", alldata$GenderCorrectResp)
  alldata$GenderCorrectResp <- ifelse(alldata$GenderCorrectResp == 3, "F", alldata$GenderCorrectResp)
  
  #insert new columns
  alldata$Resp<-alldata$RT<-alldata$CorrectResp<-alldata$Acc<-NA
  
  #code responses into one column
  alldata$Resp<-ifelse(alldata$Task=="Eyes",alldata$GazeSlide.RESP,alldata$Resp)
  alldata$Resp<-ifelse(alldata$Task=="GenderID",alldata$GenderSlide.RESP,alldata$Resp)
  
  #code RT's into one column
  alldata$RT<-ifelse(alldata$Task=="Eyes",alldata$GazeSlide.RT,alldata$RT)
  alldata$RT<-ifelse(alldata$Task=="GenderID",alldata$GenderSlide.RT,alldata$RT)
  
  #code correct responses into one column (gender only)
  alldata$CorrectResp<-ifelse(alldata$Task=="GenderID",alldata$GenderCorrectResp,alldata$CorrectResp)
  
  #code accuracy responses into one column (gender only)
  alldata$Acc<-ifelse(alldata$Task=="GenderID",alldata$GenderAcc,alldata$GenderAcc)
  
  #remove non response trials
  alldata<-subset(alldata,!is.na(alldata$Resp))
  
  alldata<-subset(alldata,alldata$RT>=200&alldata$RT<=4000)
  
  #code response boundary (for ddm; yes and female = upper (1) / no and male = lower (2) )
  alldata$RespBound<-ifelse(alldata$Resp=="Y"|alldata$Resp=="F",1,2)
  
  alldata$Subj<-alldata$subj
  
  #code diagnostic group
  ## schizgaze1 = 1000s=HC, 2000s=BP w Psychosis, 3000-4000s = schizoaffective + schizophrenia
  ## schizgaze2 = 1000s=HC, 2000s= psychosis
  alldata$Group<-alldata$subj
  
  alldata$Group <- ifelse(alldata$Study=="schizgaze1"&(substr(alldata$Group, 1, 1) == "1"), "HC",alldata$Group)
  alldata$Group <- ifelse(alldata$Study=="schizgaze1"&(substr(alldata$Group, 1, 1) == "2"), "BPwPsychosis",alldata$Group)
  alldata$Group <- ifelse(alldata$Study=="schizgaze1"&(substr(alldata$Group, 1, 1) == "3"), "SZ",alldata$Group)
  alldata$Group <- ifelse(alldata$Study=="schizgaze1"&(substr(alldata$Group, 1, 1) == "4"), "SZ",alldata$Group)
  alldata$Group <- ifelse(alldata$Study=="schizgaze2"&(substr(alldata$Group, 1, 1) == "1"), "HC",alldata$Group)
  alldata$Group <- ifelse(alldata$Study=="schizgaze2"&(substr(alldata$Group, 1, 1) == "2"), "SZ",alldata$Group)
  
  #subset to include only relevant columns
  alldata<-dplyr::select(alldata,c(Study,Group,Subj,GazeAngle,Task,Gender,Resp,RespBound,RT,CorrectResp,Acc))
  outfile<-paste(datadir,"/schizgaze12_gaze_beh.csv",sep="")
  write.csv(alldata,outfile,row.names = FALSE)
  
  ##############################################################################
  ## PROBLEM WITH SG1 = ONLY ALLOWED RESPONSES DURING STIM PRESENTATION (<1500MS) SO RIGHT TAILS OF RT DISTRIBUTIONS FOR ALL GROUPS ARE ALL CUT OFF 
  ##############################################################################
}else{
  alldata<-read.csv(outfile)
}

#if processed CORRELATES data doesnt exist, process it now
outfile<-paste(datadir,"/schizgaze2_preproc_correlates.csv",sep="")
if(file.exists(outfile)==FALSE){
  
  # get all subjects with behavioral data
  tmp<-subset(alldata,alldata$Study=="schizgaze2")
  sg2_subjs<-data.frame(subj=unique(tmp$Subj))
  data<-paste(datadir,'schizgaze2_raw_correlates.xlsx',sep="")
  sheet_names <- excel_sheets(data)
  iteration<-1
  
  for (sheet in sheet_names) {
    
    tmp_data <- data.frame(read_excel(data, sheet = sheet))#load temp data sheet
    tmp_combine<-merge(sg2_subjs, tmp_data,all.x=TRUE,by.x = names(sg2_subjs), by.y = "subj",no.dups = T)
    
    if(iteration==1){
      sg2_correlates<-tmp_combine
    }else{

      sg2_correlates<-cbind(sg2_correlates,tmp_combine)
    }
    iteration<-iteration+1
  }
  
  #get rid of extra subject columns
  tmp_col<-grep("subj", colnames(sg2_correlates),fixed = TRUE) 
  sg2_correlates<-sg2_correlates[,-tmp_col[-1]]
  
  # Replace "NA" with NA 
  sg2_correlates[sg2_correlates == "NA"] <- NA
  
  sg2_correlates$has_beh_data<-0
  
  for(i in 1:nrow(sg2_subjs)){
    sg2_correlates$has_beh_data[which(sg2_correlates$subj==sg2_subjs$subj[i])]<-1
  }
  
  sg2_correlates<-subset(sg2_correlates,sg2_correlates$has_beh_data==1&sg2_correlates$exclude==0)
  
  colnames(sg2_correlates)<-tolower(colnames(sg2_correlates))
  
  # extract matrics principal component(exclude social cognition scale to avoid overlap w/ MSCEIT)
  tmp_data<-data.frame(sop=as.numeric(sg2_correlates$matrics_sop_agt),
                       av=as.numeric(sg2_correlates$matrics_av_agt),
                       wm=as.numeric(sg2_correlates$matrics_wm_agt),
                       verl=as.numeric(sg2_correlates$matrics_verl_agt),
                       visl=as.numeric(sg2_correlates$matrics_visl_agt),
                       reas=as.numeric(sg2_correlates$matrics_reasprsolv_agt))

  sg2_correlates$matrics<-pcamethods_bpca(data=tmp_data,npcs=1,scale="uv",center=TRUE,method="pcamethods")$PC1
  
  sg2_correlates$africanamer<-ifelse(sg2_correlates$race=="AfricanAmer",1,0)
  sg2_correlates$africanamer<-ifelse(is.na(sg2_correlates$race),NA,sg2_correlates$africanamer)
  
  sg2_correlates$nativeamer<-ifelse(sg2_correlates$race=="NativeAmer",1,0)
  sg2_correlates$nativeamer<-ifelse(is.na(sg2_correlates$race),NA,sg2_correlates$nativeamer)
  
  sg2_correlates$white<-ifelse(sg2_correlates$race=="White",1,0)
  sg2_correlates$white<-ifelse(is.na(sg2_correlates$race),NA,sg2_correlates$white)
  
  sg2_correlates$asian<-ifelse(sg2_correlates$race=="Asian",1,0)
  sg2_correlates$asian<-ifelse(is.na(sg2_correlates$race),NA,sg2_correlates$asian)
  
  sg2_correlates$other<-ifelse(sg2_correlates$race=="Other",1,0)
  sg2_correlates$other<-ifelse(is.na(sg2_correlates$race),NA,sg2_correlates$other)
  
  sg2_correlates$multi<-ifelse(sg2_correlates$race=="Multi",1,0)
  sg2_correlates$multi<-ifelse(is.na(sg2_correlates$race),NA,sg2_correlates$multi)
  
  sg2_correlates$hispanic<-ifelse(sg2_correlates$ethnicity=="Hispanic",1,0)
  sg2_correlates$hispanic<-ifelse(is.na(sg2_correlates$ethnicity),NA,sg2_correlates$hispanic)
  
  sg2_correlates$schizoaffective<-ifelse(sg2_correlates$psychosis_dx=="Schizoaffective",1,0)
  sg2_correlates$schizoaffective<-ifelse(is.na(sg2_correlates$psychosis_dx),NA,sg2_correlates$schizoaffective)
  
  sg2_correlates$schizophrenia<-ifelse(sg2_correlates$psychosis_dx=="Schizophrenia",1,0)
  sg2_correlates$schizophrenia<-ifelse(is.na(sg2_correlates$psychosis_dx),NA,sg2_correlates$schizophrenia)
  
  sg2_correlates$schizophreniform<-ifelse(sg2_correlates$psychosis_dx=="Schizophreniform",1,0)
  sg2_correlates$schizophreniform<-ifelse(is.na(sg2_correlates$psychosis_dx),NA,sg2_correlates$schizophreniform)

  # extract social cognition principal component from er40/rme/msceit data with missing values 
  
  #remove and impute er40 and rme for 2057 and 2072 for rme only who performed at chance levels
  sg2_correlates$rme_accuracy[which(sg2_correlates$subj=="2057")]<-NA
  sg2_correlates$rme_rt_word_correct[which(sg2_correlates$subj=="2057")]<-NA
  sg2_correlates$rme_rt_average_word[which(sg2_correlates$subj=="2057")]<-NA
  sg2_correlates$er40_correctresp[which(sg2_correlates$subj=="2057")]<-NA
  sg2_correlates$er40_correctrt[which(sg2_correlates$subj=="2057")]<-NA
  
  sg2_correlates$rme_accuracy[which(sg2_correlates$subj=="2072")]<-NA
  sg2_correlates$rme_rt_word_correct[which(sg2_correlates$subj=="2072")]<-NA
  sg2_correlates$rme_rt_average_word[which(sg2_correlates$subj=="2072")]<-NA

    tmp_data<-data.frame(msceit=as.numeric(sg2_correlates$msceit),
                         er40=as.numeric(sg2_correlates$er40_correctresp),
                         rme=as.numeric(sg2_correlates$rme_accuracy))
  
  sg2_correlates$soc_cog_pc<-pcamethods_bpca(data=tmp_data,npcs=1,scale="uv",center=TRUE,method="pcamethods")$PC1
  
  tmp_data<-data.frame(msceit=as.numeric(sg2_correlates$msceit),
                       er40=as.numeric(sg2_correlates$er40_correctresp),
                       rme=as.numeric(sg2_correlates$rme_accuracy),
                       er40rt=scale(as.numeric(sg2_correlates$er40_correctrt))*-1)

  sg2_correlates$soc_cog_pc_wrt<-pcamethods_bpca(data=tmp_data,npcs=1,scale="uv",center=T,method="pcamethods")$PC1
  
  write.csv(sg2_correlates,outfile,row.names = F)
  
}else{
  sg2_correlates<-read.csv(outfile)
}

```

# Model Specifications

__Naming conventions:__ Model 2 (Gaze) is the primary gaze model described in the main text and Model 3 (Gender) is the primary gender model described in the main text.

__Note on gender model parameterization:__ For the gender models in which gender was allowed to categorically modulate drift rates (Gender Model 3 [winning model from main text] and 4), we originally estimated separate drift rates for male and female faces. To recast those parameters to match the parameterization of the winning gaze model (Gaze Model 2), which is equivalent to the effects-coding approach described in the main text, we calculated drift sensitivity and drift biases in the gender condition as follows:

Drift Sensitivity (Gender)* = (Drift Rate<sup>Male</sup> - Drift Rate<sup>Female</sup>)/2
Drift Bias (Gender) = (Drift Rate<sup>Male</sup> + Drift Rate<sup>Female</sup>)/2

As such, the model evaluation steps are performed on the gender models with separate male/female drift rate parameters. It is for the analyses that we recast those parameters in the gender condition to drift sensitivity and drift bias (as shown above).

```{r id12433333422222, echo=FALSE, fig.height=5, fig.show="hold", fig.width=5, message=FALSE, warning=FALSE, cache=TRUE, dpi=500, out.width='1500px'}
#
model_table<-read.csv("/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/manuscript/figures and tables/all_models.csv")
  
model_table$Condition<-""

model_table %>%
  kbl(caption = "Specifications of All Models Tested",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  pack_rows("Self-Referential Gaze Perception", 1, 2) %>%
  pack_rows("Non-Self-Referential Gender Perception", 3, 6) %>%
  kable_styling(full_width=T)

```
  
# Initial Fits 

All models were run on a high performance computing cluster using 1500 warmup samples, 1000 postwarmup draws per chain, 10 chains, resulting in 10000 postwarmup draws in cmdstanr.

# Convergence

We assessed convergence for all models by ensuring trace plots showed well-mixed chains, Rhat values were < 1.1 for all parameters, autocorrelation was ~0 by a lag of 30, and there were no divergences. This was true of all parameters for all models tested. For brevity, we only show the convergence check outputs below for the 2 winning models.

## Model 2 (Gaze) {.tabset}

Convergence tests for gaze model described in the main text.

### ESS/Rhat

```{r id342809374873412, echo=FALSE, cache=TRUE}
##
gaze2_all_samples<-read.csv('/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_m2_psychophys/gaze/init_fit/allgroups_init_fit_samples.csv')
chain_id<-gaze2_all_samples$.chain
n_chains<-length(unique(chain_id))
n_iter<-nrow(gaze2_all_samples)/n_chains

gaze_samples<-gaze2_all_samples[,-grep("beta_pr|delta_pr|alpha_pr|ndt_pr|absent_pr|present_pr|draw|iteration|lp|chain",colnames(gaze2_all_samples))]
params<-colnames(gaze_samples)

for(i in 1:length(params)){
  tmp_data<-gaze_samples[,i]
  tmp_matrix<-matrix(tmp_data,nrow=n_iter,ncol=n_chains,byrow=FALSE)
  tmp_summary<-data.frame(
      parameter=params[i],
      mean=round(mean(tmp_matrix),2),
      sd=round(sd(tmp_matrix),2),
      rhat=round(posterior::rhat(tmp_matrix),2),
      bulk_ess=round(posterior::ess_bulk(tmp_matrix),0),
      tail_ess=round(posterior::ess_tail(tmp_matrix),0))
    if(i==1){
      fit_summary<-tmp_summary
    }else{
      fit_summary<-rbind(fit_summary,tmp_summary)
    }
}

params<-rename_gaze_pars(fit_summary$parameter)
fit_summary$parameter<-params
fit_summary

```

### Trace Plots

```{r id211123123122231231, echo=FALSE, fig.height=1.75, fig.width=3.5, message=FALSE, warning=FALSE, cache=TRUE, tidy=TRUE}

tmp_gaze_samples<-gaze_samples[,-grep("^sub_", colnames(gaze_samples)) ]
params<-rename_gaze_pars(colnames(tmp_gaze_samples))
bayesplot_theme_set(theme_default(base_size = 8, base_family = "sans"))

for (i in 1:ncol(tmp_gaze_samples)){
  tmp_data<-tmp_gaze_samples[,i]
  tmp_matrix<-matrix(tmp_data,nrow=n_iter,ncol=n_chains,byrow=FALSE) #make matrix (draws*chains)
  post_draws<-simplify2array(list(tmp_matrix))
  dimnames(post_draws)<-list(Iteration=seq(from=1,to=n_iter,by=1),
                             Chain=seq(from=1,to=n_chains,by=1),
                             Parameter=params[i])
  title<-paste("Model 2 (Gaze):\n", params[i], sep="")
  p<-mcmc_trace(post_draws,pars=params[i])+legend_none()+ggtitle(title)+ ylab("Estimate")
  print(p)
}

```

### Autocorrelation

Note: We originally examined all 10 chains for autocorrelation. But here we randomly select 6 and display those (for brevity)

```{r id22312312312212, echo=FALSE, fig.height=2.5, fig.width=3.5, message=FALSE, warning=FALSE, cache=TRUE, tidy=TRUE}

chain_nums<-seq(from=1,to=n_chains,by=1)
subsample_chains<-6 #chains to subsample and display autocorr for
bayesplot_theme_set(theme_default(base_size = 8, base_family = "sans"))

for (i in 1:ncol(tmp_gaze_samples)){
  tmp_data<-tmp_gaze_samples[,i]
  tmp_matrix<-matrix(tmp_data,nrow=n_iter,ncol=n_chains,byrow=FALSE) #make this back into matrix (#draws * #chains)
  rand_chains<-sample(seq(from=1,to=n_chains,by=1),subsample_chains,replace=FALSE) #randomly sample some chains
  tmp_matrix<-tmp_matrix[,rand_chains] #get only data for those chains
  post_draws<-simplify2array(list(tmp_matrix))
  dimnames(post_draws)<-list(Iteration=seq(from=1,to=n_iter,by=1),
                             Chain=rand_chains,
                             Parameter=params[i])
  title<-paste("Model 2 (Gaze): Autocorrelation\n", params[i], sep="")
  p<-mcmc_acf(post_draws[,,1], lags=30)+ggtitle(title)
  print(p)
}

rm(list=c('tmp_gaze_samples'))
invisible(gc())

```

## Model 3 (Gender) {.tabset}

Convergence tests for gender model described in the main text.

### ESS/Rhat

```{r id342, echo=FALSE, cache=TRUE}
##
gender3_all_samples<-read.csv('/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_m3_psychophys/gender/init_fit/allgroups_init_fit_samples.csv')
chain_id<-gender3_all_samples$.chain
n_chains<-length(unique(chain_id))
n_iter<-nrow(gender3_all_samples)/n_chains

gender_samples<-gender3_all_samples[,-grep("beta_pr|delta_pr|alpha_pr|ndt_pr|absent_pr|present_pr|draw|iteration|lp|chain",colnames(gender3_all_samples))]
params<-colnames(gender_samples)

for(i in 1:length(params)){
  tmp_data<-gender_samples[,i]
  tmp_matrix<-matrix(tmp_data,nrow=n_iter,ncol=n_chains,byrow=FALSE)
  tmp_summary<-data.frame(
      parameter=params[i],
      mean=round(mean(tmp_matrix),2),
      sd=round(sd(tmp_matrix),2),
      rhat=round(posterior::rhat(tmp_matrix),2),
      bulk_ess=round(posterior::ess_bulk(tmp_matrix),0),
      tail_ess=round(posterior::ess_tail(tmp_matrix),0))
    if(i==1){
      fit_summary<-tmp_summary
    }else{
      fit_summary<-rbind(fit_summary,tmp_summary)
    }
}

params<-rename_gender_pars(fit_summary$parameter)
fit_summary$parameter<-params
fit_summary

```

### Trace Plots

```{r id2231, echo=FALSE, fig.height=1.75, fig.width=3.5, message=FALSE, warning=FALSE, cache=TRUE, tidy=TRUE}

tmp_gender_samples<-gender_samples[,-grep("^sub_", colnames(gender_samples)) ]
params<-rename_gender_pars(colnames(tmp_gender_samples))

bayesplot_theme_set(theme_default(base_size = 8, base_family = "sans"))
for (i in 1:ncol(tmp_gender_samples)){
  tmp_data<-tmp_gender_samples[,i]
  tmp_matrix<-matrix(tmp_data,nrow=n_iter,ncol=n_chains,byrow=FALSE) #make matrix (draws*chains)
  post_draws<-simplify2array(list(tmp_matrix))
  dimnames(post_draws)<-list(Iteration=seq(from=1,to=n_iter,by=1),
                             Chain=seq(from=1,to=n_chains,by=1),
                             Parameter=params[i])
  title<-paste("Model 3 (Gender):\n", params[i], sep="")
  p<-mcmc_trace(post_draws,pars=params[i])+legend_none()+ggtitle(title)+ ylab("Estimate")
  print(p)
}

```

### Autocorrelation

Note: We originally examined all 10 chains for autocorrelation. But here we randomly select 6 and display those (for brevity)

```{r id22311212, echo=FALSE, fig.height=2.5, fig.width=3, message=FALSE, warning=FALSE, cache=TRUE, tidy=TRUE}

chain_nums<-seq(from=1,to=n_chains,by=1)
subsample_chains<-6 #of of chains to subsample and display autocorr for
bayesplot_theme_set(theme_default(base_size = 8, base_family = "sans"))

for (i in 1:ncol(tmp_gender_samples)){
  tmp_data<-tmp_gender_samples[,i]
  tmp_matrix<-matrix(tmp_data,nrow=n_iter,ncol=n_chains,byrow=FALSE) #make matrix (#draws * #chains)
  rand_chains<-sample(seq(from=1,to=n_chains,by=1),subsample_chains,replace=FALSE) #randomly sample some chains
  tmp_matrix<-tmp_matrix[,rand_chains] #get only data for those chains
  post_draws<-simplify2array(list(tmp_matrix))
  dimnames(post_draws)<-list(Iteration=seq(from=1,to=n_iter,by=1),
                             Chain=rand_chains,
                             Parameter=params[i])
  title<-paste("Model 3 (Gender): Autocorrelation\n", params[i], sep="")
  p<-mcmc_acf(post_draws[,,1], lags=30)+ggtitle(title)
  print(p)
}

rm(list=c('tmp_gender_samples'))
invisible(gc())

```
 
# Model Comparison

## Gaze Models

For the self-referential gaze perception condition, the winning model ("Model 2: Gaze") assumed that gaze cues continuously modulated evidence accumulation. This allowed the model to capture two aspects of the evidence accumulation process. Drift sensitivity indexed how evidence accumulation was continuously modulated by physical changes in gaze signals. Those with higher values are more attuned to gaze signals, as they better translate stronger gaze cues into more efficient decision-making. Drift bias captured perceptual bias for direct/indirect gaze, regardless of physical changes in gaze cues. As “looking at me” and “not looking at me” responses were coded as the top and bottom boundaries, respectively, drift bias > 0 indicates a perceptual bias for direct gaze, and drift bias < 0 indicates a perceptual bias for indirect gaze. The DDM assumes that the remaining processes—threshold separation (response caution), start point (expectancy bias), and NDT (motor/sensory processes)—are set before the stimulus appears. This means that they can only vary based on stimulus features that can be anticipated before the stimulus onset. As such, threshold separation, start point, and NDT operated as trait-level processes that did not vary based on changes in gaze, which varied trial-by-trial. 
 
```{r id3924321387324, echo=FALSE, cache=TRUE}

outpath <- '/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_'
models<-c('m1_psychophys','m2_psychophys')
groupnames<-c('HC','SZ')
loo_headers<-c('group','model','elpd_loo','delta_elpd','delta_elpd_se')
all_loo<-data.frame(matrix(data=NA,nrow=length(models),ncol=length(loo_headers)))
colnames(all_loo)<-loo_headers

for(h in 1:length(groupnames)){
  group<-h
  groupname<-groupnames[h]
  all_loo_objects<-list() #list to hold loo_psis objects for each model
  
  for (i in 1:length(models)){
    outfile<-paste0(outpath,models[i],"/gaze/log_lik/loglik_group",group,".RData")
    load(outfile)
    all_loo$group<-groupname
    all_loo$model[i]<-models[i]
    all_loo_objects[[models[i]]]<-loo_2
    all_loo$elpd_loo[i]<-round(data.frame(loo_2[["loo"]][["estimates"]])[1,1],2)
  }
  
  #sort table by from lowest to highest loo val
  all_loo <- all_loo[order(all_loo$elpd_loo,decreasing=TRUE),]
  
  #calculate elpd loo diff 
  for (i in 1:length(models)){
    tmp_model<-all_loo$model[i] #current model
    tmp_loo_object<-all_loo_objects[[tmp_model]][["loo"]]
    best_model<-all_loo$model[1] #best fitting model
    best_loo_object<-all_loo_objects[[best_model]][["loo"]]
    if (i > 1){
      compare<-data.frame(loo_compare(tmp_loo_object,best_loo_object))
      all_loo$delta_elpd[i]<-round(compare$elpd_diff[2],2)
      all_loo$delta_elpd_se[i]<-round(compare$se_diff[2],2)
      
    }else{
      all_loo$delta_elpd[i]<-"--"
      all_loo$delta_elpd_se[i]<-"--"
    }
  }
  if(h==1){
    all_loo_combined<-all_loo
  }else{
    all_loo_combined<-rbind(all_loo_combined,all_loo)
  }
}

rownames(all_loo_combined)<-NULL

all_loo_combined$model<-gsub("_psychophys","",all_loo_combined$model)
all_loo_combined$model<-gsub("m","Model ",all_loo_combined$model)
all_loo_combined$model<-paste0(all_loo_combined$model,": Gaze")

all_loo_combined %>%
  kbl(caption = "Gaze Model Comparison",valign = "t") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  kable_styling(full_width=T) %>%
  footnote("Models are presented in order from best- to worst-fitting based on the expected log pointwise predictive density within each group ('elpd_loo'). Higher ELPD indicates better model fit. Differences in out-of-sample predictive accuracy were assessed relative to the best fitting model based on changes in the ELPD ('delta_elpd'). Uncertainty around ∆ELPD-LOO is captured through the standard error ('delta_elpd_se') of the estimated pointwise ELPD differences. An absolute change in ELPD < 1 SE is weak evidence for improved predictive accuracy.")

```
 
## Gender Models

For the non-self-referential gender perception condition, the winning model (“Model 3: Gender”) was one in which evidence accumulation (drift rate) was influenced by the gender of stimuli, but not other task-irrelevant social cues (i.e., gaze). The effect of gender on evidence accumulation was captured in the original models via separate drift rates for male and female stimuli (drift rate<sup>gend-M</sup> and drift rate<sup>gend-F</sup>). As “male” responses were coded as the top boundary and “female” responses were coded as the bottom boundary, higher values for drift rate<sup>gend-M</sup> and lower values for drift rate<sup>gend-F</sup> reflected more efficient evidence accumulation. 

We then recast these parameters into the same parameterization as the gaze perception model described in the main text, even though gender was manipulated categorically. To achieve this, when calculating trial-level drift rates in the gender condition using the equation in Figure 1 (main text), we replaced the gaze signal on each trial i with the effects-coded gender of the stimulus on each trial (1=male, -1=female). As a result, during gender perception, drift sensitivity captured the categorical modulation of drift rates by gender, where larger values indicated greater discriminability between male and female faces and, therefore, more sensitivity. Although drift sensitivity parameters were highly correlated across gaze and gender conditions (r=0.68; Supplement 8.3), they capture slightly different processes: in the gender condition, they capture evidence accumulation rates and sensitivity to changes in gender cues; in the gaze condition, they are a purer measure of sensitivity to changes in gaze cues. Additionally, during gender perception, the drift bias reflected a perceptual bias for male/female. As “male” and “female” responses were coded as the top and bottom boundaries, respectively, drift bias > 0 indicates a perceptual bias for male, and drift bias < 0 indicates a perceptual bias for female. All other parameters were the same as in the gaze model. 

This 'effects coding approach' is equivalent to the following calculations of drift sensitivity and drift bias for the gender condition, which is how we calculated these parameters in our case:

Drift Sensitivity (Gender)* = (Drift Rate<sup>Male</sup> - Drift Rate<sup>Female</sup>)/2
Drift Bias (Gender) = (Drift Rate<sup>Male</sup> + Drift Rate<sup>Female</sup>)/2
 
```{r id392422321387324, echo=FALSE, cache=TRUE}

outpath <- '/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_'
models<-c('m1_psychophys','m2_psychophys','m3_psychophys','m4_psychophys')
groupnames<-c('HC','SZ')
loo_headers<-c('group','model','elpd_loo','delta_elpd','delta_elpd_se')
all_loo<-data.frame(matrix(data=NA,nrow=length(models),ncol=length(loo_headers)))
colnames(all_loo)<-loo_headers

for(h in 1:length(groupnames)){
  group<-h
  groupname<-groupnames[h]
  
  all_loo_objects<-list() #list to hold loo_psis objects for each model
  
  for (i in 1:length(models)){
    
    outfile<-paste0(outpath,models[i],"/gender/log_lik/loglik_group",group,".RData")
    
    load(outfile)
    all_loo$group<-groupname
    all_loo$model[i]<-models[i]
    all_loo_objects[[models[i]]]<-loo_2
    all_loo$elpd_loo[i]<-round(data.frame(loo_2[["loo"]][["estimates"]])[1,1],2)
    
  }
  
  #sort table by from lowest to highest loo val
  all_loo <- all_loo[order(all_loo$elpd_loo,decreasing=TRUE),]
  
  #calculate elpd loo diff 
  for (i in 1:length(models)){
    
    tmp_model<-all_loo$model[i] #current model
    tmp_loo_object<-all_loo_objects[[tmp_model]][["loo"]]
    
    best_model<-all_loo$model[1] #best fitting model
    best_loo_object<-all_loo_objects[[best_model]][["loo"]]
    
    if (i > 1){
      compare<-data.frame(loo_compare(tmp_loo_object,best_loo_object))
      all_loo$delta_elpd[i]<-round(compare$elpd_diff[2],2)
      all_loo$delta_elpd_se[i]<-round(compare$se_diff[2],2)
      
    }else{
      all_loo$delta_elpd[i]<-"--"
      all_loo$delta_elpd_se[i]<-"--"
    }
  }
  
  if(h==1){
    all_loo_combined<-all_loo
  }else{
    all_loo_combined<-rbind(all_loo_combined,all_loo)
  }

}

rownames(all_loo_combined)<-NULL
all_loo_combined$model<-gsub("_psychophys","",all_loo_combined$model)
all_loo_combined$model<-gsub("m","Model ",all_loo_combined$model)
all_loo_combined$model<-paste0(all_loo_combined$model,": Gender")

all_loo_combined %>%
  kbl(caption = "Gender Model Comparison",valign = "t") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  kable_styling(full_width=T) %>%
  footnote("Models are presented in order from best- to worst-fitting based on the expected log pointwise predictive density within each group ('elpd_loo'). Higher ELPD indicates better model fit. Differences in out-of-sample predictive accuracy were assessed relative to the best fitting model based on changes in the ELPD ('delta_elpd'). Uncertainty around ∆ELPD-LOO is captured through the standard error ('delta_elpd_se') of the estimated pointwise ELPD differences. An absolute change in ELPD < 1 SE is weak evidence for improved predictive accuracy.")

```

## Intercorrelations Between Winning Model Parameters {.tabset}

### Full Sample

```{r id9249, echo=FALSE, fig.height=6, fig.width=6.5, message=FALSE, warning=FALSE, cache=TRUE}

all_sub_data<-read.csv("/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/data/schizgaze2_all_sub_params_w_correlates.csv")

tmp_pars<-c('delta_bias','delta_bias_gend',
            'delta_b1','delta_gend',
            'beta','beta_gend',
            'alpha','alpha_gend')

tmp_corr_data<-all_sub_data[tmp_pars]
colnames(tmp_corr_data)<-rename_corr_pars(colnames(tmp_corr_data))
p<-bayes_corr_matrix(tmp_corr_data,credMass=.95)
p[["plot"]]

```

### SZ Only

```{r id92491, echo=FALSE, fig.height=6, fig.width=6.5, message=FALSE, warning=FALSE, cache=TRUE}

temp<-subset(all_sub_data,all_sub_data$group=="SZ")
tmp_pars<-c('delta_bias','delta_bias_gend',
            'delta_b1','delta_gend',
            'beta','beta_gend',
            'alpha','alpha_gend')

tmp_corr_data<-all_sub_data[tmp_pars]
colnames(tmp_corr_data)<-rename_corr_pars(colnames(tmp_corr_data))
p<-bayes_corr_matrix(tmp_corr_data,credMass=.95)
p[["plot"]]

```

### HC Only

```{r id92491221, echo=FALSE, fig.height=6, fig.width=6.5, message=FALSE, warning=FALSE, cache=TRUE}

temp<-subset(all_sub_data,all_sub_data$group=="HC")
tmp_pars<-c('delta_bias','delta_bias_gend',
            'delta_b1','delta_gend',
            'beta','beta_gend',
            'alpha','alpha_gend')

tmp_corr_data<-all_sub_data[tmp_pars]
colnames(tmp_corr_data)<-rename_corr_pars(colnames(tmp_corr_data))
p<-bayes_corr_matrix(tmp_corr_data,credMass=.95)
p[["plot"]]

```

# Parameter Recovery

Parameter recovery was performed for both winning models to determine if parameter values used to simulate choice and RT data could be accurately recovered.

__Procedure:__ For each model, for each group, for each of N = 50 simulations, data of n = 35 subjects were simulated by sampling parameters from distributions matching the means and standard deviations (SD) of the group-level posteriors for each model parameter. This was done using 35 subjects per group because it was slightly less than the number of subjects we had per group, operating under the assumption that if we could recover the parameters in fewer subjects, then we definitely could in more. Sampled parameters were then used to simulate n = 108 trials of choice and RT data for each subject (the number of trials the task had per condition; with 12 trials per signal strength), using ‘rwiener’ in RWiener R package. These data were then fitted to the winning model to extract simulated parameter values. Correspondence between the generating parameter values and the simulated parameter values was compared by determining the percentage of group-level and subject-level simulated parameters whose 95% highest density interval (HDI) contained the original generating value. Note: NDT was fixed at 0.2 for parameter recovery analyses and scaled the SD of the sampled distribution down slightly to avoid getting extreme, implausible values in our simulated data.

__Results:__ Both models showed good posterior coverage for parameters across SZ and HC at the group- (88-100% recovery) and subject-level (93-95% recovery). Recovery of individual differences in parameters at the subject-level (i.e., Pearson R between generating and recovered values) was slightly lower for drift sensitivity<sup>gaze</sup> (r’s =0.62-0.73) than drift rate(s)<sup>gend-M/F</sup> (r’s =0.68-0.79) and other parameters (r’s=0.77-0.92), but was still considered acceptable.

## Model 2 (Gaze)

### Group-Level Parameters (Averaged over simulations)

First, we examined how well the group-level generating values were recovered by simulated models fits *on average*. We averaged over the posterior means and 95% HDIs for group-level parameters for all 50 simulations; plotted generating group-level values (red X's in plots below) superimposed over the averaged mean and 95% HDI (points and error bars in figure below); and assessed how well, on average, the group-level generating values were captured by simulated model fits. 

Results below suggest that group level parameters were well-recovered on average because 1) the generating values (red X's in plots below) were contained within the average 95% HDI for all parameters (error bars in plots below), and 2) the generating values are closely aligned with the group-level posterior mean (data points in plots below) of simulated fits on average for all parameters.

```{r id189724310911, echo=FALSE, message=FALSE, warning=FALSE,fig.show="hold",fig.height=3, fig.width=3.25,cache=TRUE}

group_summary<-read.csv("/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_m2_psychophys/gaze/par_recover/parRecovery_groupLvl_summary.csv")

# average over recovered posterior means and HDI's of all simulations (within group)
plot_data<-group_summary %>%
  summarise_all(mean)

#ALPHA
plot_data2<-data.frame(group=c('HC','SZ'),
                       sim_mean=c(plot_data$alpha_grp_sim_mean1,plot_data$alpha_grp_sim_mean2),
                       gen_mean=c(plot_data$alpha_grp_gen1,plot_data$alpha_grp_gen2),
                       sim_hdi_lo=c(plot_data$alpha_grp_sim_hdi_lo1,plot_data$alpha_grp_sim_hdi_lo2),
                       sim_hdi_hi=c(plot_data$alpha_grp_sim_hdi_hi1,plot_data$alpha_grp_sim_hdi_hi2))

p <- ggplot(data = plot_data2, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data2, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data2, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Gaze: Threshold Separation\n(Averaged over 50 sims)") + 
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data2$group, value = plot_data2$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)

#BETA
plot_data2<-data.frame(group=c('HC','SZ'),
                       sim_mean=c(plot_data$beta_grp_sim_mean1,plot_data$beta_grp_sim_mean2),
                       gen_mean=c(plot_data$beta_grp_gen1,plot_data$beta_grp_gen2),
                       sim_hdi_lo=c(plot_data$beta_grp_sim_hdi_lo1,plot_data$beta_grp_sim_hdi_lo2),
                       sim_hdi_hi=c(plot_data$beta_grp_sim_hdi_hi1,plot_data$beta_grp_sim_hdi_hi2))


p <- ggplot(data = plot_data2, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data2, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data2, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Gaze: Start Point\n(Averaged over 50 sims)") + 
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data2$group, value = plot_data2$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)

#DELTA 1.1
plot_data2<-data.frame(group=c('HC','SZ'),
                       sim_mean=c(plot_data$delta_grp_sim_mean1,plot_data$delta_grp_sim_mean2),
                       gen_mean=c(plot_data$delta_grp_gen1,plot_data$delta_grp_gen2),
                       sim_hdi_lo=c(plot_data$delta_grp_sim_hdi_lo1,plot_data$delta_grp_sim_hdi_lo2),
                       sim_hdi_hi=c(plot_data$delta_grp_sim_hdi_hi1,plot_data$delta_grp_sim_hdi_hi2))


p <- ggplot(data = plot_data2, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data2, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data2, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Gaze: Drift Bias\n(Averaged over 50 sims)") + 
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data2$group, value = plot_data2$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)

#B1 (Drift Sensitivity Parameter)
plot_data2<-data.frame(group=c('HC','SZ'),
                       sim_mean=c(plot_data$b1_grp_sim_mean1,plot_data$b1_grp_sim_mean2),
                       gen_mean=c(plot_data$b1_grp_gen1,plot_data$b1_grp_gen2),
                       sim_hdi_lo=c(plot_data$b1_grp_sim_hdi_lo1,plot_data$b1_grp_sim_hdi_lo2),
                       sim_hdi_hi=c(plot_data$b1_grp_sim_hdi_hi1,plot_data$b1_grp_sim_hdi_hi2))

p <- ggplot(data = plot_data2, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data2, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data2, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Gaze: Drift Sensitivity\n(Averaged over 50 sims)") + 
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data2$group, value = plot_data2$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)


```

### Group-Level Parameters (Separate for each simulation)

Second, we examined group-level parameters for the individual simulations to determine what % of the 50 simulations successfully recovered the group-level generating parameter values (vertical black line in plots below). Recovery was "successful" if the 95% HDI of the fitted group-level posterior (error bars in plots below) for a given simulation contained the original subject-level generating value. Data in green in the plots below indicate the group-level parameters were recovered and data in red indicates they were not. This was done separately for each task condition.

Results below suggest that group level generating parameters were well---recovered because--across all  task conditions---the generating values (vertical black line in plots below) were contained within the average 95% HDI (error bars in plots below) in 88-100% of simulations for all parameters.

```{r id122189371829311, echo=FALSE, fig.height=5.5, fig.width=2.5, message=FALSE, warning=FALSE,fig.show="hold",cache=TRUE}

#define function to summarize group-level par recovery for separate simulations
## data = dataframe containing summary (one row per simulation) of parameter recovery at the group level. must include group level gen values for each param, simulated posterior mean for each param, the 95% HDI of the simulated posterior for each param, and a column indicating whether the generating value was recovered (1=yes, 0=no)
## par = parameter name in the posterior samples
## parname = how you would like the parameter name to be displayed on resulting plot
## genMean_suffix = suffix of generating mean in data
## simMean_suffix= suffix of simulated mean in data
## simHDI_suffix =  suffix of simulated HDI in data (excluding lo and hi)
## recover_suffix =  suffix in data df that indexing column of data indicating whether par was recovered or not

params<-c('alpha','beta','delta','b1')
parnames<-c('Gaze: Threshold Separation','Gaze: Start Point','Gaze: Drift Bias','Gaze: Drift Sensitivity')
groups<-c('HC','SZ')

group_summary$sim_id<-group_summary$sim

for (i in 1:length(groups)){
  for (j in 1:length(params)){
    temp_summary<-group_summary[,grep(paste0("sim|run|", i, "$"),colnames(group_summary))]
    colnames(temp_summary) <- gsub(paste0(i, "$"), "", colnames(temp_summary))
    temp_parname<-paste0(parnames[j]," (",groups[i],")")
    p<-plot_parrecover_group_bysim(data=temp_summary,
                                   par=params[j],
                                   parname=temp_parname,
                                   sim_id<-'sim_id',
                                   genMean_suffix<-'_grp_gen',
                                   simMean_suffix<-'_grp_sim_mean',
                                   simHDI_suffix<-'_grp_sim_hdi_',
                                   recover_suffix<-'_recover')
    print(p)
  }
}

```

### Subject-Level Parameters

Third, we examined how well subject-level generating values were captured by simulated model fits. We determined what % of subject-level generating parameter values were successfully recovered across the 50 simulations. Recovery was "successful" if the 95% HDI of the fitted subject-level posterior contained the original subject-level generating value. Data in green in the plots below indicate the subject-level generating values were recovered and data in red indicates they were not. This was done separately for each  task condition.

Results below suggest that subject level generating parameters were well-recovered because the generating values (x-axis) were contained within the average 95% HDI of fitted values (error bars in plots below) in 93-95% of simulated participants.

```{r id1313489273421, echo=FALSE, message=FALSE, warning=FALSE, tidy=TRUE,fig.show="hold",cache=TRUE,fig.height=3.5,fig.width=3.35}

subj_summary<-read.csv("/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_m2_psychophys/gaze/par_recover/parRecovery_subjLvl_summary.csv")

#define function to summarize subject-level par recovery for separate simulations
## data = dataframe containing summary (one row per subject per simulation) of parameter recovery at the SUBJECT level. must include subj level gen values for each param, simulated subj posterior mean for each param, the subj-level  95% HDI of the simulated posterior for each param, and a column indicating whether the generating value was recovered (1=yes, 0=no) for each subj for each sim.
## par = parameter name in the posterior samples
## parname = how you would like the parameter name to be displayed on resulting plot
## genMean_suffix = suffix of generating mean in data
## simMean_suffix= suffix of simulated mean in data
## simHDI_suffix =  suffix of simulated HDI in data (excluding lo and hi)
## recover_suffix =  suffix in data df that indexing column of data indicating whether par was recovered or not

params<-c('alpha','beta','delta','b1')
parnames<-c('Gaze: Threshold Separation','Gaze: Start Point','Gaze: Drift Bias','Gaze: Drift Sensitivity')
groups<-c('HC','SZ')
subj_summary$sim_id<-subj_summary$sim

for (i in 1:length(groups)){
  for (j in 1:length(params)){
    temp_summary<-subset(subj_summary,subj_summary$group==i)
    temp_parname<-paste0(parnames[j]," (",groups[i],")")
    p<-plot_parrecover_subj(data=temp_summary,
                                   par=params[j],
                                   parname=temp_parname,
                                   sim_id<-'sim_id',
                                   genMean_suffix<-'_sub_gen',
                                   simMean_suffix<-'_sub_sim_mean',
                                   simHDI_suffix<-'_sub_sim_hdi_',
                                   recover_suffix<-'_recover')
    print(p)
  }
}

```

## Model 3 (Gender)

### Group-Level Parameters (Averaged over simulations)

Now we do the same for the gender models. First, we examined how well the group-level generating values were recovered by simulated models fits *on average*. We averaged over the posterior means and 95% HDIs for group-level parameters for all 50 simulations; plotted generating group-level values (red X's in plots below) superimposed over the averaged mean and 95% HDI (points and error bars in figure below); and assessed how well, on average, the group-level generating values were captured by simulated model fits. 

Results below suggest that group level parameters were well-recovered on average because 1) the generating values (red X's in plots below) were contained within the average 95% HDI for all parameters (error bars in plots below), and 2) the generating values are closely aligned with the group-level posterior mean (data points in plots below) of simulated fits on average for all parameters.

```{r id189724377710911, echo=FALSE, message=FALSE, warning=FALSE,fig.show="hold",fig.height=3, fig.width=3.25,cache=TRUE}

group_summary<-read.csv("/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_m3_psychophys/gender/par_recover/parRecovery_groupLvl_summary.csv")

# average over recovered posterior means and HDI's of all simulations (within group)
plot_data<-group_summary %>%
  summarise_all(mean)

#ALPHA
plot_data3<-data.frame(group=c('HC','SZ'),
                       sim_mean=c(plot_data$alpha_grp_sim_mean1,plot_data$alpha_grp_sim_mean2),
                       gen_mean=c(plot_data$alpha_grp_gen1,plot_data$alpha_grp_gen2),
                       sim_hdi_lo=c(plot_data$alpha_grp_sim_hdi_lo1,plot_data$alpha_grp_sim_hdi_lo2),
                       sim_hdi_hi=c(plot_data$alpha_grp_sim_hdi_hi1,plot_data$alpha_grp_sim_hdi_hi2))

p <- ggplot(data = plot_data3, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data3, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data3, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Gender: Threshold Sep\n(Averaged over 50 sims)") + 
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data3$group, value = plot_data3$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)

#BETA
plot_data3<-data.frame(group=c('HC','SZ'),
                       sim_mean=c(plot_data$beta_grp_sim_mean1,plot_data$beta_grp_sim_mean2),
                       gen_mean=c(plot_data$beta_grp_gen1,plot_data$beta_grp_gen2),
                       sim_hdi_lo=c(plot_data$beta_grp_sim_hdi_lo1,plot_data$beta_grp_sim_hdi_lo2),
                       sim_hdi_hi=c(plot_data$beta_grp_sim_hdi_hi1,plot_data$beta_grp_sim_hdi_hi2))


p <- ggplot(data = plot_data3, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data3, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data3, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Gender: Start Point\n(Averaged over 50 sims)") + 
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data3$group, value = plot_data3$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)

#DELTA female
plot_data3<-data.frame(group=c('HC','SZ'),
                       sim_mean=c(plot_data$delta_female_grp_sim_mean1,plot_data$delta_female_grp_sim_mean2),
                       gen_mean=c(plot_data$delta_female_grp_gen1,plot_data$delta_female_grp_gen2),
                       sim_hdi_lo=c(plot_data$delta_female_grp_sim_hdi_lo1,plot_data$delta_female_grp_sim_hdi_lo2),
                       sim_hdi_hi=c(plot_data$delta_female_grp_sim_hdi_hi1,plot_data$delta_female_grp_sim_hdi_hi2))


p <- ggplot(data = plot_data3, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data3, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data3, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Gender: Drift Rate (Female)\n(Averaged over 50 sims)") + 
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data3$group, value = plot_data3$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)

#DELTA male
plot_data3<-data.frame(group=c('HC','SZ'),
                       sim_mean=c(plot_data$delta_male_grp_sim_mean1,plot_data$delta_male_grp_sim_mean2),
                       gen_mean=c(plot_data$delta_male_grp_gen1,plot_data$delta_male_grp_gen2),
                       sim_hdi_lo=c(plot_data$delta_male_grp_sim_hdi_lo1,plot_data$delta_male_grp_sim_hdi_lo2),
                       sim_hdi_hi=c(plot_data$delta_male_grp_sim_hdi_hi1,plot_data$delta_male_grp_sim_hdi_hi2))


p <- ggplot(data = plot_data3, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data3, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data3, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Gender: Drift Rate (Male)\n(Averaged over 50 sims)") + 
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data3$group, value = plot_data3$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)

```

### Group-Level Parameters (Separate for each simulation)

Second, we examined group-level parameters for the individual simulations to determine what % of the 50 simulations successfully recovered the group-level generating parameter values (vertical black line in plots below). Recovery was "successful" if the 95% HDI of the fitted group-level posterior (error bars in plots below) for a given simulation contained the original subject-level generating value. Data in green in the plots below indicate the group-level parameters were recovered and data in red indicates they were not. This was done separately for each task condition.

Results below suggest that group level generating parameters were well---recovered because--across all  task conditions---the generating values (vertical black line in plots below) were contained within the average 95% HDI (error bars in plots below) in 86-98% of simulations for all parameters.

```{r id122133893718229311, echo=FALSE, fig.height=5.5, fig.show="hold", fig.width=2.5, message=FALSE, warning=FALSE, cache=TRUE}

params<-c('alpha','beta','delta_female','delta_male')
parnames<-c('Gender: Threshold Sep','Gender: Start Point','Gender: Drift Female','Gender: Drift Male')
groups<-c('HC','SZ')

group_summary$sim_id<-group_summary$sim

for (i in 1:length(groups)){
  for (j in 1:length(params)){
    temp_summary<-group_summary[,grep(paste0("sim|run|", i, "$"),colnames(group_summary))]
    colnames(temp_summary) <- gsub(paste0(i, "$"), "", colnames(temp_summary))
    temp_parname<-paste0(parnames[j]," (",groups[i],")")
    p<-plot_parrecover_group_bysim(data=temp_summary,
                                   par=params[j],
                                   parname=temp_parname,
                                   sim_id<-'sim_id',
                                   genMean_suffix<-'_grp_gen',
                                   simMean_suffix<-'_grp_sim_mean',
                                   simHDI_suffix<-'_grp_sim_hdi_',
                                   recover_suffix<-'_recover')
    print(p)
  }
}

```

### Subject-Level Parameters

Third, we examined how well subject-level generating values were captured by simulated model fits. We determined what % of subject-level generating parameter values were successfully recovered across the 50 simulations. Recovery was "successful" if the 95% HDI of the fitted subject-level posterior contained the original subject-level generating value. Data in green in the plots below indicate the subject-level generating values were recovered and data in red indicates they were not. This was done separately for each  task condition.

Results below suggest that subject level generating parameters were well-recovered because the generating values (x-axis) were contained within the average 95% HDI of fitted values (error bars in plots below) in 93-95% of simulated participants.

```{r id131342289273421, echo=FALSE, message=FALSE, warning=FALSE, tidy=TRUE,fig.show="hold",cache=TRUE,fig.height=3.5,fig.width=3.35}

subj_summary<-read.csv("/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_m3_psychophys/gender/par_recover/parRecovery_subjLvl_summary.csv")
params<-c('alpha','beta','delta_female','delta_male')
parnames<-c('Gender: Threshold Sep','Gender: Start Point','Gender: Drift Female','Gender: Drift Male')
groups<-c('HC','SZ')
subj_summary$sim_id<-subj_summary$sim

for (i in 1:length(groups)){
  for (j in 1:length(params)){
    temp_summary<-subset(subj_summary,subj_summary$group==i)
    temp_parname<-paste0(parnames[j]," (",groups[i],")")
    p<-plot_parrecover_subj(data=temp_summary,
                                   par=params[j],
                                   parname=temp_parname,
                                   sim_id<-'sim_id',
                                   genMean_suffix<-'_sub_gen',
                                   simMean_suffix<-'_sub_sim_mean',
                                   simHDI_suffix<-'_sub_sim_hdi_',
                                   recover_suffix<-'_recover')
    print(p)
  }
}

```

# Posterior Predictions

Posterior predictions were run using all posterior samples from the "Initial Fits" above. For these, models were fit to the real data of participants using 1500 warmup samples, 1000 postwarmup draws per chain, 10 chains, resulting in 10000 postwarmup draws. This was done separately for each model. Posterior means of all group and subject-level parameters were used to simulate Trial-level choice proportions and RT distributions for each subject were simulated using values that matched the group and subject level posterior means and SDs of all group level parameters. Note that SD was scaled down slightly to avoid extreme values. Results are specified below and generally suggested the models were capable of making accurate predictions about choice proportions and RT distributions.

## Model 2 (Gaze) 

### Quantile Predictions

For the winning gaze model, predicted RT distributions for both groups mapped closely onto observed RT distributions across all signal strengths.

```{r id9924, echo=FALSE, fig.height=3.5, fig.width=4, message=FALSE, warning=FALSE, cache=TRUE}

library(HDInterval)
library(ggplot2)

load(paste0("/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_m2_psychophys/gaze/ppc/","quantile_choice_estimates.RData"))
quantiles<-c(.1,.3,.5,.7,.9)
estim_quantiles<-estim_quantiles_yesprop[,,1:length(quantiles)]
estim_choice_props<-estim_quantiles_yesprop[,,length(quantiles)+1]
beh_data_file<-paste(datadir,"/schizgaze12_gaze_beh.csv",sep="")
beh_data<-read.csv(beh_data_file)
beh_data<-subset(beh_data,beh_data$Study=="schizgaze2")
beh_data<-subset(beh_data,beh_data$Task=="Eyes")
beh_data$Resp<-ifelse(beh_data$Resp=="Y",1,2) #recode "yes" as 1 and "no as 2
beh_data$SubjID<-match(beh_data$Subj, unique(beh_data$Subj)) #subj IDs to sequential indexes 
levels<-unique(beh_data$GazeAngle)
levels<-levels[order(levels)]
groups<-unique(beh_data$Group)
subsample<-1000 #how much to subsample posterior by (helps cut down on ggplot rendering time)

for (l in 1:length(levels)){
  for (m in 1:length(groups)){
  
    #get list of subjects in this group
    current_subj_list<-unique(subset(beh_data,beh_data$Group==groups[m])$SubjID)
    
    #create empty array: #obs x #iterations x #quantiles
    estim_pred_quantiles_over_subjs <- array(numeric(),c(length(current_subj_list),subsample,length(quantiles))) 
    estim_real_quantiles_over_subjs <- array(numeric(),c(length(current_subj_list),length(quantiles))) 
    
    for (n in 1:length(current_subj_list)){

      #get real quantiles for this subj
      tmp_realdata<-subset(beh_data,beh_data$SubjID==current_subj_list[n]&beh_data$GazeAngle==levels[l])
      estim_real_quantiles_over_subjs[n,]<-quantile(tmp_realdata$RT,type=8)
      
      #get predicted quantiles (means) and HDI's
      
      ## get cols from n_obs that correspond to current cond, subj
      tmp_quant_cols<-which(beh_data$SubjID==current_subj_list[n]&beh_data$GazeAngle==levels[l])
      
      #use those to select observations from the predicted quantiles array
      tmp_quantile_est<-estim_quantiles[tmp_quant_cols,,]
      
      #average estimates over trials for this subject (gives matrix = #subsamples*#quantiles)
      if(length(tmp_quant_cols)==0){ #if no trials for current condition for current subject, create array of NA
        estim_pred_quantiles_over_subjs[n,,]<-matrix(NA,nrow=subsample,ncol=length(quantiles))
      }else if(length(tmp_quant_cols)==1){
        estim_pred_quantiles_over_subjs[n,,]<-tmp_quantile_est
      }else{ # if subj has 2 or more trials for this cond
        estim_pred_quantiles_over_subjs[n,,]<-apply(tmp_quantile_est, c(2,3), mean,na.rm=TRUE) 
      }
    }
    
    for (o in 1:length(quantiles)){
      
      tmp_hdi<-HDInterval::hdi(as.vector(estim_pred_quantiles_over_subjs[,,o]),credMass=0.95)
      tmp_hdi<-data.frame(quantile=quantiles[o],
                          real_quantile_mean=mean(estim_real_quantiles_over_subjs[,o],na.rm = TRUE),
                          pred_quantile_mean=mean(estim_pred_quantiles_over_subjs[,,o],na.rm = TRUE)*1000,
                          hdi_lo=tmp_hdi[1]*1000,
                          hdi_hi=tmp_hdi[2]*1000)
      if(o==1){
        all_hdi<-tmp_hdi
      }else{
        all_hdi<-rbind(all_hdi,tmp_hdi)
      }
    }
    
    #setup data in long form
    quantile_data<-data.frame(
      type=c(rep("real",length(quantiles)),rep("pred",length(quantiles))), # type of data (long form)
      quantile=c(quantiles,quantiles), # quantiles
      rt=c(all_hdi$real_quantile_mean,all_hdi$pred_quantile_mean), #type of quantile data
      hdi_lo=c(all_hdi$real_quantile_mean,all_hdi$hdi_lo),
      hdi_hi=c(all_hdi$real_quantile_mean, all_hdi$hdi_hi))
    
    quantile_data$group<-groups[m]
    
    if(m==1){#if group 1
      grp_quantile_data<-quantile_data
      grp1_trials<-dim(subset(beh_data,beh_data$Group==groups[m]))[1]
    }else if(m==2){#if group 2
      tmp_grp_quantile_data<-quantile_data
      tmp_grp_quantile_data$quantile<-tmp_grp_quantile_data$quantile+.03 #shift slightly to right for plotting
      grp_quantile_data<-rbind(tmp_grp_quantile_data,grp_quantile_data)
      grp2_trials<-dim(subset(beh_data,beh_data$Group==groups[m]))[1]
    }
  }
  
  grp_quantile_data$group<-ifelse(grp_quantile_data$group==1,"HC","SZ")
  grp_quantile_data$group<-as.factor(grp_quantile_data$group)
  
  title<-paste("Gaze: Quantile predictions:\n","Signal strength ",levels[l]/max(levels),sep="")
  trials_text<-paste("Trials per group:\nHC=",grp1_trials," SZ=",grp2_trials,sep="")
  
  #plot quantiles (xaxis against all quantiles colored by "types", shapes vary by group)
  plot<-ggplot(grp_quantile_data, aes(x=quantile, y=rt, color=type)) +
    geom_errorbar(aes(ymin=hdi_lo,ymax=hdi_hi),width=0,alpha=.7) +
    geom_point(alpha=.7,aes(shape=group)) +
    ggtitle(title) +
    ylab("RT") +
    xlab("Quantile")+
    theme(plot.title = element_text(size=10),axis.text.y=element_text(size=7))+
    coord_cartesian(ylim = c(0,2500))+
    annotate(geom = "text", x = .5, y = 2450, label = trials_text, hjust = "center",size=3.5,lineheight=.9)
  
  print(plot)

}    

```
 
### Choice Predictions

Predicted choice proportions for the winning gaze model generally mapped captured patterns of choice proportions across signal strengths in both groups. The exception was that the self-referential gaze model predicted slightly less extreme choice proportions at higher and lower gaze signal strengths than the observed data (likely a by-product of shrinkage due to only 12 trials per signal strength). Despite this, the model accurately predicted the general response pattern over signal strengths, namely the characteristic sigmoid shape of psychophysical data. Note that we tested whether this could be improved by including trial-level variability in the drift rates. This did not improve the choice predictions (and introduced other issues into the data, thus, we opted for the more parsimonious model).

```{r id103123121, echo=FALSE, fig.height=6.5, fig.width=4.25, message=FALSE, warning=FALSE, cache=TRUE}

# need to flip choice probabilities because PPC calculated probability of given choice option, not a "yes" choice (which is what we want)

library(dplyr)
library(ggplot2)

for (i in 1:nrow(beh_data)){# for i in n_observations
  tmp_choice<-beh_data$Resp[i] #get current choice
  if(tmp_choice==2){ #if current choice is lower bound response, flip predicted choice prob
    estim_choice_props[i,] <- 1-(estim_choice_props[i,])
  }
}

for (l in 1:length(levels)){
  for (m in 1:length(groups)){
    
    #get list of subjects in this group
    current_subj_list<-unique(subset(beh_data,beh_data$Group==groups[m])$SubjID)
    
    #create empty array: #subjs in this group x #iterations
    estim_pred_props_over_subjs <- array(numeric(),c(length(current_subj_list),subsample)) 
    estim_real_props_over_subjs <- array(numeric(),c(length(current_subj_list),1)) 
    
    for (n in 1:length(current_subj_list)){
      
      #calculate real YES choice proportion
      tmp_realdata<-subset(beh_data,beh_data$SubjID==current_subj_list[n] & beh_data$GazeAngle==levels[l]) # grab real # of yes reponses
      total_choices<-dim(tmp_realdata)[1] #total choices made for this level for this subj
      total_yes_choice<-length(which(tmp_realdata$RespBound==1))
      estim_real_props_over_subjs[n,1]<-total_yes_choice/total_choices
      
      #calculate predicted YES choice proportion + HDIs for this condition/subj
      tmp_prop_cols<-which(beh_data$SubjID==current_subj_list[n] & beh_data$GazeAngle==levels[l]) #YES coded as 1 in data
      tmp_prop_est<-estim_choice_props[tmp_prop_cols,]
      
      #average estimates over trials for this subject (gives matrix = #subsamples*#quantiles)
      if(length(tmp_prop_cols)==0){ #if no trials for current condition for current subject, create array of NA
        estim_pred_props_over_subjs[n,]<-as.numeric(matrix(NA,nrow=1,ncol=subsample))
      }else if(length(tmp_prop_cols)==1){
        estim_pred_props_over_subjs[n,]<-tmp_prop_est
      }else{ # if subj has 2 or more trials for this cond, average over trials
        estim_pred_props_over_subjs[n,]<-apply(tmp_prop_est, c(2), mean,na.rm=TRUE) 
      }
    }
    
    estim_pred_props_over_subjs<-apply(estim_pred_props_over_subjs, 1, mean)
    
    #calculate predicted YES choice proportion + HDIs
    tmp_hdi_pred<-HDInterval::hdi(as.vector(estim_pred_props_over_subjs),credMass=0.95)
    tmp_hdi_real<-HDInterval::hdi(as.vector(estim_real_props_over_subjs),credMass=0.95)
    
    tmp_cond<-paste0(groups[m]," | ",levels[l]/max(levels))
    
    #setup data in long form (for plots)
    choiceprop_data<-data.frame(
      type=as.factor(c("real","pred")), # type of data (long form)
      mean_choiceprop=as.numeric(c(mean(estim_real_props_over_subjs),mean(estim_pred_props_over_subjs,na.rm = TRUE))),
      pred_hdi_lo=as.numeric(c(tmp_hdi_real[1],tmp_hdi_pred[1])),
      pred_hdi_hi=as.numeric(c(tmp_hdi_real[2],tmp_hdi_pred[2])))
    
    choiceprop_data$group<-as.factor(m)
    choiceprop_data$cond<-tmp_cond
    
    if(l==1&m==1){ #if first iteration
      grp_choiceprop_data<-choiceprop_data   
    }else{
      grp_choiceprop_data<-rbind(grp_choiceprop_data,choiceprop_data)
    }
  }
}

grp_choiceprop_data <- grp_choiceprop_data[order(grp_choiceprop_data$cond),]
beh_data$plot_cond<-paste0(beh_data$Group," | ",beh_data$GazeAngle/max(beh_data$GazeAngle))

plot_beh_data <- beh_data[order(beh_data$plot_cond),]
plot_beh_data$RespBound[plot_beh_data$RespBound == 2] <- 0

plot_beh_data <- plot_beh_data %>% 
  dplyr::group_by(plot_cond,SubjID) %>% 
  dplyr::summarize(Resp = mean(RespBound,na.rm=TRUE))

plot_beh_data$type<-"real"
grp_choiceprop_data$group_name<-factor(grp_choiceprop_data$group, levels = c("1", "2"), labels = c("HC", "SZ"))

title<-paste("Gaze: Choice pred (% yes):\n All signal strengths",sep="")
plot28<-ggplot(data=grp_choiceprop_data, aes(x=cond, y=mean_choiceprop,color=type)) +
  geom_point(data=grp_choiceprop_data,alpha=1,aes(shape=group_name),position=position_dodge(width = .5),size=3) + theme_bw()+
  geom_errorbar(data=grp_choiceprop_data,aes(ymin=pred_hdi_lo,ymax=pred_hdi_hi),linewidth=.8,
                width=0,alpha=1,position=position_dodge(width = .5)) +
  scale_fill_manual(values=c('#F8766D','#00BFC4'))+
  coord_cartesian(ylim = c(0,1))+
  ggtitle(title) + xlab("") +
  ylab("Choice Proportions")+
  theme(plot.title = element_text(size=10),axis.text.y = element_text(size=7),axis.text.x = element_text(size=7,angle = 90,vjust=1,hjust=1))+
  gghalves::geom_half_point(data=plot_beh_data,aes(x=plot_cond,y=Resp,color=type),
                            size=1.5,
                            side = "r", ## draw jitter on the right
                            range_scale = 0, ## control range of jitter
                            alpha = .3## add some transparency
  ) +
  coord_flip()
print(plot28)

```
 
## Model 3 (Gender) 

### Quantile Predictions

For the winning gender model, predicted RT distributions for both groups generally mapped well onto observed RT distributions for correct trials. The exception was that the non-self-referential gender model slightly underpredicted the tails of the RT distribution of correct trials (likely because performance was near ceiling and lacked sufficient variability to effectively capture these effects) and poorly predicted RT distributions of incorrect trials (as there were few incorrect trials [< 100] to base predictions on). 

```{r id9238429392, echo=FALSE, fig.height=3.5, fig.width=4, message=FALSE, warning=FALSE, cache=TRUE}
####
load(paste0("/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_m3_psychophys/gender/ppc/","quantile_choice_estimates.RData"))
quantiles<-c(.1,.3,.5,.7,.9)

estim_quantiles<-estim_quantiles_yesprop[,,1:length(quantiles)]
estim_choice_props<-estim_quantiles_yesprop[,,length(quantiles)+1]
beh_data_file<-paste(datadir,"/schizgaze12_gaze_beh.csv",sep="")
beh_data<-read.csv(beh_data_file)
beh_data<-subset(beh_data,beh_data$Study=="schizgaze2")
beh_data<-subset(beh_data,beh_data$Task=="GenderID")
beh_data$SubjID<-match(beh_data$Subj, unique(beh_data$Subj)) #subj IDs to sequential indexes 
beh_data$Resp<-ifelse(beh_data$Resp=="F",1,2) #recode female as 1 and male as 2
beh_data$Gender<-ifelse(beh_data$Gender=="F",1,2) #recode female gender of stim as 1 and male as 2
beh_data$Group<-match(beh_data$Group, unique(beh_data$Group))
beh_data$Acc<-ifelse(beh_data$Gender==beh_data$Resp,1,0)
gends<-unique(beh_data$Gender)
gend_names<-c("Female","Male")
groups<-unique(beh_data$Group)
group_names<-c("HC","SZ")
acc<-unique(beh_data$Acc)
acc_names<-c("Incorrect","Correct")
subsample<-1000 #how much to subsample posterior by (helps cut down on ggplot rendering time)

for (l in 1:length(gends)){
  for(p in 1:length(acc)){
    for (m in 1:length(groups)){
    
      #get list of subjects in this group
      current_subj_list<-unique(subset(beh_data,beh_data$Group==groups[m])$SubjID)
      
      #create empty array: #obs x #iterations x #quantiles
      estim_pred_quantiles_over_subjs <- array(numeric(),c(length(current_subj_list),subsample,length(quantiles))) 
      estim_real_quantiles_over_subjs <- array(numeric(),c(length(current_subj_list),length(quantiles))) 
      
      for (n in 1:length(current_subj_list)){
        
        #get real quantiles for this subj
        tmp_realdata<-subset(beh_data,beh_data$SubjID==current_subj_list[n]&beh_data$Gender==gends[l]&beh_data$Acc==acc[p])
        estim_real_quantiles_over_subjs[n,]<-quantile(tmp_realdata$RT,type=8)
        
        #get predicted quantiles (means) and HDI's
        
        ## get cols from n_obs that correspond to current cond, subj, acc
        tmp_quant_cols<-which(beh_data$SubjID==current_subj_list[n]&beh_data$Gender==gends[l]&beh_data$Acc==acc[p])
        
        #use those to select observations from the predicted quantiles array
        tmp_quantile_est<-estim_quantiles[tmp_quant_cols,,]
        
        #average estimates over trials for this subject (gives matrix = #subsamples*#quantiles)
        if(length(tmp_quant_cols)==0){ #if no trials for current condition for current subject, create array of NA
          estim_pred_quantiles_over_subjs[n,,]<-matrix(NA,nrow=subsample,ncol=length(quantiles))
        }else if(length(tmp_quant_cols)==1){
          estim_pred_quantiles_over_subjs[n,,]<-tmp_quantile_est
        }else{ # if subj has 2 or more trials for this cond
          estim_pred_quantiles_over_subjs[n,,]<-apply(tmp_quantile_est, c(2,3), mean,na.rm=TRUE) 
        }
      }
      
      for (o in 1:length(quantiles)){
        
        tmp_hdi<-HDInterval::hdi(as.vector(estim_pred_quantiles_over_subjs[,,o]),credMass=0.95)
        tmp_hdi<-data.frame(quantile=quantiles[o],
                            real_quantile_mean=mean(estim_real_quantiles_over_subjs[,o],na.rm = TRUE),
                            pred_quantile_mean=mean(estim_pred_quantiles_over_subjs[,,o],na.rm = TRUE)*1000,
                            hdi_lo=tmp_hdi[1]*1000,
                            hdi_hi=tmp_hdi[2]*1000)
        if(o==1){
          all_hdi<-tmp_hdi
        }else{
          all_hdi<-rbind(all_hdi,tmp_hdi)
        }
      }
      
      #setup data in long form
      quantile_data<-data.frame(
        type=c(rep("real",length(quantiles)),rep("pred",length(quantiles))), # type of data (long form)
        quantile=c(quantiles,quantiles), # quantiles
        rt=c(all_hdi$real_quantile_mean,all_hdi$pred_quantile_mean), #type of quantile data
        hdi_lo=c(all_hdi$real_quantile_mean,all_hdi$hdi_lo),
        hdi_hi=c(all_hdi$real_quantile_mean, all_hdi$hdi_hi))
      
      quantile_data$group<-groups[m]
      
      if(m==1){#if group 1
        grp_quantile_data<-quantile_data
        grp1_trials<-dim(subset(beh_data,beh_data$Group==groups[m]&beh_data$Gender==gends[l]&beh_data$Acc==acc[p]))[1]
      }else if(m==2){#if group 2
        tmp_grp_quantile_data<-quantile_data
        tmp_grp_quantile_data$quantile<-tmp_grp_quantile_data$quantile+.03 #shift slightly to right for plotting
        grp_quantile_data<-rbind(tmp_grp_quantile_data,grp_quantile_data)
        grp2_trials<-dim(subset(beh_data,beh_data$Group==groups[m]&beh_data$Gender==gends[l]&beh_data$Acc==acc[p]))[1]
      }
    }
    
    grp_quantile_data$group<-ifelse(grp_quantile_data$group==1,"HC","SZ")
    grp_quantile_data$group<-as.factor(grp_quantile_data$group)
  
    title<-paste("Gender: Quantile predictions:\n","Gender=",gend_names[l]," / Accuracy=",acc_names[p],sep="")
    trials_text<-paste("Trials per group:\nHC=",grp1_trials," SZ=",grp2_trials,sep="")
    
    #plot quantiles (xaxis against all quantiles colored by "types", shapes vary by group)
    plot<-ggplot(grp_quantile_data, aes(x=quantile, y=rt, color=type)) +
      geom_errorbar(aes(ymin=hdi_lo,ymax=hdi_hi),width=0,alpha=.7) +
      geom_point(alpha=.7,aes(shape=group)) +
      ggtitle(title) +
      ylab("RT") +
      xlab("Quantile")+
      theme(plot.title = element_text(size=10),axis.text.y=element_text(size=7))+
      coord_cartesian(ylim = c(0,2500))+
      annotate(geom = "text", x = .5, y = 2450, label = trials_text, hjust = "center",size=3.5,lineheight=.9)
    
    print(plot)
    
  }
}    

```
 
### Choice Predictions

For the winning gender model, predicted choice proportions for both models also generally mapped well onto observed choice proportions for both groups. 

```{r ra, echo=FALSE, fig.height=3.5, fig.width=4.25, message=FALSE, warning=FALSE, cache=TRUE}
#
load(paste0("/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_m3_psychophys/gender/ppc/","quantile_choice_estimates.RData"))
quantiles<-c(.1,.3,.5,.7,.9)

estim_quantiles<-estim_quantiles_yesprop[,,1:length(quantiles)]
estim_choice_props<-estim_quantiles_yesprop[,,length(quantiles)+1]
beh_data_file<-paste(datadir,"/schizgaze12_gaze_beh.csv",sep="")
beh_data<-read.csv(beh_data_file)
beh_data<-subset(beh_data,beh_data$Study=="schizgaze2")
beh_data<-subset(beh_data,beh_data$Task=="GenderID")
beh_data$SubjID<-match(beh_data$Subj, unique(beh_data$Subj)) #subj IDs to sequential indexes 
beh_data$Resp<-ifelse(beh_data$Resp=="F",1,2) #recode female as 1 and male as 2
beh_data$Gender<-ifelse(beh_data$Gender=="F",1,2) #recode female gender of stim as 1 and male as 2
beh_data$Group<-match(beh_data$Group, unique(beh_data$Group))
beh_data$Acc<-ifelse(beh_data$Gender==beh_data$Resp,1,0)
gends<-unique(beh_data$Gender)
groups<-unique(beh_data$Group)
group_names<-c("HC","SZ")
gend_names<-c("F","M")

for (i in 1:nrow(beh_data)){# for i in n_observations
  tmp_choice<-beh_data$Resp[i] #get current choice
  if(tmp_choice==2){ #if current choice is lower bound,flip predicted choice prob
    estim_choice_props[i,] <- 1-(estim_choice_props[i,])
  }
}

for (l in 1:length(gends)){
    for (m in 1:length(groups)){
      
      #get list of subjects in this group
      current_subj_list<-unique(subset(beh_data,beh_data$Group==groups[m])$SubjID)
      
      #create empty array: #subjs in this group x #iterations
      estim_pred_props_over_subjs <- array(numeric(),c(length(current_subj_list),subsample)) 
      estim_real_props_over_subjs <- array(numeric(),c(length(current_subj_list),1)) 
      
      for (n in 1:length(current_subj_list)){
  
        #calculate real FEMALE choice proportion
        tmp_realdata<-subset(beh_data,beh_data$SubjID==current_subj_list[n] & beh_data$Gender==gends[l]) # grab real # of yes reponses
        total_choices<-dim(tmp_realdata)[1] #total choices made for this level for this subj
        total_F_choice<-length(which(tmp_realdata$Resp==1))
        estim_real_props_over_subjs[n,1]<-total_F_choice/total_choices
        
        #calculate predicted YES choice proportion + HDIs for this condition/subj
        tmp_prop_cols<-which(beh_data$SubjID==current_subj_list[n] & beh_data$Gender==gends[l]) #YES coded as 1 in data
        tmp_prop_est<-estim_choice_props[tmp_prop_cols,]
        
        #average estimates over trials for this subject (gives matrix = #subsamples*#quantiles)
        if(length(tmp_prop_cols)==0){ #if no trials for current condition for current subject, create array of NA
          estim_pred_props_over_subjs[n,]<-as.numeric(matrix(NA,nrow=1,ncol=subsample))
        }else if(length(tmp_prop_cols)==1){
          estim_pred_props_over_subjs[n,]<-tmp_prop_est
        }else{ # if subj has 2 or more trials for this cond, average over trials
          estim_pred_props_over_subjs[n,]<-apply(tmp_prop_est, c(2), mean,na.rm=TRUE) 
        }
      }
      
      #calculate predicted YES choice proportion + HDIs
      tmp_hdi_pred<-HDInterval::hdi(as.vector(estim_pred_props_over_subjs),credMass=0.95)
      tmp_hdi_real<-HDInterval::hdi(as.vector(estim_real_props_over_subjs),credMass=0.95)
      tmp_cond<-paste0(groups[m]," | ",gends[l])
      
      #setup data in long form (for plots)
      choiceprop_data<-data.frame(
        type=as.factor(c("real","pred")), # type of data (long form)
        mean_choiceprop=as.numeric(c(mean(estim_real_props_over_subjs,na.rm = TRUE),mean(estim_pred_props_over_subjs,na.rm = TRUE))),
        pred_hdi_lo=as.numeric(c(tmp_hdi_real[1],tmp_hdi_pred[1])),
        pred_hdi_hi=as.numeric(c(tmp_hdi_real[2],tmp_hdi_pred[2])))
      
      choiceprop_data$group<-as.factor(m)
      choiceprop_data$cond<-tmp_cond
      
      if(l==1&m==1){ #if first iteration
        grp_choiceprop_data<-choiceprop_data   
      }else{
        grp_choiceprop_data<-rbind(grp_choiceprop_data,choiceprop_data)
      }
    }
}

grp_choiceprop_data <- grp_choiceprop_data[order(grp_choiceprop_data$cond),]
grp_choiceprop_data$cond <- gsub("1 | ","HC | ",fixed=T,grp_choiceprop_data$cond)
grp_choiceprop_data$cond <- gsub("2 | ","SZ | ",fixed=T,grp_choiceprop_data$cond)
grp_choiceprop_data$cond <- gsub("1","Female",fixed=T,grp_choiceprop_data$cond)
grp_choiceprop_data$cond <- gsub("2","Male",fixed=T,grp_choiceprop_data$cond)
beh_data$Group<-ifelse(beh_data$Group==1,"HC","SZ")
beh_data$Gender<-ifelse(beh_data$Gender==1,"Female","Male")
beh_data$plot_cond<-paste0(beh_data$Group," | ",beh_data$Gender)
plot_beh_data <- beh_data[order(beh_data$plot_cond),]
plot_beh_data$Resp[plot_beh_data$Resp == 2] <- 0

plot_beh_data <- plot_beh_data %>% 
  dplyr::group_by(plot_cond,SubjID) %>% 
  dplyr::summarize(Resp = mean(Resp,na.rm=TRUE))

plot_beh_data$type<-"real"
grp_choiceprop_data$group_name<-factor(grp_choiceprop_data$group, levels = c("1", "2"), labels = c("HC", "SZ"))

title<-paste("Gender: Choice pred (% yes):\n All genders",sep="")
plot<-ggplot(data=grp_choiceprop_data, aes(x=cond, y=mean_choiceprop,color=type)) +
  geom_point(data=grp_choiceprop_data,alpha=1,aes(shape=group_name),position=position_dodge(width = .5),size=3) + theme_bw()+
  geom_errorbar(data=grp_choiceprop_data,aes(ymin=pred_hdi_lo,ymax=pred_hdi_hi),linewidth=.8,
                width=0,alpha=1,position=position_dodge(width = .5)) +
  scale_fill_manual(values=c('#F8766D','#00BFC4'))+
  coord_cartesian(ylim = c(0,1))+
  ggtitle(title) + xlab("") +
  ylab("Choice Proportions")+
  theme(plot.title = element_text(size=10),axis.text.y = element_text(size=7),axis.text.x = element_text(size=7,angle = 90,vjust=1,hjust=1))+
  gghalves::geom_half_point(data=plot_beh_data,aes(x=plot_cond,y=Resp,color=type),
                            size=1.5,
                            side = "r", ## draw jitter on the right
                            range_scale = 0.2, ## control range of jitter
                            alpha = .2## add some transparency
  ) +
  coord_flip()
print(plot)

```
 
# [ANALYSIS SUPPLEMENT] {.unnumbered}

Note on coding of response boundaries: For modeling, testing, and evaluation -- in the gender condition -- female stimuli were modeled as upper bound responses and male as lower boundary. In the gaze condition, direct was modeled as upper boundary and indirect as lower boundary. However, in the task design, participants pressed their INDEX finger for "Yes-looking at me responses" (in the gaze condition) and for "male" (in the gender condition). They pressed their MIDDLE finger for "No-not looking at me responses" (in the gaze condition) and for "female" (in the gender condition). So, in order to be able to rule out response biases across the two conditions (in the start points, drift biases, etc), we recoded the samples from the gender condition so that upper boundary responses corresponded to choices made with the index finger (Yes, Looking at me and Male) and lower boundary responses corresponded to choices made with the index finger (No, not looking at me and female). This was done simply by multiplying subject and group-level drift rates in the gender condition by -1. For this reason, the directionality of drift rate estimates in the gender condition for the analyses below will be opposite to the directionality of those estimates shown above for model testing.

# Post-Hoc Power Analysis

__Cohen (1988) guidelines of effect sizes:__
Pearson’s r: small = 0.10, medium = 0.30, large = 0.50
Cohen’s d: small = 0.20, medium = 0.50, large = 0.80 
Incremental f²: small = 0.02, medium = 0.15, large = 0.35	

```{r id92439, echo=FALSE, fig.height=6, fig.width=6, message=FALSE, warning=FALSE, cache=TRUE}

library(pwr)

################################################################
# power analysis for group differences
################################################################

group_diff_power<-data.frame(Type=rep("Group Difference",3),
                             Effect_Size=c("Small","Medium","Large"),
                             Cohen_d=c(0.2,0.5,0.8),
                             Power_to_Detect_in_Current_Sample=c(round(pwr.t2n.test(n1 = 39, n2 = 42,  sig.level = 0.05, d=.2)$power,3)*100,
                                                       round(pwr.t2n.test(n1 = 39, n2 = 42,  sig.level = 0.05, d=.5)$power,3)*100,
                                                       round(pwr.t2n.test(n1 = 39, n2 = 42,  sig.level = 0.05, d=.8)$power,3)*100))

group_diff_power %>%
  kbl(caption = "Post-Hoc Power Analysis: Group Differences",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") 

################################################################
# power analysis for regressions (full sample)
################################################################

new_pred <- 2 # Number of added predictors 
old_pred<-4   # Number of old predictors incl. intercept
n<-81 #full sample n
v <- n - (new_pred+old_pred)  

regression_power<-data.frame(Type=rep("Full Sample Regression",3),
                             Effect_Size=c("Small","Medium","Large"),
                             Cohen_f2=c(0.02,0.15,0.35),
                             Power_to_Detect_in_Current_Sample=c(round(pwr.f2.test(u=new_pred,v=v,  sig.level = 0.05, f2=.02)$power,3)*100,
                                                       round(pwr.f2.test(u=new_pred,v=v,  sig.level = 0.05, f2=.15)$power,3)*100,
                                                       round(pwr.f2.test(u=new_pred,v=v,  sig.level = 0.05, f2=.35)$power,3)*100))


regression_power %>%
  kbl(caption = "\nPost-Hoc Power Analysis: Full Sample Regressions",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") 

```

```{r id9933219, echo=FALSE, cache=TRUE,fig.width=4,fig.height=3.75}

n_subj<-81

# read in all samples for both gaze and gender tasks
# GAZE MODEL

gaze2_all_samples<-as.data.frame(fread('/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_m2_psychophys/gaze/init_fit/allgroups_init_fit_samples.csv'))

colnames(gaze2_all_samples) <- gsub("\\[|\\]", ".", colnames(gaze2_all_samples))

for(i in 1:n_subj){
  temp_col1<-paste0("sub_delta.",i,".")
  gaze2_all_samples[[temp_col1]]<-pnorm(gaze2_all_samples[[temp_col1]])*10-5
}

#get subject level variables
sub_alpha<-colMeans(gaze2_all_samples[,grep("sub_alpha.",fixed=T,colnames(gaze2_all_samples))])
sub_beta<-colMeans(gaze2_all_samples[,grep("sub_beta.",fixed=T,colnames(gaze2_all_samples))])
sub_delta_b1<-colMeans(gaze2_all_samples[,grep("sub_b1.",fixed=T,colnames(gaze2_all_samples))])
sub_ndt<-colMeans(gaze2_all_samples[,grep("sub_ndt.",fixed=T,colnames(gaze2_all_samples))])
sub_delta_bias<-colMeans(gaze2_all_samples[,grep("sub_delta.",fixed=T,colnames(gaze2_all_samples))])

sub_samples<-data.frame(alpha=sub_alpha,
                        beta=sub_beta,
                        delta_bias=sub_delta_bias,
                        delta_b1=sub_delta_b1,
                        ndt=sub_ndt)

all_sub_data<-cbind(sg2_correlates,sub_samples)

#GENDER MODEL

#note: we have to multiply all of the delta pars by -1 so that the upper/lower boundaries 
#align with the buttons (i.e., upper bound = index finger, lower=middle)
# so that it's consistent with the gaze condition

gaze3_all_samples<-as.data.frame(fread('/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/output/hddm_m3_psychophys/gender/init_fit/allgroups_init_fit_samples.csv'))

colnames(gaze3_all_samples) <- gsub("\\[|\\]", ".", colnames(gaze3_all_samples))

#flip response boundaries for group-level variables (so it aligns with buttons in gaze condition)
gaze3_all_samples <- gaze3_all_samples %>%
  dplyr::mutate(across(starts_with("mu_delta"), ~ .x * -1))

for(i in 1:n_subj){
  # transform/flip resp boundaries for female/male to align with buttons in gaze condition
  temp_col1<-paste0("sub_delta_male.",i,".")
  gaze3_all_samples[[temp_col1]]<-(pnorm(gaze3_all_samples[[temp_col1]])*10-5)*-1
  
  temp_col2<-paste0("sub_delta_female.",i,".")
  gaze3_all_samples[[temp_col2]]<-(pnorm(gaze3_all_samples[[temp_col2]])*10-5)*-1
  
  temp_col3<-paste0("sub_delta_bias.",i,".")
  gaze3_all_samples[[temp_col3]]<-(gaze3_all_samples[[temp_col1]]+(gaze3_all_samples[[temp_col2]]))/2
  
  temp_col4<-paste0("sub_delta.",i,".")
  gaze3_all_samples[[temp_col4]]<-(gaze3_all_samples[[temp_col1]]-(gaze3_all_samples[[temp_col2]]))/2
}

sub_alpha<-colMeans(gaze3_all_samples[,grep("sub_alpha.",fixed=T,colnames(gaze3_all_samples))])
sub_beta<-colMeans(gaze3_all_samples[,grep("sub_beta.",fixed=T,colnames(gaze3_all_samples))])
sub_ndt<-colMeans(gaze3_all_samples[,grep("sub_ndt.",fixed=T,colnames(gaze3_all_samples))])
sub_delta_female<-colMeans(gaze3_all_samples[,grep("sub_delta_female.",fixed=T,colnames(gaze3_all_samples))])
sub_delta_male<-colMeans(gaze3_all_samples[,grep("sub_delta_male.",fixed=T,colnames(gaze3_all_samples))])
sub_delta_bias<-colMeans(gaze3_all_samples[,grep("sub_delta_bias.",fixed=T,colnames(gaze3_all_samples))])
sub_delta<-colMeans(gaze3_all_samples[,grep("sub_delta.",fixed=T,colnames(gaze3_all_samples))])

sub_samples_gend<-data.frame(alpha_gend=sub_alpha,
                        beta_gend=sub_beta,
                        delta_female_gend=sub_delta_female,
                        delta_male_gend=sub_delta_male,
                        delta_bias_gend=sub_delta_bias,
                        delta_gend=sub_delta,
                        ndt_gend=sub_ndt)

all_sub_data<-cbind(all_sub_data,sub_samples_gend)
all_sub_data$sz_dummy<-ifelse(all_sub_data$group=="SZ",1,0)
all_sub_data$female_dummy<-ifelse(all_sub_data$gender=="Female",1,0)

write.csv(all_sub_data,"/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/data/schizgaze2_all_sub_params_w_correlates.csv",row.names = FALSE)

all_sub_data<-read.csv("/Users/carlylasagna/University of Michigan Dropbox/Carly Lasagna/ddm_gaze_psychophys/ddm/data/schizgaze2_all_sub_params_w_correlates.csv")

#Delta_sensitivity = (delta_m – delta_f)/2 and
#delta_bias = (delta_m + delta_f)/2

# the raw parameters still have male as bottom boundary
gaze3_all_samples$mu_grp_delta_male_pr.1.<-gaze3_all_samples$mu_grp_delta_male_pr.1.*-1
gaze3_all_samples$mu_grp_delta_female_pr.1.<-gaze3_all_samples$mu_grp_delta_female_pr.1.*-1

gaze3_all_samples$mu_grp_delta_male_pr.2.<-gaze3_all_samples$mu_grp_delta_male_pr.2.*-1
gaze3_all_samples$mu_grp_delta_female_pr.2.<-gaze3_all_samples$mu_grp_delta_female_pr.2.*-1

gaze3_all_samples$mu_grp_delta_bias_pr.1.<-(gaze3_all_samples$mu_grp_delta_male_pr.1.+gaze3_all_samples$mu_grp_delta_female_pr.1.)/2
gaze3_all_samples$mu_grp_delta_bias_pr.2.<-(gaze3_all_samples$mu_grp_delta_male_pr.2.+gaze3_all_samples$mu_grp_delta_female_pr.2.)/2

gaze3_all_samples$mu_grp_delta_pr.1.<-(gaze3_all_samples$mu_grp_delta_male_pr.1.-gaze3_all_samples$mu_grp_delta_female_pr.1.)/2
gaze3_all_samples$mu_grp_delta_pr.2.<-(gaze3_all_samples$mu_grp_delta_male_pr.2.-gaze3_all_samples$mu_grp_delta_female_pr.2.)/2

gaze3_all_samples$mu_delta.1.<-pnorm(gaze3_all_samples$mu_grp_delta_pr.1.)*10-5
gaze3_all_samples$mu_delta.2.<-pnorm(gaze3_all_samples$mu_grp_delta_pr.2.)*10-5

gaze3_all_samples$mu_delta_bias.1.<-pnorm(gaze3_all_samples$mu_grp_delta_bias_pr.1.)*10-5
gaze3_all_samples$mu_delta_bias.2.<-pnorm(gaze3_all_samples$mu_grp_delta_bias_pr.2.)*10-5

```

# Demographics

Tests below are Bayesian t-tests, ANOVA, and proportion analyses run using ttestBF, anovaBF, and proportion BF in the 'BayesFactor' R package (Morey, 2015). 

```{r id1243333322222, echo=FALSE, fig.height=5, fig.show="hold", fig.width=5, message=FALSE, warning=FALSE, cache=TRUE, dpi=500, out.width='1500px'}
###
#Demographics
demo_table<-dplyr::select(all_sub_data,c('subj','group','age','female_dummy','edu','parent_edu',
                                         'white','africanamer','asian','nativeamer','multi','other','hispanic',
                                         'schizophrenia','schizoaffective','schizophreniform',
                                         'gpts_socref','gpts_persec',
                                         'antipsychotic_user',
                                         'antidepressant_user','moodstab_user',
                                         'stimulant_user','anticholinergic_user',
                                         'cpzeq',
                                         'matrics','soc_cog_pc_wrt'))

colnames(demo_table)<-c('Subj','Group','Age','Sex (% Female)','Education (Years)','Parent Education (Years)',
                                         'White (%)','African American (%)','Asian (%)',
                        'Native American (%)','Multiracial (%)','Other (%)','Hispanic (%)',
                                         'Schizophrenia (%)','Schizoaffective (%)','Schizophreniform (%)',
                                         'GPTS-Referential Thinking','GPTS-Persecution',
                                         'Antipsychotic User (%)',
                                         'Antidepressant User (%)','Mood Stabilizer User (%)',
                                         'Stimulant User (%)','Anticholinergic User (%)',
                                         'CPZeq',
                                         'General Cognition (MCCB PC)','Social Cognition (MSCEIT/RME/ER40 PC)')

demo_table_hc<-subset(demo_table,demo_table$Group=="HC")
demo_table_sz<-subset(demo_table,demo_table$Group=="SZ")

demo_table_hc_sd<-apply((demo_table_hc[,3:ncol(demo_table_hc)]),2,sd,na.rm=T)
demo_table_sz_sd<-apply((demo_table_sz[,3:ncol(demo_table_sz)]),2,sd,na.rm=T)

demo_table_hc_n<-colSums(!is.na(demo_table_hc[,3:ncol(demo_table_hc)]))
demo_table_sz_n<-colSums(!is.na(demo_table_sz[,3:ncol(demo_table_sz)]))

demo_table_hc<-colMeans(demo_table_hc[,3:ncol(demo_table_hc)],na.rm=T)
demo_table_sz<-colMeans(demo_table_sz[,3:ncol(demo_table_sz)],na.rm=T)

demo_table_all<-data.frame(cbind(demo_table_hc,demo_table_hc_sd,demo_table_hc_n,
                                 demo_table_sz,demo_table_sz_sd,demo_table_sz_n))

colnames(demo_table_all)<-c("HC (M)","HC (SD)","HC (N)",
                            "SZ (M)","SZ (SD)","SZ (N)")

demo_table_all<-round(demo_table_all,2)

demo_table_all[demo_table_all=="NaN"]<-"--"
demo_table_all[is.na(demo_table_all)]<-"--"

# remove SD's for % variables
rows_to_replace <- grep("%", rownames(demo_table_all))
cols_to_replace <- grep("SD", colnames(demo_table_all))
demo_table_all[rows_to_replace, cols_to_replace] <- "--"

rows_with_percent <- grepl("%", rownames(demo_table_all))
demo_table_all$GroupDiff<-"--"
demo_table_all$PostHoc<-"--"

for (i in 1:nrow(demo_table_all)){

  tmp_var<-rownames(demo_table_all)[i]
  
  # skip variables with data only in 1 group
  if(grepl("Hallucination|Delusion|Motivation|Expressive|Schizo|User|Other|CPZ", tmp_var)==FALSE){
    grp_var<-factor(demo_table$Group,levels=c("HC","SZ"))
    tmp_data<-demo_table[[tmp_var]]
    remove<-which(is.na(tmp_data))
    if(length(remove)>0){
      grp_var<-grp_var[-remove]
      tmp_data<-tmp_data[-remove]
    }
    
    if(rows_with_percent[i]==TRUE){#if categorical
      demo_table_all$GroupDiff[i]<-BFchisq_summary(grp_var,tmp_data,credMass=0.95)[[1]]
      demo_table_all$PostHoc[i]<-BFchisq_summary(grp_var,tmp_data,credMass=0.95)[[2]]
    }else{ #if continuous
      demo_table_all$GroupDiff[i]<-BFanova_summary(grp_var,tmp_data,credMass=0.95)[[1]]
      demo_table_all$PostHoc[i]<-BFanova_summary(grp_var,tmp_data,credMass=0.95)[[2]]
    }
  }
}

demo_table_all %>%
  kbl(caption = "Sample Characteristics",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  pack_rows("Demographic", 1, 4) %>%
  pack_rows("Race", 5, 10,hline_before = T,italic=T,bold=F) %>%
  pack_rows("Ethinicity", 11, 11,hline_before = T,italic=T,bold=F) %>%
  pack_rows("Clinical", 12, 22) %>%
  pack_rows("Diagnosis", 12, 14,hline_before = T,italic=T,bold=F) %>%
  pack_rows("Symptoms", 15, 16,hline_before = T,italic=T,bold=F) %>%
  pack_rows("Medications", 17, 22,hline_before = T,italic=T,bold=F) %>%
  pack_rows("General/Social Cognition", 23, 24) %>%
  kable_styling(full_width=T)

```
 
# Group Differences

## Model 2 (Gaze)

```{r id9933212322999, echo=FALSE, cache=TRUE,fig.width=4,fig.height=3.75}
#
# calculate and plot group differences
params<-c('mu_alpha.','mu_beta.','mu_ndt.','mu_grp_b1_pr.','mu_delta.')
raw_params<-c('mu_grp_alpha_pr.','mu_grp_beta_pr.','mu_grp_ndt_pr.','mu_grp_b1_pr.','mu_grp_delta_pr.')
sig_params<-c('sig_grp_alpha_pr.','sig_grp_beta_pr.','sig_grp_ndt_pr.','sig_grp_b1_pr.','sig_grp_delta_pr.')
param_names<-c('Threshold Separation','Start Point','NDT','Drift Sensitivity','Drift Bias')

# "data" = data frame of posterior samples. col names should be paste0(par_prefix,group_num,".") to work properly with this function

#loop through and plot/compare group differences
for(i in 1:length(params)){
  p<-plot_posterior_diffs(
    data=gaze2_all_samples, 
    ci=.95, #credible interval
    par_prefix=params[i],
    rawpar_prefix=raw_params[i],
    sig_prefix=sig_params[i],
    par_name = param_names[i],
    group_nums = c(1,2),
    group_names=c('HC','SZ'),
    sample_sizes=c(39,42),
    round_digits=3,
    showtext=TRUE)
  print(p)
}

```

## Model 3 (Gender)

```{r id99322, echo=FALSE, cache=TRUE,fig.width=4,fig.height=3.75}

params<-c('mu_alpha.','mu_beta.','mu_ndt.','mu_delta.','mu_delta_bias.')
raw_params<-c('mu_grp_alpha_pr.','mu_grp_beta_pr.','mu_grp_ndt_pr.','mu_grp_delta_pr.','mu_grp_delta_bias_pr.')
sig_params<-c('sig_grp_alpha_pr.','sig_grp_beta_pr.','sig_grp_ndt_pr.','sig_grp_delta_pr.','sig_grp_delta_pr.')
param_names<-c('Threshold Separation','Start Point','NDT','Drift Sensitivity','Drift Bias')

#loop through and plot/compare group differences
for(i in 1:length(params)){
  p<-plot_posterior_diffs(
    data=gaze3_all_samples, 
    ci=.95, #credible interval
    par_prefix=params[i],
    rawpar_prefix=raw_params[i],
    sig_prefix=sig_params[i],
    par_name = param_names[i],
    group_nums = c(1,2),
    group_names=c('HC','SZ'),
    sample_sizes=c(39,42),
    round_digits=3,
    showtext = TRUE)
  print(p)
}

```

## Plots for Figure 2

```{r id99332122322999, fig.height=3, fig.width=2, cache=TRUE, dpi=500,include=FALSE}

#(Plots for the manuscript main text)
params<-c('mu_alpha.','mu_beta.','mu_ndt.','mu_grp_b1_pr.','mu_delta.')
raw_params<-c('mu_grp_alpha_pr.','mu_grp_beta_pr.','mu_grp_ndt_pr.','mu_grp_b1_pr.','mu_grp_delta_pr.')
sig_params<-c('sig_grp_alpha_pr.','sig_grp_beta_pr.','sig_grp_ndt_pr.','sig_grp_b1_pr.','sig_grp_delta_pr.')
param_names<-c('Threshold Separation','Start Point','NDT','Drift Sensitivity','Drift Bias')

ylims<-list(c(1.6,2.4),
            c(.45,.58),
            c(.85,.95),
            c(.3,.6),
            c(-2,.5))

for(i in 1:length(params)){
  p<-plot_posterior_diffs(
    data=gaze2_all_samples, 
    ci=.95, #credible interval
    par_prefix=params[i],
    rawpar_prefix=raw_params[i],
    sig_prefix=sig_params[i],
    par_name = param_names[i],
    group_nums = c(1,2),
    group_names=c('HC','SZ'),
    sample_sizes=c(39,42),
    round_digits=3,
    showtext=F)
  print(p + coord_cartesian(ylim = ylims[[i]]))
}

params<-c('mu_alpha.','mu_beta.','mu_ndt.','mu_delta_bias.','mu_delta.')
raw_params<-c('mu_grp_alpha_pr.','mu_grp_beta_pr.','mu_grp_ndt_pr.','mu_grp_delta_bias_pr.','mu_grp_delta_pr.')
sig_params<-c('sig_grp_alpha_pr.','sig_grp_beta_pr.','sig_grp_ndt_pr.','sig_grp_delta_pr.','sig_grp_delta_pr.')
param_names<-c('Threshold Separation','Start Point','NDT','Drift Bias','Drift Sensitivity')

ylims<-list(c(1.6,2.4),
            c(.45,.58),
            c(.85,.95),
            c(-2.1,.5),c(2,3.1))

#loop through and plot/compare group differences
for(i in 1:length(params)){
  p<-plot_posterior_diffs(
    data=gaze3_all_samples, 
    ci=.95, #credible interval
    par_prefix=params[i],
    rawpar_prefix=raw_params[i],
    sig_prefix=sig_params[i],
    par_name = param_names[i],
    group_nums = c(1,2),
    group_names=c('HC','SZ'),
    sample_sizes=c(39,42),
    round_digits=3,
    showtext=F)
  print(p + coord_cartesian(ylim = ylims[[i]])+theme(legend.position = "none"))
}

```

## Observed Power (Post-Hoc)

__Cohen (1988) guidelines of effect sizes:__
Pearson’s r: small = 0.10, medium = 0.30, large = 0.50
Cohen’s d: small = 0.20, medium = 0.50, large = 0.80 
Incremental f²: small = 0.02, medium = 0.15, large = 0.35	

```{r id993322, echo=FALSE, fig.height=3.75, fig.width=4, cache=TRUE}

library(pwr)

effect_names<-c("Threshold Separation (Gaze)", "Start Point (Gaze)", "NDT (Gaze)","Drift Sensitivity (Gaze)", "Drift Bias (Gaze)",
                'Threshold Separation (Gender)','Start Point (Gender)','NDT (Gender)','Drift Sensitivity (Gender)','Drift Bias (Gender)')
effect_sizes<-c(-.107,.472,.011, .881,-1.062,.349,-.876,-.961,.832,-.142)

post_hoc_power<-data.frame(Effect=effect_names,
                           Cohen_d=effect_sizes)

post_hoc_power$Power_to_Detect<-NA

n_g1<-39
n_g2<-42
total_n<-n_g1+n_g2

for(i in 1:nrow(post_hoc_power)){
  post_hoc_power$Power_to_Detect[i]<-round(pwr.t2n.test(n1 = n_g1, n2 = n_g2,  sig.level = 0.05, d=post_hoc_power$Cohen_d[i])[["power"]]*100,2)
}

post_hoc_power %>%
  kbl(caption = "Post Hoc Power to Detect Observed Group Differences",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T) 

```

# Regressions (All) 

__Do model parameters predict clinical correlates dimensionally (after controlling for age and sex) or do these relationships differ credibly between between HC and SZ?__

Bayesian regression models run in brms examined the effects of social cue processing efficiency for DDM parameters, schizophrenia diagnosis (sz_dummy; SZ=1, HC=0), and their interaction on key clinical outcomes (GPTS referential thinking and persecution; social cognition). All measures were z-scored, and evidence was assessed using Bayes factors (BF) via the Savage-Dickey ratio and 95% Highest Density Intervals (HDIs). We used weakly informative priors for predictors (Normal(0,1)) and between-subject variance parameters (student_t(3,0,2.5)).

## Primary - Controlling for Age, Sex {.tabset}

### GPTS-Referential Thinking
 
```{r id9932991112121331, echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE, cache=TRUE}

tmp_pred<-c('delta_bias','delta_bias_gend','delta_b1','delta_gend',
            'beta','beta_gend','sz_dummy','female_dummy','age')

tmp_outcome<-c('gpts_socref')

tmp_data<-subset(all_sub_data,all_sub_data$group=="SZ"|all_sub_data$group=="HC")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])

#define models to test (list predictors in order they should be entered) 
test_models<-list(c("age","female_dummy","sz_dummy"),
                  c("age","female_dummy","sz_dummy","delta_b1","sz_dummy:delta_b1"),
                  c("age","female_dummy","sz_dummy","delta_bias","sz_dummy:delta_bias"),
                  c("age","female_dummy","sz_dummy","beta","sz_dummy:beta"),
                  c("age","female_dummy","sz_dummy","delta_gend","sz_dummy:delta_gend"))

outcome<-"gpts_socref" #outcome variable
outfile<-paste0(dirname,"/analysis/full/saved_brms_models_1a1.rds")

result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=outfile,
                         credMass=.95,
                         nu=4) #student t degrees of freedom

result_glm_ddm$Pred<-rename_corr_pars(result_glm_ddm$Pred)

result_glm_ddm %>%
  kbl(caption = "Bayesian Regression: Predicting GPTS Referential Thinking (Full Sample, Controlling for Age, Sex)",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T) 

```
 
### GPTS-Persecution

```{r id99329911121331121, echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE, cache=TRUE}

tmp_pred<-c('delta_bias','delta_bias_gend','delta_b1','delta_gend',
            'beta','beta_gend','sz_dummy','female_dummy','age')

tmp_outcome<-c('gpts_persec')

tmp_data<-subset(all_sub_data,all_sub_data$group=="SZ"|all_sub_data$group=="HC")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])

#define models to test (list predictors in order they should be entered)
test_models<-list(c("age","female_dummy","sz_dummy"),
                  c("age","female_dummy","sz_dummy","delta_b1","sz_dummy:delta_b1"),
                  c("age","female_dummy","sz_dummy","delta_bias","sz_dummy:delta_bias"),
                  c("age","female_dummy","sz_dummy","beta","sz_dummy:beta"),
                  c("age","female_dummy","sz_dummy","delta_gend","sz_dummy:delta_gend"))

outcome<-"gpts_persec" #outcome
outfile<-paste0(dirname,"/analysis/full/saved_brms_models_2a1.rds")

result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=outfile,
                         credMass=.95,
                         nu=4) #student t df

result_glm_ddm$Pred<-rename_corr_pars(result_glm_ddm$Pred)

result_glm_ddm %>%
  kbl(caption = "Bayesian Regression: Predicting GPTS Persecution (Full Sample, Controlling for Age, Sex)",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T) 

```

### Social Cognition

```{r id99329933121412111211, echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE, cache=TRUE}

tmp_pred<-c('delta_bias','delta_b1','delta_gend',
            'beta','beta_gend','sz_dummy','female_dummy','age')
tmp_outcome<-c('soc_cog_pc_wrt')

tmp_data<-subset(all_sub_data,all_sub_data$group=="SZ"|all_sub_data$group=="HC")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])

#define models to test (list predictors in order they should be entered)
test_models<-list(c("age","female_dummy","sz_dummy"),
                  c("age","female_dummy","sz_dummy","delta_b1","sz_dummy:delta_b1"),
                  c("age","female_dummy","sz_dummy","delta_bias","sz_dummy:delta_bias"),
                  c("age","female_dummy","sz_dummy","beta","sz_dummy:beta"),
                  c("age","female_dummy","sz_dummy","delta_gend","sz_dummy:delta_gend"))

outcome<-"soc_cog_pc_wrt" #outcome variable
outfile<-paste0(dirname,"/analysis/full/saved_brms_models_3a1.rds")

result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=outfile,
                         credMass=.95,
                         nu=4) 

result_glm_ddm$Pred<-rename_corr_pars(result_glm_ddm$Pred)

result_glm_ddm %>%
  kbl(caption = "Bayesian Regression: Predicting Social Cognition (Full Sample, Controlling for Age, Sex)",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T) 

```

### Observed Power (Post-Hoc)

__Cohen (1988) guidelines of effect sizes:__
Pearson’s r: small = 0.10, medium = 0.30, large = 0.50
Cohen’s d: small = 0.20, medium = 0.50, large = 0.80 
Incremental f²: small = 0.02, medium = 0.15, large = 0.35	

```{r id993332332, echo=FALSE, cache=TRUE,fig.width=4,fig.height=3.75}

effect_names<-c("GPTS: Referential Thinking - Drift Sensitivity (Gaze)", 
                "GPTS: Referential Thinking - Drift Bias (Gaze)", 
                "GPTS: Referential Thinking - Start Point (Gaze)",
                "GPTS: Referential Thinking - Drift Sensitivity (Gender)",
                "GPTS: Persecution - Drift Sensitivity (Gaze)", 
                "GPTS: Persecution - Drift Bias (Gaze)", 
                "GPTS: Persecution - Start Point (Gaze)",
                "GPTS: Persecution - Drift Sensitivity (Gender)",
                "Social Cognition - Drift Sensitivity (Gaze)", 
                "Social Cognition - Drift Bias (Gaze)", 
                "Social Cognition - Start Point (Gaze)",
                "Social Cognition - Drift Sensitivity (Gender)")
effect_sizes<-c(.13,.03,.01,.21,.07,.03,.02,.19,.03,.03,.02,.01)

post_hoc_power<-data.frame(Effect=effect_names,
                           Cohen_f2=effect_sizes)

post_hoc_power$Power_to_Detect<-NA

n_g1<-39
n_g2<-42
total_n<-n_g1+n_g2
total_pred<-6
new_pred<-2

for(i in 1:nrow(post_hoc_power)){
  post_hoc_power$Power_to_Detect[i]<-round(
    pwr.f2.test(u = new_pred, v = total_n-total_pred, f2 = post_hoc_power$Cohen_f2[i], sig.level = 0.05)[["power"]]*100,2)
}

post_hoc_power %>%
  kbl(caption = "Post Hoc Power to Detect Observed Incremental Effect Above Null Model",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T) 

```

## Sensitivity - Controlling for Age, Sex, General Cognition {.tabset}

### GPTS-Referential Thinking
 
```{r id99329911121921331, echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE, cache=TRUE}

tmp_pred<-c('delta_b1','delta_gend','sz_dummy','female_dummy','age','matrics')
tmp_outcome<-c('gpts_socref')

tmp_data<-subset(all_sub_data,all_sub_data$group=="SZ"|all_sub_data$group=="HC")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])

#define models to test (list predictors in order they should be entered) 
test_models<-list(c("age","female_dummy","sz_dummy","matrics"),
                  c("age","female_dummy","sz_dummy","matrics","delta_b1","sz_dummy:delta_b1"),
                  c("age","female_dummy","sz_dummy","matrics","delta_gend","sz_dummy:delta_gend"))

outcome<-"gpts_socref" #outcome variable
outfile<-paste0(dirname,"/analysis/full/saved_brms_models_1a2.rds")

result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=outfile,
                         credMass=.95,
                         nu=4) #student t degrees of freedom

result_glm_ddm$Pred<-rename_corr_pars(result_glm_ddm$Pred)

result_glm_ddm %>%
  kbl(caption = "Bayesian Regression: Predicting GPTS Referential Thinking (Full Sample, Controlling for Age, Sex, General Cognition)",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T) 

```
 
### GPTS-Persecution

```{r id993299111213931121, echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE, cache=TRUE}

tmp_pred<-c('delta_b1','delta_gend','matrics','sz_dummy','female_dummy','age')
tmp_outcome<-c('gpts_persec')

tmp_data<-subset(all_sub_data,all_sub_data$group=="SZ"|all_sub_data$group=="HC")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])

#define models to test (list predictors in order they should be entered)
test_models<-list(c("age","female_dummy","sz_dummy","matrics"),
                  c("age","female_dummy","sz_dummy","matrics","delta_b1","sz_dummy:delta_b1"),
                  c("age","female_dummy","sz_dummy","matrics","delta_gend","sz_dummy:delta_gend"))

outcome<-"gpts_persec" #outcome
outfile<-paste0(dirname,"/analysis/full/saved_brms_models_2a2.rds")

result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=outfile,
                         credMass=.95,
                         nu=4) #student t df

result_glm_ddm$Pred<-rename_corr_pars(result_glm_ddm$Pred)

result_glm_ddm %>%
  kbl(caption = "Bayesian Regression: Predicting GPTS Persecution (Full Sample, Controlling for Age, Sex, General Cognition)",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T) 

```
 
# Regressions (SZ Only)

## Primary - Controlling for Age, Sex {.tabset}

__For primary regressions in which DDM parameters showed interactions with SZ diagnosis to predict any of our clinical outcomes of interest, re-run within group__

### GPTS-Referential Thinking

```{r id9932991112, echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE, cache=TRUE,fig.show='asis',results='asis'}

show_modelchange<-TRUE
options(mc.cores = parallel::detectCores())
tmp_pred<-c('delta_b1','delta_gend','age','female_dummy')
tmp_outcome<-c('gpts_socref')
tmp_data<-subset(all_sub_data,all_sub_data$group=="SZ")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) 
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])
pars<-c("delta_b1","delta_gend")
mod_suffix<-c("b","c")
outcome<-"gpts_socref"
outcome_name<-"GPTS Referential Thinking (SZ Only)"
outcome_prefix<-1

for(i in 1:length(pars)){
  
  #define models to test (list predictors in order they should be entered)
  test_models<-list(c("age","female_dummy"),
                    c("age","female_dummy",pars[i]))
  
  result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=paste0(dirname,
                                        paste0("/analysis/full/saved_brms_models_",
                                                       outcome_prefix,
                                                       mod_suffix[i],".rds")),
                         credMass=.95,
                         nu=4)
  
  if(i!=1){
    startrow<-which(result_glm_ddm$Model==2)
    endrow<-nrow(result_glm_ddm)
    result_glm_ddm<-result_glm_ddm[c(startrow:endrow),]
  }
  
  if(show_modelchange==FALSE){
    startcol<-which(colnames(result_glm_ddm)=="Ref Model")
    endcol<-which(colnames(result_glm_ddm)=="Pred")-1
    result_glm_ddm<-result_glm_ddm[,-c(startcol:endcol)]
    result_glm_ddm$Model[1]<-i
  }
  
  if(i==1){
    all_result_glm_ddm<-result_glm_ddm
  }else{
    all_result_glm_ddm<-rbind(all_result_glm_ddm,result_glm_ddm)
  }
}

rownames(all_result_glm_ddm)<-NULL
all_result_glm_ddm$Pred<-rename_corr_pars(all_result_glm_ddm$Pred)

print(all_result_glm_ddm %>%
  kbl(caption = paste0("Bayesian Regression: Predicting ", outcome_name),valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T))

```

### GPTS-Persecution

```{r id993299111218, echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE, cache=TRUE,fig.show='asis',results='asis'}

show_modelchange<-TRUE
tmp_pred<-c('delta_b1','delta_gend','age','female_dummy')
tmp_outcome<-c('gpts_persec')
tmp_data<-subset(all_sub_data,all_sub_data$group=="SZ")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])
pars<-c("delta_b1","delta_gend")
mod_suffix<-c("b","c")
outcome<-"gpts_persec"
outcome_name<-"GPTS Persecution (SZ Only)"
outcome_prefix<-2

for(i in 1:length(pars)){
  
  #define models to test (list predictors in order they should be entered)
  test_models<-list(c("age","female_dummy"),
                    c("age","female_dummy",pars[i]))
  
  result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=paste0(dirname,paste0("/analysis/full/saved_brms_models_",
                                                       outcome_prefix,
                                                       mod_suffix[i],".rds")),
                         credMass=.95,
                         nu=4)
  
  if(i!=1){
    startrow<-which(result_glm_ddm$Model==2)
    endrow<-nrow(result_glm_ddm)
    result_glm_ddm<-result_glm_ddm[c(startrow:endrow),]
  }
  
  if(show_modelchange==FALSE){
    startcol<-which(colnames(result_glm_ddm)=="Ref Model")
    endcol<-which(colnames(result_glm_ddm)=="Pred")-1
    result_glm_ddm<-result_glm_ddm[,-c(startcol:endcol)]
    result_glm_ddm$Model[1]<-i
  }
  
  if(i==1){
    all_result_glm_ddm<-result_glm_ddm
  }else{
    all_result_glm_ddm<-rbind(all_result_glm_ddm,result_glm_ddm)
  }
}

rownames(all_result_glm_ddm)<-NULL
all_result_glm_ddm$Pred<-rename_corr_pars(all_result_glm_ddm$Pred)

print(all_result_glm_ddm %>%
  kbl(caption = paste0("Bayesian Regression: Predicting ", outcome_name),valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T))
```
 
### Social Cognition

```{r id9932991112118, echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE, cache=TRUE,fig.show='asis',results='asis'}

show_modelchange<-TRUE
tmp_pred<-c('delta_b1','delta_bias','delta_gend','age','female_dummy')
tmp_outcome<-c('soc_cog_pc_wrt')
tmp_data<-subset(all_sub_data,all_sub_data$group=="SZ")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])
pars<-c("delta_b1","delta_bias",'delta_gend')
mod_suffix<-c("b","c","d","e")
outcome<-"soc_cog_pc_wrt"
outcome_name<-"Social Cognition (SZ Only)"
outcome_prefix<-3

for(i in 1:length(pars)){
  
  #define models to test (list predictors in order they should be entered)
  test_models<-list(c("age","female_dummy"),
                    c("age","female_dummy",pars[i]))
  
  result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=paste0(dirname,paste0("/analysis/full/saved_brms_models_",
                                                       outcome_prefix,
                                                       mod_suffix[i],".rds")),
                         credMass=.95,
                         nu=4)
  
  if(i!=1){
    startrow<-which(result_glm_ddm$Model==2)
    endrow<-nrow(result_glm_ddm)
    result_glm_ddm<-result_glm_ddm[c(startrow:endrow),]
  }
  
  if(show_modelchange==FALSE){
    startcol<-which(colnames(result_glm_ddm)=="Ref Model")
    endcol<-which(colnames(result_glm_ddm)=="Pred")-1
    result_glm_ddm<-result_glm_ddm[,-c(startcol:endcol)]
    result_glm_ddm$Model[1]<-i
  }
  
  if(i==1){
    all_result_glm_ddm<-result_glm_ddm
  }else{
    all_result_glm_ddm<-rbind(all_result_glm_ddm,result_glm_ddm)
  }
}

rownames(all_result_glm_ddm)<-NULL
all_result_glm_ddm$Pred<-rename_corr_pars(all_result_glm_ddm$Pred)

print(all_result_glm_ddm %>%
  kbl(caption = paste0("Bayesian Regression: Predicting ", outcome_name),valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T))
```

{-}
 
## Sensitivity - Controlling for Age, Sex, General Cognition {.tabset}

__Re-run those follow-up within group regressions to see if effects remain after controlling for general cognitive ability.__

### GPTS-Referential Thinking

```{r id99329911129, echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE, cache=TRUE,fig.show='asis',results='asis'}

show_modelchange<-TRUE
options(mc.cores = parallel::detectCores())
tmp_pred<-c('delta_b1','delta_gend','age','female_dummy','matrics')
tmp_outcome<-c('gpts_socref')
tmp_data<-subset(all_sub_data,all_sub_data$group=="SZ")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])
pars<-c("delta_b1","delta_gend")
mod_suffix<-c("b","c")
outcome<-"gpts_socref"
outcome_name<-"GPTS Referential Thinking (SZ Only)"
outcome_prefix<-"1.1"

for(i in 1:length(pars)){
  
  #define models to test (list predictors in order they should be entered)
  test_models<-list(c("age","female_dummy","matrics"),
                    c("age","female_dummy","matrics",pars[i]))
  
  result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=paste0(dirname,
                                        paste0("/analysis/full/saved_brms_models_",
                                                       outcome_prefix,
                                                       mod_suffix[i],".rds")),
                         credMass=.95,
                         nu=4)
  
    if(i!=1){
    startrow<-which(result_glm_ddm$Model==2)
    endrow<-nrow(result_glm_ddm)
    result_glm_ddm<-result_glm_ddm[c(startrow:endrow),]
  }
  
  if(show_modelchange==FALSE){
    startcol<-which(colnames(result_glm_ddm)=="Ref Model")
    endcol<-which(colnames(result_glm_ddm)=="Pred")-1
    result_glm_ddm<-result_glm_ddm[,-c(startcol:endcol)]
    result_glm_ddm$Model[1]<-i
  }
  
  if(i==1){
    all_result_glm_ddm<-result_glm_ddm
  }else{
    all_result_glm_ddm<-rbind(all_result_glm_ddm,result_glm_ddm)
  }
}

rownames(all_result_glm_ddm)<-NULL
all_result_glm_ddm$Pred<-rename_corr_pars(all_result_glm_ddm$Pred)

print(all_result_glm_ddm %>%
  kbl(caption = paste0("Bayesian Regression: Predicting ", outcome_name),valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T))

```
 
### GPTS-Persecution

```{r id9932991112218, echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE, cache=TRUE,fig.show='asis',results='asis'}

show_modelchange<-TRUE
tmp_pred<-c('delta_b1','delta_gend','age','female_dummy','matrics')
tmp_outcome<-c('gpts_persec')
tmp_data<-subset(all_sub_data,all_sub_data$group=="SZ")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])
pars<-c("delta_b1","delta_gend")
mod_suffix<-c("b","c")
outcome<-"gpts_persec"
outcome_name<-"GPTS Persecution (SZ Only)"
outcome_prefix<-"2.1"

for(i in 1:length(pars)){
  
  #define models to test (list predictors in order they should be entered)
  test_models<-list(c("age","female_dummy","matrics"),
                    c("age","female_dummy","matrics",pars[i]))
  
  result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=paste0(dirname,paste0("/analysis/full/saved_brms_models_",
                                                       outcome_prefix,
                                                       mod_suffix[i],".rds")),
                         credMass=.95,
                         nu=4)
  
    if(i!=1){
    startrow<-which(result_glm_ddm$Model==2)
    endrow<-nrow(result_glm_ddm)
    result_glm_ddm<-result_glm_ddm[c(startrow:endrow),]
  }
  
  if(show_modelchange==FALSE){
    startcol<-which(colnames(result_glm_ddm)=="Ref Model")
    endcol<-which(colnames(result_glm_ddm)=="Pred")-1
    result_glm_ddm<-result_glm_ddm[,-c(startcol:endcol)]
    result_glm_ddm$Model[1]<-i
  }
  
  if(i==1){
    all_result_glm_ddm<-result_glm_ddm
  }else{
    all_result_glm_ddm<-rbind(all_result_glm_ddm,result_glm_ddm)
  }
}

rownames(all_result_glm_ddm)<-NULL
all_result_glm_ddm$Pred<-rename_corr_pars(all_result_glm_ddm$Pred)

print(all_result_glm_ddm %>%
  kbl(caption = paste0("Bayesian Regression: Predicting ", outcome_name),valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T))
```
 
{-}

# Regressions (HC Only)

__For primary regressions in which DDM parameters showed interactions with SZ diagnosis to predict any of our clinical outcomes of interest, re-run within group__

## Primary - Controlling for Age, Sex {.tabset}

### GPTS-Referential Thinking

```{r id99329911121, echo=FALSE, fig.height=4, fig.show='asis', fig.width=5, message=FALSE, warning=FALSE, cache=TRUE, results='asis'}
#
show_modelchange<-TRUE
tmp_pred<-c('delta_b1','delta_gend','age','female_dummy')
tmp_outcome<-c('gpts_socref')
tmp_data<-subset(all_sub_data,all_sub_data$group=="HC")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])
pars<-c("delta_b1","delta_gend")
mod_suffix<-c("d","e")
outcome<-"gpts_socref"
outcome_name<-"GPTS Referential Thinking (HC Only)"
outcome_prefix<-1

for(i in 1:length(pars)){
  
  #define models to test (list predictors in order they should be entered)
  test_models<-list(c("age","female_dummy"),
                    c("age","female_dummy",pars[i]))
  
  result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=paste0(dirname,paste0("/analysis/full/saved_brms_models_",
                                                       outcome_prefix,
                                                       mod_suffix[i],".rds")),
                         credMass=.95,
                         nu=4)
  
  
  if(i!=1){
    startrow<-which(result_glm_ddm$Model==2)
    endrow<-nrow(result_glm_ddm)
    result_glm_ddm<-result_glm_ddm[c(startrow:endrow),]
  }
  
  if(show_modelchange==FALSE){
    startcol<-which(colnames(result_glm_ddm)=="Ref Model")
    endcol<-which(colnames(result_glm_ddm)=="Pred")-1
    result_glm_ddm<-result_glm_ddm[,-c(startcol:endcol)]
    result_glm_ddm$Model[1]<-i
  }
  
  if(i==1){
    all_result_glm_ddm<-result_glm_ddm
  }else{
    all_result_glm_ddm<-rbind(all_result_glm_ddm,result_glm_ddm)
  }
}

rownames(all_result_glm_ddm)<-NULL
all_result_glm_ddm$Pred<-rename_corr_pars(all_result_glm_ddm$Pred)

print(all_result_glm_ddm %>%
  kbl(caption = paste0("Bayesian Regression: Predicting ", outcome_name),valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T))

```
 
### GPTS-Persecution

```{r id9932991112181, echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE, cache=TRUE,fig.show='asis',results='asis'}
#
show_modelchange<-TRUE
tmp_pred<-c('delta_b1','delta_gend','age','female_dummy')
tmp_outcome<-c('gpts_persec')
tmp_data<-subset(all_sub_data,all_sub_data$group=="HC")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])
pars<-c("delta_b1","delta_gend")
mod_suffix<-c("d","e")
outcome<-"gpts_persec"
outcome_name<-"GPTS Persecution (HC Only)"
outcome_prefix<-2

for(i in 1:length(pars)){
  
  #define models to test (list predictors in order they should be entered)
  test_models<-list(c("age","female_dummy"),
                    c("age","female_dummy",pars[i]))
  
  result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=paste0(dirname,paste0("/analysis/full/saved_brms_models_",
                                                       outcome_prefix,
                                                       mod_suffix[i],".rds")),
                         credMass=.95,
                         nu=4)
  
  if(i!=1){
    startrow<-which(result_glm_ddm$Model==2)
    endrow<-nrow(result_glm_ddm)
    result_glm_ddm<-result_glm_ddm[c(startrow:endrow),]
  }
  
  if(show_modelchange==FALSE){
    startcol<-which(colnames(result_glm_ddm)=="Ref Model")
    endcol<-which(colnames(result_glm_ddm)=="Pred")-1
    result_glm_ddm<-result_glm_ddm[,-c(startcol:endcol)]
    result_glm_ddm$Model[1]<-i
  }
  
  if(i==1){
    all_result_glm_ddm<-result_glm_ddm
  }else{
    all_result_glm_ddm<-rbind(all_result_glm_ddm,result_glm_ddm)
  }
}

rownames(all_result_glm_ddm)<-NULL
all_result_glm_ddm$Pred<-rename_corr_pars(all_result_glm_ddm$Pred)

print(all_result_glm_ddm %>%
  kbl(caption = paste0("Bayesian Regression: Predicting ", outcome_name),valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T))

```
 
### Social Cognition

```{r id99329911121181, echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE, cache=TRUE,fig.show='asis',results='asis'}
#
show_modelchange<-TRUE
tmp_pred<-c('delta_b1','delta_bias','delta_gend','age','female_dummy')
tmp_outcome<-c('soc_cog_pc_wrt')
tmp_data<-subset(all_sub_data,all_sub_data$group=="HC")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) 
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])
pars<-c("delta_b1","delta_bias","delta_gend")
mod_suffix<-c("f","g","h","i")
outcome<-"soc_cog_pc_wrt"
outcome_name<-"Social Cognition (HC Only)"
outcome_prefix<-3

for(i in 1:length(pars)){
  
  #define models to test (list predictors in order they should be entered)
  test_models<-list(c("age","female_dummy"),
                    c("age","female_dummy",pars[i]))
  
  result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=paste0(dirname,paste0("/analysis/full/saved_brms_models_",
                                                       outcome_prefix,
                                                       mod_suffix[i],".rds")),
                         credMass=.95,
                         nu=4)
  
  if(i!=1){
    startrow<-which(result_glm_ddm$Model==2)
    endrow<-nrow(result_glm_ddm)
    result_glm_ddm<-result_glm_ddm[c(startrow:endrow),]
  }
  
  if(show_modelchange==FALSE){
    startcol<-which(colnames(result_glm_ddm)=="Ref Model")
    endcol<-which(colnames(result_glm_ddm)=="Pred")-1
    result_glm_ddm<-result_glm_ddm[,-c(startcol:endcol)]
    result_glm_ddm$Model[1]<-i
  }
  
  if(i==1){
    all_result_glm_ddm<-result_glm_ddm
  }else{
    all_result_glm_ddm<-rbind(all_result_glm_ddm,result_glm_ddm)
  }
}

rownames(all_result_glm_ddm)<-NULL
all_result_glm_ddm$Pred<-rename_corr_pars(all_result_glm_ddm$Pred)

print(all_result_glm_ddm %>%
  kbl(caption = paste0("Bayesian Regression: Predicting ", outcome_name),valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T))

```

## Sensitivity - Controlling for Age, Sex, General Cognition {.tabset}

__Re-run those follow-up within group regressions to see if effects remain after controlling for general cognitive ability.__

### GPTS-Referential Thinking

```{r id993299131121, echo=FALSE, fig.height=4, fig.show='asis', fig.width=5, message=FALSE, warning=FALSE, cache=TRUE, results='asis'}
#
show_modelchange<-TRUE
tmp_pred<-c('delta_b1','delta_gend','age','female_dummy','matrics')
tmp_outcome<-c('gpts_socref')
tmp_data<-subset(all_sub_data,all_sub_data$group=="HC")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])
pars<-c("delta_b1","delta_gend")
mod_suffix<-c("d","e")
outcome<-"gpts_socref"
outcome_name<-"GPTS Referential Thinking (HC Only)"
outcome_prefix<-"1.1"

for(i in 1:length(pars)){
  
  #define models to test (list predictors in order they should be entered)
  test_models<-list(c("age","female_dummy","matrics"),
                    c("age","female_dummy","matrics",pars[i]))
  
  result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=paste0(dirname,paste0("/analysis/full/saved_brms_models_",
                                                       outcome_prefix,
                                                       mod_suffix[i],".rds")),
                         credMass=.95,
                         nu=4)
  
  if(i!=1){
    startrow<-which(result_glm_ddm$Model==2)
    endrow<-nrow(result_glm_ddm)
    result_glm_ddm<-result_glm_ddm[c(startrow:endrow),]
  }
  
  if(show_modelchange==FALSE){
    startcol<-which(colnames(result_glm_ddm)=="Ref Model")
    endcol<-which(colnames(result_glm_ddm)=="Pred")-1
    result_glm_ddm<-result_glm_ddm[,-c(startcol:endcol)]
    result_glm_ddm$Model[1]<-i
  }
  
  if(i==1){
    all_result_glm_ddm<-result_glm_ddm
  }else{
    all_result_glm_ddm<-rbind(all_result_glm_ddm,result_glm_ddm)
  }
}

rownames(all_result_glm_ddm)<-NULL
all_result_glm_ddm$Pred<-rename_corr_pars(all_result_glm_ddm$Pred)

print(all_result_glm_ddm %>%
  kbl(caption = paste0("Bayesian Regression: Predicting ", outcome_name),valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T))

```
 
### GPTS-Persecution

```{r id998329911121891, echo=FALSE, fig.height=4, fig.width=5, message=FALSE, warning=FALSE, cache=TRUE,fig.show='asis',results='asis'}
#
show_modelchange<-TRUE
tmp_pred<-c('delta_b1','delta_gend','age','female_dummy','matrics')
tmp_outcome<-c('gpts_persec')
tmp_data<-subset(all_sub_data,all_sub_data$group=="HC")
tmp_corr_data<-dplyr::select(tmp_data,c(tmp_pred,tmp_outcome))
clean_data<-na.omit(tmp_corr_data) #keep complete cases only
cols_to_zscore <- c(tmp_pred,tmp_outcome)
cols_to_zscore<-cols_to_zscore[-grep("dummy",cols_to_zscore)]
clean_data[cols_to_zscore] <- scale(clean_data[cols_to_zscore])
pars<-c("delta_b1","delta_gend")
mod_suffix<-c("d","e")
outcome<-"gpts_persec"
outcome_name<-"GPTS Persecution (HC Only)"
outcome_prefix<-"2.1"

for(i in 1:length(pars)){
  
  #define models to test (list predictors in order they should be entered)
  test_models<-list(c("age","female_dummy","matrics"),
                    c("age","female_dummy","matrics",pars[i]))
  
  result_glm_ddm<-brms_robust_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=paste0(dirname,paste0("/analysis/full/saved_brms_models_",
                                                       outcome_prefix,
                                                       mod_suffix[i],".rds")),
                         credMass=.95,
                         nu=4)
  
  if(i!=1){
    startrow<-which(result_glm_ddm$Model==2)
    endrow<-nrow(result_glm_ddm)
    result_glm_ddm<-result_glm_ddm[c(startrow:endrow),]
  }
  
  if(show_modelchange==FALSE){
    startcol<-which(colnames(result_glm_ddm)=="Ref Model")
    endcol<-which(colnames(result_glm_ddm)=="Pred")-1
    result_glm_ddm<-result_glm_ddm[,-c(startcol:endcol)]
    result_glm_ddm$Model[1]<-i
  }
  
  if(i==1){
    all_result_glm_ddm<-result_glm_ddm
  }else{
    all_result_glm_ddm<-rbind(all_result_glm_ddm,result_glm_ddm)
  }
}

rownames(all_result_glm_ddm)<-NULL
all_result_glm_ddm$Pred<-rename_corr_pars(all_result_glm_ddm$Pred)

print(all_result_glm_ddm %>%
  kbl(caption = paste0("Bayesian Regression: Predicting ", outcome_name),valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T))

```
 
{-}

# References












Andreasen, N. C. (1984). Scale for the assessment of positive symptoms. Group, 17(2), 173-180.

Andreasen NC (1989). The Scale for the Assessment of Negative Symptoms (SANS): Conceptual and Theoretical Foundations. British Journal of Psychiatry. 155(S7):49-52. doi:10.1192/S0007125000291496

August, S. M., Kiwanuka, J. N., McMahon, R. P., & Gold, J. M. (2012). The MATRICS Consensus Cognitive Battery (MCCB): Clinical and cognitive correlates. Schizophrenia Research, 134(1), 76–82.

Baron-Cohen, S., Wheelwright, S., Hill, J., Raste, Y., & Plumb, I. (2001). The Reading the Mind in the Eyes Test Revised Version: A Study with Normal Adults, and Adults with Asperger Syndrome or High-functioning Autism. Journal of Child Psychology and Psychiatry, 42(2), 241–251. https://doi.org/10.1111/1469-7610.00715

Beck, A.T., Ward, C. H., Mendelson, M., Mock, J., & Erbaugh, J. (1961) An inventory for measuring depression. Archives of General Psychiatry, 4, 561-571.

Burton, C. Z., Vella, L., Harvey, P. D., Patterson, T. L., Heaton, R. K., & Twamley, E. W. (2013). Factor structure of the MATRICS Consensus Cognitive Battery (MCCB) in schizophrenia. Schizophrenia Research, 146(0), 244–248. https://doi.org/10.1016/j.schres.2013.02.026

Cohen. (1988). Dedekind completion of positive concatenation structures: Necessary and sufficient conditions. Journal of Mathematical Psychology, 32(1), 64–71. https://doi.org/10.1016/0022-2496(88)90038-7

Cornblatt BA, Auther AM, Niendam T, et al. (2007). Preliminary findings for two new measures of social and role functioning in the prodromal phase of schizophrenia. Schizophrenia Bulletin. 33:688–702

Couture SM, Grant PM, Beck AT, Morrison AP (2007, unpublished). Success and resource appraisals questionnaire (SARA-Q).

Morey, R., & Rouder, J. (2015). BayesFactor: Computation of Bayes Factors for common designs. R package version 0.9.12–12.

First, M. B., Spitzer, R. L., Gibbon, M., & Williams, J. B. W. (2002). Structured Clinical Interview for DSM-IV-TR Axis I Disorders, Research Version, Patient Edition (SCID-I/P). Biometrics Research.

Freeman, D., Loe, B. S., Kingdon, D., Startup, H., Molodynski, A., Rosebrock, L., Brown, P., Sheaves, B., Waite, F., & Bird, J. C. (2019). The revised Green et al., Paranoid Thoughts Scale (R-GPTS): Psychometric properties, severity ranges, and clinical cut-offs. Psychological Medicine, 51(2), 244–253. https://doi.org/10.1017/S0033291719003155

Green, C. E. L., Freeman, D., Kuipers, E., Bebbington, P., Fowler, D., Dunn, G., & Garety, P. A. (2007). Measuring ideas of persecution and social reference: The Green et al Paranoid Thought Scales (GPTS). Psychological Medicine, 38(1), 101–111. https://doi.org/10.1017/s0033291707001638

Keefe, R. S. E., Fox, K. H., Harvey, P. D., Cucchiaro, J., Siu, C., & Loebel, A. (2011). Characteristics of the MATRICS Consensus Cognitive Battery in a 29-site antipsychotic schizophrenia clinical trial. Schizophrenia Research, 125(2–3), 161–168. https://doi.org/10.1016/j.schres.2010.09.015

Kohler, C. G., Turner, T. H., Gur, R. E., & Gur, R. C. (2004). Recognition of Facial Emotions in Neuropsychiatric Disorders. CNS Spectrums, 9(4), 267–274. https://doi.org/10.1017/S1092852900009202

Lasagna, C., McLaughlin, M., Deng, W., Whiting, E., & Tso, I. (2020). Deconstructing eye contact perception: Measuring perceptual precision and self-referential tendency using an online psychophysical eye contact detection task. PLoS ONE, 15(3), 1–20. https://doi.org/10.1371/journal.pone.0230258

Mayer, Salovey, & Caruso. (2002). Mayer-Salovey-Caruso Emotional Intelligence Test (MSCEIT) User’s Manual. MHS Publishers.

Morey, R., & Rouder, J. (2015). BayesFactor: Computation of Bayes Factors for common  designs. R package version 0.9.12–12.

Overall, J. E., & Gorham, D. R. (1962). The brief psychiatric rating scale. Psychological reports, 10(3), 799-812.

Patterson T.L., Goldman S., McKibbin C.L., Hughs T., Jeste D.V. (2011). UCSD Performance- Based Skills Assessment: development of a new measure of everyday functioning for severely mentally ill adults. Schizophrenia. Bulletin. doi: 10.1093/oxfordjournals.schbul.a006870

Patterson TL, Moscona S, McKibbin CL, Davidson K, Jeste DV. (2001). Social skills performance assessment among older patients with schizophrenia. Schizophrenia Research. 48(2–3), 351–360. 10.1016/s0920-9964(00)00109-2

Pinkham, A. E., Penn, D. L., Green, M. F., & Harvey, P. D. (2016). Social Cognition Psychometric Evaluation: Results of the Initial Psychometric Study. Schizophrenia Bulletin, 42(2), 494–504. https://doi.org/10.1093/schbul/sbv056

Reniers RL, Corcoran R, Drake R, Shryane NM, Völlm BA. (2011). The QCAE: a Questionnaire of Cognitive and Affective Empathy. Journal of Personality Assessment. doi: 10.1080/00223891.2010.528484.

Silverstein SM, Keane BP, Barch DM, Carter CS, Gold JM, Kovács I, MacDonald A 3rd, Ragland JD, Strauss ME. (2012). Optimization and validation of a visual integration test for schizophrenia research. Schizophrenia Bulletin. doi: 10.1093/schbul/sbr141. 

Spielberger, C. D., Gorsuch, R. L., Lushene, R., Vagg, P. R., & Jacobs, G. A. (1983). Manual for the State-Trait Anxiety Inventory. Palo Alto, CA: Consulting Psychologists Press.

Stacklies, W., Redestig, H., Scholz, M., Walther, D., & Selbig, J. (2007). pcaMethods—A bioconductor package providing PCA methods for incomplete data. Bioinformatics, 23(9), 1164–1167. https://doi.org/10.1093/bioinformatics/btm069

Tso IF, Grove TB, Taylor SF. (2012). Self-assessment of psychological stress in schizophrenia: Preliminary evidence of reliability and validity. Psychiatry Research. doi: 10.1016/j.psychres.2011.07.009. 

Wilkinson, G. S., & Robertson, G. J. (1993). Wide range achievement test 4. Journal of Clinical and Experimental Neuropsychology.




